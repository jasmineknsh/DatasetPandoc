<p><img src="media/image1.jpeg" style="width:0.8273in;height:0.90667in" /><img src="media/image2.jpeg" /><a
        href="https://doi.org/10.1016/j.ejor.2024.03.020">European Journal of Operational Research 320 (2025)
        271–289</a></p>
<blockquote>
    <p>Invited Review</p>
    <p><img src="media/image3.png" style="width:0.38966in;height:0.38966in" />A survey of contextual optimization
        methods for decision-making under uncertainty</p>
    <p>Utsav Sadana <a href="#_bookmark0">a</a>, Abhilash Chenreddy <a href="#_bookmark1">b</a>, Erick Delage <a
            href="#_bookmark1">b</a>,<a href="#_bookmark4">∗</a>, Alexandre Forel <a href="#_bookmark2">c</a>, Emma
        Frejinger <a href="#_bookmark3">d</a>,</p>
    <p>Thibaut Vidal <a href="#_bookmark2">c</a></p>
    <p><span id="_bookmark0" class="anchor"></span>a <span id="_bookmark1" class="anchor"></span><em>Department of
            Computer Science and Operations Research, Université de Montréal, Québec, Canada</em></p>
    <p>b <span id="_bookmark2" class="anchor"></span><em>GERAD &amp; Department of Decision Sciences, HEC Montréal,
            Québec, Canada</em></p>
    <p>c <span id="_bookmark3" class="anchor"></span><em>CIRRELT &amp; SCALE-AI Chair in Data-Driven Supply Chains,
            Department of Mathematical and Industrial Engineering, Polytechnique Montréal, Québec, Canada</em></p>
    <p>d <em>CIRRELT &amp; Department of Computer Science and Operations Research, Université de Montréal, Québec,
            Canada</em></p>
    <p>A R T I C L E I N F O</p>
    <p><em>Keywords:</em></p>
    <p>Contextual optimization Conditional stochastic programming Task-based learning</p>
    <p>Data-driven optimization Policy optimization</p>
    <p>A B S T R A C T</p>
    <p>Recently there has been a surge of interest in operations research (OR) and the machine learning (ML) community
        in combining prediction algorithms and optimization techniques to solve decision-making problems in the face of
        uncertainty. This gave rise to the field of contextual optimization, under which data-driven procedures are
        developed to prescribe actions to the decision-maker that make the best use of the most recently updated
        information. A large variety of models and methods have been presented in both OR and ML literature under a
        variety of names, including data-driven optimization, prescriptive optimization, predictive stochastic
        programming, policy optimization, (smart) predict/estimate-then-optimize, decision-focused learning, (task-
        based) end-to-end learning/forecasting/optimization, etc. This survey article unifies these models under the
        lens of contextual stochastic optimization, thus providing a general presentation of a large variety of
        problems. We identify three main frameworks for learning policies from data and present the existing models and
        methods under a uniform notation and terminology. Our objective with this survey is to both strengthen the
        general understanding of this active field of research and stimulate further theoretical and algorithmic
        advancements in integrating ML and stochastic programming.</p>
</blockquote>
<h2 id="introduction">Introduction</h2>
<p>This article surveys the literature on single and two-stage contex- tual optimization. In contextual optimization, a
    decision-maker faces a decision-making problem with uncertainty where the distribution of uncertain parameters that
    affect the objective and the constraints is unknown, although correlated side information (covariates or fea- tures)
    can be exploited. The usefulness of side information in inferring relevant statistics of uncertain parameters and,
    thereby, in decision- making is evident in many different fields. For example, weather and time of day can help
    resolve uncertainty about congestion on a road network and aid in finding the shortest path traversing a city. In
    portfolio optimization, stock returns may depend on historical prices and sentiments posted on Twitter (<a
        href="#_bookmark221">Xu &amp; Cohen</a>, <a href="#_bookmark221">2018</a>). Harnessing this information can
    allow decision-makers to build a less risky portfolio. Similarly, a retailer facing uncertain demand for summer
    products can infer whether the demand will be low or high depending on the forecasted weather conditions (<a
        href="#_bookmark155">Martínez-de Albeniz &amp; Belkaid</a>, <a href="#_bookmark155">2021</a>).</p>
<p>In these applications, the decision-maker has access to historical data, that is, past values of the covariates
    (e.g., weather) and the corresponding uncertain parameter (e.g., congestion). Data-driven con- textual optimization
    methods use this data to estimate the conditional distribution of the uncertain parameter (or a sufficient
    statistic) based on the covariate. Conversely, traditional stochastic optimization models ignore contextual
    information and use unconditional distributions of the uncertain parameters to make a decision (<a
        href="#_bookmark64">Birge &amp; Louveaux</a>, <a href="#_bookmark64">2011</a>). Such a decision may be
    suboptimal (<a href="#_bookmark51">Ban &amp; Rudin</a>, <a href="#_bookmark51">2019</a>) and, in <a
        href="#_bookmark180">some</a> cases, even at the risk of being infeasible (<a href="#_bookmark180">Rahimian
        &amp; Pagnon-</a> <a href="#_bookmark180">celli</a>, <a href="#_bookmark180">2022</a>). The availability of data
    and huge computational power combined with advancements in machine learning (ML) and optimiza- tion techniques have
    resulted in a shift of paradigm to contextual optimization (<a href="#_bookmark159">Mišić &amp; Perakis</a>, <a
        href="#_bookmark159">2020</a>).</p>
<p>Making prescriptions using the side information requires a decision rule that maps the observed covariate to an
    action. We identify three different paradigms for learning this mapping.</p>
<blockquote>
    <p>∗ <span id="_bookmark4" class="anchor"></span>Corresponding author.</p>
    <p><em>E-mail addresses:</em> <a href="mailto:utsav.sadana@umontreal.ca">utsav.sadana@umontreal.ca</a> (U. Sadana),
        <a href="mailto:abhilash.chenreddy@hec.ca">abhilash.chenreddy@hec.ca</a> (A. Chenreddy), <a
            href="mailto:erick.delage@hec.ca">erick.delage@hec.ca</a> (E. Delage), <a
            href="mailto:alexandre.forel@polymtl.ca">alexandre.forel@polymtl.ca</a> (A. Forel), <a
            href="mailto:emma.frejinger@umontreal.ca">emma.frejinger@umontreal.ca</a> (E. Frejinger), <a
            href="mailto:thibaut.vidal@polymtl.ca">thibaut.vidal@polymtl.ca</a> (T. Vidal).</p>
    <p><a href="https://doi.org/10.1016/j.ejor.2024.03.020">https://doi.org/10.1016/j.ejor.2024.03.020</a> Received 10
        July 2023; Accepted 14 March 2024</p>
    <p>Available online 15 March 2024</p>
    <p>0377-2217/© 2024 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license
        (<a href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</a>).</p>
</blockquote>
<ul>
    <li>
        <blockquote>
            <p><strong>Decision rule optimization</strong>: This approach was introduced to the operation research
                community in <a href="#_bookmark148">Liyanage and Shanthikumar</a> (<a href="#_bookmark148">2005</a>)
                for data-driven optimization and popularized in <a href="#_bookmark51">Ban and</a> <a
                    href="#_bookmark51">Rudin</a> (<a href="#_bookmark51">2019</a>) for big data environments, although
                a similar idea was already common practice in reinforcement learning under the name of policy gradient
                methods (see <a href="#_bookmark204">Sutton et al.</a>, <a href="#_bookmark204">1999</a>, and literature
                that followed). It consists in employing a parameterized mapping as the decision rule and in identifying
                the parameter that achieves the best empirical performance based on the available data. The decision
                rule can be formed as a linear combination of functions of the covariates or even using a deep neural
                net- work (DNN). When the data available is limited, some form of regularization might also be needed.
            </p>
        </blockquote>
    </li>
    <li>
        <blockquote>
            <p><strong>Sequential learning and optimization (SLO)</strong>: <a href="#_bookmark58">Bertsimas and</a> <a
                    href="#_bookmark58">Kallus</a> (<a href="#_bookmark58">2020</a>) appears to be the first to have
                formalized this two-stage procedure (also referred to as predict/estimate-then- optimize or prescriptive
                optimization/stochastic programming) that first uses a trained model to predict a conditional
                distribution for the uncertain parameters given the covariates, and then solves an associated contextual
                stochastic optimization (CSO) problem to obtain the optimal action. This procedure can be robustified to
                reduce post-decision disappointment (<a href="#_bookmark196">Smith &amp; Winkler</a>, <a
                    href="#_bookmark196">2006</a>) caused by model overfitting or misspecification by using proper
                regularization at training time or by adapting the CSO problem formulation.</p>
        </blockquote>
    </li>
    <li>
        <blockquote>
            <p><strong>Integrated learning and optimization (ILO)</strong>: In the spirit of identifying the best
                decision rule, one might question in SLO the need for high precision predictors when one is instead
                mostly interested in the quality of the resulting prescribed action. This idea motivates an integrated
                version of learning and optimiza- tion that searches for the predictive model that guides the CSO
                problem toward the best-performing actions. The ILO paradigm appears as early as in <a
                    href="#_bookmark54">Bengio</a> (<a href="#_bookmark54">1997</a>) and has seen a resurgence recently
                in active streams of literature under various names such as smart predict-then-optimize,
                decision-focused learning, and (task-based) end-to-end learning/forecasting/optimization.</p>
        </blockquote>
    </li>
</ul>
<p>The outline of the survey goes as follows. Section <a href="#_bookmark6">2</a> rigorously defines the three
    frameworks for identifying the best mapping from covariate to action based on data: decision rule optimization, SLO,
    and ILO. Section <a href="#_bookmark20">3</a> reviews the literature on decision rule optimization with linear and
    non-linear decision rules. Section <a href="#_bookmark24">4</a> focuses on SLO, including the models that lead to
    robust decisions, and Section <a href="#_bookmark28">5</a> describes the models based on the ILO framework and the
    algorithms used to train them. Because ILO is the more recent and less explored framework of the three identified,
    we provide a separate subsection of applications of ILO to diverse problems such as logistics and energy management.
    Section <a href="#_bookmark40">6</a> provides an overview of active research directions being pursued both from a
    theoretical and applications perspective. Section <a href="#_bookmark41">7</a> concludes our survey with a summary
    of our contributions.</p>
<p>We note that there are other surveys and tutorials in the literature that are complementary to ours. <a
        href="#_bookmark159">Mišić and Perakis</a> (<a href="#_bookmark159">2020</a>) survey the applications of the SLO
    framework to problems in supply chain man- agement, revenue management, and healthcare operations. <a
        href="#_bookmark178">Qi and Shen</a> (<a href="#_bookmark178">2022</a>) is a tutorial that mainly focuses on the
    application of ILO to expected value-based models with limited discussions on more general approaches. It summarizes
    the most popular methods and some of their theoretical guarantees. <a href="#_bookmark136">Kotary et al.</a> (<a
        href="#_bookmark136">2021</a>) provide a comprehensive</p>
<p><a href="#_bookmark153">et al.</a> (<a href="#_bookmark153">2023</a>)<a href="#_bookmark5">1</a> include more recent
    expected value-based models and provide a comprehensive evaluation of their methods, complementing the toolbox of <a
        href="#_bookmark205">Tang and Khalil</a> (<a href="#_bookmark205">2022</a>) that provided an interface for
    solving expected value-based models.</p>
<p>Our survey of ILO literature goes beyond the expected value-based models and reflects better the more modern
    literature by casting the contextual decision problem as a CSO problem and presenting a com- prehensive overview of
    the current state of this rapidly progressing field of research. We establish links between approaches that minimize
    regret (<a href="#_bookmark100">Elmachtoub &amp; Grigas</a>, <a href="#_bookmark100">2022</a>), (task-based)
    end-to-end learn- ing (<a href="#_bookmark94">Donti et al.</a>, <a href="#_bookmark94">2017</a>) and imitation-based
    models (<a href="#_bookmark135">Kong et al.</a>, <a href="#_bookmark135">2022</a>). Further, we create a taxonomy
    based on the training procedure for a general ILO framework encompassing recent theoretical and algorith- mic
    progresses in designing differentiable surrogates and optimizers and improving training procedures based on
    unrolling and implicit differentiation.</p>
<h2 id="contextual-optimization-an-overview">Contextual optimization: An overview</h2>
<p><span id="_bookmark6" class="anchor"></span>The contextual optimization paradigm seeks a decision (i.e., an action)
    <em><strong>𝒛</strong></em> in a feasible set  ⊆ R𝑑<em><strong>𝒛</strong></em> that minimizes a cost function
    𝑐(<em><strong>𝒛</strong></em>, <em><strong>𝒚</strong></em>) with uncertain parameters
    <em><strong>𝒚</strong></em> ∈  ⊆ R<sup>𝑑</sup><em><strong>𝒚</strong></em> . The uncertain parameters are unknown
    when making the decision. However, a vector of relevant covariates <em><strong>𝒙</strong></em> ∈  ⊆
    R𝑑<em><strong>𝒙</strong></em> , which is correlated with the uncertain param- eters <em><strong>𝒚</strong></em>,
    is revealed before having to choose <em><strong>𝒛</strong></em>. The joint distribution of the covariates in  and
    uncertain parameters in  is denoted by P.</p>
<ol type="1">
    <li>
        <blockquote>
            <p><em>Contextual problem and policy</em></p>
        </blockquote>
    </li>
</ol>
<p>In general, uncertainty can appear in the objectives and constraints of the problem. In the main sections of this
    paper, we focus on problems with uncertain objectives and consider that the decision-maker is risk- neutral. We
    broaden the discussion to risk-averse settings and uncertain constraints in Section <a href="#_bookmark40">6</a>.
</p>
<p><em>The CSO problem.</em> Given a covariate described by a vector of covari- ates <em><strong>𝒙</strong></em> and
    the joint distribution P of the covariates <em><strong>𝒙</strong></em> and uncertain parameter
    <em><strong>𝒚</strong></em>, a risk-neutral decision-maker is interested in finding an optimal action
    𝑧∗(<em><strong>𝒙</strong></em>) ∈  that minimizes the expected costs conditioned on the covariate
    <em><strong>𝒙</strong></em>. Formally, the optimal action is a solution to the CSO problem given by:</p>
<p>(CSO) 𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>) ∈ argmin E<sub>P(<em><strong>𝒚</strong></em></sub>
    <sub><em><strong>𝒙</strong></em>)</sub> [𝑐 (<em><strong>𝒛</strong></em>, <em><strong>𝒚</strong></em>)] , <span
        id="_bookmark7" class="anchor"></span>(1)</p>
<blockquote>
    <p><em><strong>𝒛</strong></em>∈</p>
</blockquote>
<p>where P(<em><strong>𝒚 𝒙</strong></em>) denotes the conditional distribution of <em><strong>𝒚</strong></em> given
    the co- variate <em><strong>𝒙</strong></em> and it is assumed that a minimizer exists. For instance, a minimizer
    exists when  is compact, P(<em><strong>𝒚 𝒙</strong></em>) has bounded support and</p>
<p>𝑐(<em><strong>𝒛</strong></em>, <em><strong>𝒚</strong></em>) is continuous in <em><strong>𝒛</strong></em> almost
    surely (see <a href="#_bookmark209">Van Parys et al.</a>, <a href="#_bookmark209">2021</a>, for more details).</p>
<p>Problem (<a href="#_bookmark7">1</a>) can equivalently be written in a compact form using the expected cost operator
    ℎ(⋅, ⋅) that receives an action as a first argument and a distribution as a second argument:</p>
<p>𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>) ∈ argmin ℎ(<em><strong>𝒛</strong></em>, P(<em><strong>𝒚
            𝒙</strong></em>)) ∶= E<sub>P(<em><strong>𝒚</strong></em></sub> <sub><em><strong>𝒙</strong></em>)</sub>
    [𝑐 (<em><strong>𝒛</strong></em>, <em><strong>𝒚</strong></em>)] . <span id="_bookmark8" class="anchor"></span>(2)
</p>
<blockquote>
    <p><em><strong>𝒛</strong></em>∈</p>
</blockquote>
<p><em>Optimal policy.</em> In general, the decision-maker repeatedly solves CSO problems in many different contexts.
    Hence, the decision-maker is interested in finding the policy that provides the lowest long-term expected cost, that
    is:</p>
<blockquote>
    <p>𝜋<sup>∗</sup> ∈ argmin E<sub>P</sub>[𝑐(𝜋(<em><strong>𝒙</strong></em>), <em><strong>𝒚</strong></em>)] =
        argmin E<sub>P</sub>[ℎ(𝜋(<em><strong>𝒙</strong></em>),
        P(<em><strong>𝒚</strong></em>|<em><strong>𝒙</strong></em>))], <span id="_bookmark9" class="anchor"></span>(3)
    </p>
</blockquote>
<p>tion of constrained optimization models (see also <a href="#_bookmark55">Bengio et al.</a>, <a
        href="#_bookmark55">2021</a>). It also reviews some of the earlier literature on ILO applied to what</p>
<p>where 𝛱 ∶= {𝜋 ∶  → } denotes the class of all feasible policies.</p>
<p>we define as ‘‘expected value-based models’’, a subset of CSO problems <span class="underline"> </span></p>
<p>(see <a href="#_bookmark15">Definition</a> <a href="#_bookmark15">1</a>) where uncertainty can be completely
    described by <a href="#_bookmark153">a</a> sufficient statistic before the optimization problem is solved. <a
        href="#_bookmark153">Mandi</a></p>
<blockquote>
    <p>1 <a href="#_bookmark153">Mandi et al.</a> (<a href="#_bookmark153">2023</a>) appeared online soon after the
        submission of our paper.</p>
    <p><img src="media/image4.png" style="width:0.51912in" /></p>
</blockquote>
<p><img src="media/image5.png" style="width:2.0124in;height:0.40625in" /><img src="media/image6.png" /></p>
<blockquote>
    <p><span id="_bookmark10" class="anchor"></span><strong>Fig. 1.</strong> Decision and training pipelines based on
        the decision rule paradigm: (left) the decision pipeline and (right) the training pipeline for a given training
        example (<em><strong>𝒙</strong></em><sub>𝑖</sub>, <em><strong>𝒚</strong></em><sub>𝑖</sub>).</p>
</blockquote>
<p>Note that the optimal policy does not need to be obtained explic- itly in a closed form. Indeed, based on the
    interchangeability prop- erty (see Theorem 14.60 of <a href="#_bookmark183">Rockafellar &amp; Wets</a>, <a
        href="#_bookmark183">2009</a>), solving the CSO problem (<a href="#_bookmark7">1</a>) for any covariate
    <em><strong>𝒙</strong></em> naturally identifies an optimal policy:</p>
<blockquote>
    <p>𝜋̄(<em><strong>𝒙</strong></em>) ∈ argmin ℎ(<em><strong>𝒛</strong></em>, P(<em><strong>𝒚 𝒙</strong></em>))
        a.s.</p>
    <p><em><strong>𝒛</strong></em>∈</p>
    <p>⇔ E ( ( ) P( )) = min E ( ( ) P( ))</p>
    <p>𝜋∈𝛱</p>
</blockquote>
<p>assuming that a minimizer of ℎ(<em><strong>𝒛</strong></em>, P(<em><strong>𝒚 𝒙</strong></em>)) exists almost
    surely. Thus, the two optimal policies 𝜋∗ and 𝑧∗(⋅) coincide.</p>
<ol start="2" type="1">
    <li>
        <blockquote>
            <p><em>Mapping covariate to actions in a data-driven environment</em></p>
        </blockquote>
    </li>
</ol>
<p>Unfortunately, the joint distribution P is generally unknown. In- stead, the decision-maker possesses historical data
     𝑁 that is assumed to be made of independent and identically distribut<sup>𝑖</sup>e<sup>=</sup>d<sup>1</sup>
    realizations of (<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>) ∈  × . Using this data, the
    decision-maker aims</p>
<p>to find a policy that approximates well the optimal policy given by (<a href="#_bookmark9">3</a>). Many approaches
    have been proposed to find effective approx- imate policies. Most of them can be classified into the three main
    frameworks that we introduce below: (i) decision rule optimization,</p>
<p>(ii) sequential learning and optimization, and (iii) integrated learning and optimization.</p>
<ol type="1">
    <li>
        <blockquote>
            <p><em>Decision rule optimization</em></p>
        </blockquote>
    </li>
</ol>
<p>In this framework, the policy is assumed to belong to a hypothesis class 𝛱<em><strong>𝜽</strong></em> ∶=
    {𝜋<em><strong><sub>𝜽</sub></strong></em>}<sub><em><strong>𝜽</strong></em>∈𝛩</sub> ⊆ 𝛱 that contains a family of
    parametric policies</p>
<p>𝜋<em><strong><sub>𝜽</sub></strong></em> ∶  →  (e.g., linear functions or decision trees). The parameterized</p>
<p>policy 𝜋<em><strong><sub>𝜽</sub></strong></em> maps directly any covariate <em><strong>𝒙</strong></em> to an
    action 𝜋<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) and will be</p>
<p>referred to as a decision rule.</p>
<blockquote>
    <p>Denote by P̂ 𝑁 the empirical distribution of (<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>) given
        historical</p>
</blockquote>
<ol start="2" type="1">
    <li>
        <blockquote>
            <p><span id="bookmark11" class="anchor"></span><em>Learning and optimization</em></p>
        </blockquote>
    </li>
</ol>
<p>The second and third frameworks combine a predictive component and an optimization component. The predictive
    component is a general model 𝑓<em><strong><sub>𝜽</sub></strong></em>, parameterized by
    <em><strong>𝜽</strong></em>, whose role is to provide the input of the optimization component. For any covariate
    <em><strong>𝒙</strong></em>, the intermediate input</p>
<p>𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) can be interpreted as a predicted
    distribution that approximates the true conditional distribution P(<em><strong>𝒚 𝒙</strong></em>) (or a sufficient
    statistic in the case of expected value-based models). The predictive component is typically learned from historical
    data.</p>
<p>At decision time, a learning and optimization decision pipeline (see <a href="#_bookmark13">Fig. 2</a>) solves the
    CSO problem under 𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>), namely:</p>
<blockquote>
    <p>𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>, 𝑓<em><strong><sub>𝜽</sub></strong></em>) ∈ argmin
        ℎ(<em><strong>𝒛</strong></em>, 𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>)). <span
            id="_bookmark12" class="anchor"></span>(5)</p>
    <p><em><strong>𝒛</strong></em>∈</p>
</blockquote>
<p>The solution of Problem (<a href="#_bookmark12">5</a>) minimizes the expected cost with respect to the predicted
    distribution 𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>). Notice that the only
    approximation between Problem (<a href="#_bookmark12">5</a>) and the true CSO problem in (<a
        href="#_bookmark8">2</a>) lies in 𝑓<em><strong><sub>𝜽</sub></strong></em> being an approximation of
    P(<em><strong>𝒚 𝒙</strong></em>). Since the predicted distribution changes with the covariate
    <em><strong>𝒙</strong></em>, this pipeline also provides a policy. In fact, if the predictive component were able
    to perfectly predict the true conditional distribution P(<em><strong>𝒚 𝒙</strong></em>) for any
    <em><strong>𝒙</strong></em>, the pipeline would recover the optimal policy 𝜋∗ given in (<a
        href="#_bookmark9">3</a>).</p>
<p><img src="media/image8.png" style="width:3.40682in;height:0.40625in" /></p>
<blockquote>
    <p><span id="_bookmark13" class="anchor"></span><strong>Fig. 2.</strong> Decision pipeline for learning and
        optimization.</p>
</blockquote>
<p>We now detail the second and third frameworks to address con- textual optimization: SLO and ILO. They differ in the
    way the predic- tor 𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) is trained using the
    historical data.</p>
<blockquote>
    <p><em>Sequential learning and optimization.</em> In this framework, the contextual</p>
</blockquote>
<p>data  . One can identify the ‘‘best’’ parameterization of the policy</p>
<p>in 𝛱<em><strong>𝜽</strong></em> by solving the following empirical risk minimization (ERM) problem:</p>
<p>predictor is obtained by minimizing an estimation error, 𝜌, between the conditional distribution given by
    𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) and the true conditional <span
        id="_bookmark14" class="anchor"></span>distribution of <em><strong>𝒚</strong></em> given
    <em><strong>𝒙</strong></em>. Training the contextual parametric predictor</p>
<blockquote>
    <p>(ERM) <em><strong>𝜽</strong></em>∗ ∈ argmin 𝐻(𝜋<em><strong><sub>𝜽</sub></strong></em>, P̂ 𝑁 ) ∶= EP̂</p>
    <p><em><strong>𝜽</strong></em></p>
</blockquote>
<p>[𝑐(𝜋<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>), <em><strong>𝒚</strong></em>)]. (4)</p>
<blockquote>
    <p>usually implies solving the following estimation problem:</p>
</blockquote>
<p>In simple terms, Problem (<a href="#_bookmark14">4</a>) finds the policy 𝜋<em><strong><sub>𝜽</sub></strong></em>∗ ∈
    𝛱<em><strong>𝜽</strong></em> that minimizes the expected costs over the training data. This decision pipeline is
    shown in <a href="#_bookmark10">Fig.</a> <a href="#_bookmark10">1</a>. Notice that there are two approximations of
    Problem (<a href="#_bookmark9">3</a>) made by Problem (<a href="#_bookmark14">4</a>). First, the policy is
    restricted to a hypothesis class that may not contain the true optimal policy. Second, the long-term expected costs
    are calculated over the empirical distribu-</p>
<p>tion P̂ 𝑁 rather than the true distribution P. Furthermore, Problem (<a href="#_bookmark14">4</a>)</p>
<p>focuses its policy optimization efforts on the overall performance (av-</p>
<p>eraged over different covariates) and disregards the question of making the policy achieve a good performance
    uniformly from one covariate to another.</p>
<blockquote>
    <p>min 𝜌(𝑓<em><strong><sub>𝜽</sub></strong></em>, P̂ 𝑁 ) + 𝛺(<em><strong>𝜽</strong></em>) with
        𝜌(𝑓<em><strong><sub>𝜽</sub></strong></em>, P̂ 𝑁 ) ∶= EP̂ 𝑁
        [D(𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>), <em><strong>𝒚</strong></em>)], (6)
    </p>
</blockquote>
<p>where D is a divergence function, e.g., negative log-likelihood and the regularization term
    𝛺(<em><strong>𝜽</strong></em>) controls the complexity of 𝑓<em><strong><sub>𝜽</sub></strong></em>. The
    conditional distribution can also take a non-parametric form, e.g. 𝑘-nearest neigh- bor, where
    <em><strong>𝜽</strong></em> then captures hyper-parameters of the non-parametric model (such as the number of
    neighbors) selected through a form of <span id="_bookmark15" class="anchor"></span>cross-validation scheme. The SLO
    training pipeline is shown in <a href="#_bookmark17">Fig. 3</a>.</p>
<blockquote>
    <p><strong>Definition 1</strong> (<em>Expected Value-Based Models</em>)<strong>.</strong> When the cost function</p>
</blockquote>
<p>𝑐(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>) of the decision model is linear in
    <em><strong>𝒚</strong></em>, the problem of estimating a conditional distribution reduces to finding the expected
    value of</p>
<blockquote>
    <p><img src="media/image9.png" /><img src="media/image12.png" style="width:0.51837in" />Finding the best
        parameterization of a contextual predictor that minimizes the downstream expected costs of the CSO solution can
        be formulated as the following problem:</p>
    <p>min 𝐻(𝑧<sup>∗</sup>(⋅, 𝑓<em><strong><sub>𝜽</sub></strong></em>), P̂ 𝑁 ) = min EP̂
        [𝑐(𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>, 𝑓<em><strong><sub>𝜽</sub></strong></em>),
        <em><strong>𝒚</strong></em>)]. <span id="_bookmark16" class="anchor"></span>(9)</p>
    <p><span id="_bookmark17" class="anchor"></span><strong>Fig. 3.</strong> SLO training pipeline for a given training
        example.</p>
</blockquote>
<p>the uncertain parameter given the covariates since ℎ(<em><strong>𝒛</strong></em>, P(<em><strong>𝒚
            𝒙</strong></em>)) = E <sub>(</sub> <sub>)</sub>[𝑐(<em><strong>𝒛</strong></em>,
    <em><strong>𝒚</strong></em>)] = 𝑐(<em><strong>𝒛</strong></em>, E <sub>(</sub>
    <sub>)</sub>[<em><strong>𝒚</strong></em>]). Training the contextual predictor, therefore, reduces to a mean
    regression problem over a parameterized function
    𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>). Specifically,</p>
<blockquote>
    <p>min 𝜌(𝑔 , P̂ ) + 𝛺(<em><strong>𝜽</strong></em>) 𝜌(𝑔 , P̂ ) ∶= E 𝑑(𝑔 (<em><strong>𝒙</strong></em>),
        <em><strong>𝒚</strong></em>) , <span id="_bookmark18" class="anchor"></span>(7)</p>
    <p><em><strong>𝜽</strong></em></p>
</blockquote>
<p>for some distance metric 𝑑, usually the mean squared errors. While the mean squared error is known to asymptotically
    retrieve 𝑔<em><strong>𝜽</strong></em>̂ (<em><strong>𝒙</strong></em>) = E <sub>(</sub>
    <sub>)</sub>[<em><strong>𝒚</strong></em>] as 𝑁 → ∞ under standard conditions, other distance metric or more
    general loss functions can also be used (<a href="#_bookmark119">Hastie et al.</a>, <a
        href="#_bookmark119">2009</a>). For any new covariate <em><strong>𝒙</strong></em>, an action is obtained using:
</p>
<blockquote>
    <p>𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>) ∈ argmin
        ℎ(<em><strong>𝒛</strong></em>, 𝛿<sub>𝑔</sub> <sub>(<em><strong>𝒙</strong></em>)</sub>) = argmin
        𝑐(<em><strong>𝒛</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>)), <span
            id="_bookmark19" class="anchor"></span>(8)</p>
</blockquote>
<p>The objective function in (<a href="#_bookmark16">9</a>) minimizes the expected cost of the policy over the empirical
    distribution. The policy induced by this training problem is thus optimal with respect to the predicted distribution
    and minimizes the average historical costs over the whole training data. <a href="#_bookmark22">Fig.</a> <a
        href="#_bookmark22">5</a> describes how the downstream cost is propagated by the predictive model during
    training. This training procedure necessarily comes at the price of heavier computations because an optimization
    model needs to be solved for each data point, and differentiation needs to be applied through an argmin operation.
</p>
<ol start="3" type="1">
    <li>
        <blockquote>
            <p><em>Summary</em></p>
        </blockquote>
    </li>
</ol>
<p>This section presented the main pipelines proposed in recent years to address contextual optimization. Although these
    pipelines all in- clude a learning component, they differ significantly in their specific structures and training
    procedures. Overall, there are several design choices that the decision-maker should make when tackling contextual
    optimization: (i) the type of loss function used during training, which defines whether an approach belongs to the
    decision rule (using ERM), sequential (minimizing the estimation error), or integrated paradigm (minimizing the
    downstream cost of the policy), (ii) the class of the</p>
<p><em><strong>𝒛</strong></em>∈</p>
<blockquote>
    <p><em><strong>𝜽 𝒛</strong></em>∈</p>
    <p>predictive model (e.g., linear, neural network, or random forest) and</p>
</blockquote>
<p>where, with a slight abuse of notation, 𝑧∗ now takes an estimator of the mean of the conditional distribution as the
    second argument, and with 𝛿<em><strong><sub>𝒚</sub></strong></em> being the Dirac distribution putting all its
    mass at <em><strong>𝒚</strong></em>. In the remainder of this survey, we refer to these approaches as
    <strong>expected value-based models</strong>, while the more general models that prescribe using a conditional
    distribution estimator (i.e. 𝑧∗(<em><strong>𝒙</strong></em>, 𝑓<em><strong><sub>𝜽</sub></strong></em>)) will be
    referred as a <strong>conditional distribution-based models</strong> when it is not clear from the context.</p>
<blockquote>
    <p><em>Integrated learning and optimization.</em> Sequential approaches ignore the mismatch between the prediction
        divergence D and the cost function</p>
</blockquote>
<p>𝑐(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>). Depending on the covariate, a small prediction error
    about</p>
<p>P(<em><strong>𝒚 𝒙</strong></em>) may have a large impact on the prescription. In integrated learning, the goal is
    to maximize the prescriptive performance of the induced policy. That is, we want to train the predictive component
    to minimize the task loss (i.e., the downstream costs incurred by the decision) as stated in (<a
        href="#_bookmark9">3</a>). The prescriptive performance may guide the estimation procedure toward a solution
    with higher MSE (or any distance metric) that nevertheless produces a nearly-optimal decision. This is illustrated
    in <a href="#_bookmark21">Fig. 4</a>.</p>
<p>its hyperparameters. Each design choice has its own inductive bias and may imply specific methodological challenges,
    especially for ILO. In general, it is a priori unclear what combination of choices will lead to the best performance
    with limited data; therefore, pipelines have to be evaluated experimentally.</p>
<p>In the following sections, we survey the recent literature corre- sponding to the three main frameworks for
    contextual optimization using the notation introduced so far, which is summarized in <a href="#_bookmark23">Table
        1</a>.</p>
<h2 id="decision-rule-optimization-1">Decision rule optimization</h2>
<p><span id="_bookmark20" class="anchor"></span>Decision rules obtained by solving the ERM in Problem (<a
        href="#_bookmark14">4</a>) minimize the cost of a policy on the task, that is, the downstream optimization
    problem. Policy-based approaches are especially efficient computation- ally at decision time since it suffices to
    evaluate the estimated policy. No optimization problem needs to be solved once the policy is trained. We defined the
    decision rule approach as employing a parameterized mapping
    𝜋<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>), e.g., linear policies (<a
        href="#_bookmark51">Ban &amp; Rudin</a>, <a href="#_bookmark51">2019</a>) or a neural network (<a
        href="#_bookmark172">Oroojlooyjadid et al.</a>, <a href="#_bookmark172">2020</a>). Since policies obtained using
    neural networks lack interpretability, linear decision rules are widely used.</p>
<ol type="1">
    <li>
        <blockquote>
            <p><em>Linear decision rules</em></p>
        </blockquote>
    </li>
</ol>
<p><a href="#_bookmark51">Ban and Rudin</a> (<a href="#_bookmark51">2019</a>) show that an approach based on the sample-
    average approximation (SAA) that disregards side information can lead to inconsistent decisions (i.e.,
    asymptotically suboptimal) for a newsvendor problem. Using linear decision rules (LDRs), they study two variants of
    the newsvendor problem with and without regulariza- tion:</p>
<blockquote>
    <p><img src="media/image13.png" style="width:2.8444in;height:1.84645in" />min 𝐻(𝜋, P̂ )+𝛺(𝜋) = min <span
            class="underline"> 1</span> ∑ ( − )+ + ( − )+ + ‖ ‖2</p>
</blockquote>
<p>𝑁</p>
<p>𝑢 𝑦 <em><strong>𝜽</strong></em><sup>⊤</sup><em><strong>𝒙</strong></em> 𝑜
    <em><strong>𝜽</strong></em><sup>⊤</sup><em><strong>𝒙</strong></em> 𝑦 𝜆</p>
<blockquote>
    <p>where 𝑢 and 𝑜 denote the per unit backordering (underage) and hold- ing (overage) costs. <a
            href="#_bookmark51">Ban and Rudin</a> (<a href="#_bookmark51">2019</a>) show that for a linear</p>
    <p><span id="_bookmark21" class="anchor"></span><sub>∗</sub> <sub>∗</sub> demand model, the generalization error for
        the ERM model scales as</p>
    <p><strong>Fig. 4.</strong> Predicting 𝑔<em><strong><sub>𝜽</sub></strong></em> (<em><strong>𝒙</strong></em>)
        results in the optimal action 𝑧 (<em><strong>𝒙</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em> ) = 𝑧
        (<em><strong>𝒙</strong></em>) whereas a small</p>
</blockquote>
<p>O(𝑑 ∕√𝑁) when there is no regularization and as O(𝑑 ∕(√𝑁𝜆)) with</p>
<blockquote>
    <p>𝑐(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>) ∶= −<em><strong>𝒚</strong></em>⊤
        <em><strong>𝒙</strong></em>, i.e., ℎ(<em><strong>𝒛</strong></em>,
        P(<em><strong>𝒚</strong></em>|<em><strong>𝒙</strong></em>)) =
        −E[<em><strong>𝒚</strong></em>|<em><strong>𝒙</strong></em>]⊤ <em><strong>𝒛</strong></em> (adapted from <a
            href="#_bookmark100">Elmachtoub and Grigas</a></p>
</blockquote>
<p>regularization. However, one needs to balance the trade-off between</p>
<blockquote>
    <p><img src="media/image14.png" style="width:0.51912in" /></p>
</blockquote>
<p><img src="media/image15.png" /></p>
<p><span id="_bookmark22" class="anchor"></span><strong>Fig. 5.</strong> ILO training pipeline for a given training
    example.</p>
<blockquote>
    <p><span id="_bookmark23" class="anchor"></span><strong>Table 1</strong></p>
</blockquote>
<p><span class="underline">Notation: distributions, variables, and operators.</span></p>
<table>
    <thead>
        <tr class="header">
            <th></th>
            <th>Domain</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr class="odd">
            <td>
                <blockquote>
                    <p>P</p>
                </blockquote>
            </td>
            <td>( × )</td>
            <td>True (unknown) joint distribution of (<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>)</td>
        </tr>
        <tr class="even">
            <td>
                <blockquote>
                    <p>P̂ 𝑁</p>
                </blockquote>
            </td>
            <td>( × )</td>
            <td>Joint empirical distribution of (<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>)</td>
        </tr>
        <tr class="odd">
            <td>
                <blockquote>
                    <p>𝛿<sub>𝑦</sub></p>
                </blockquote>
            </td>
            <td>()</td>
            <td>Dirac distribution that puts all of its weight on <em><strong>𝒚</strong></em></td>
        </tr>
        <tr class="even">
            <td>
                <blockquote>
                    <p><em><strong>𝒙</strong></em></p>
                </blockquote>
            </td>
            <td> ⊆ R𝑑<em><strong>𝒙</strong></em></td>
            <td>Contextual information</td>
        </tr>
        <tr class="odd">
            <td>
                <blockquote>
                    <p><em><strong>𝒚</strong></em></p>
                </blockquote>
            </td>
            <td>𝑌 ⊆ R𝑑<em><strong>𝒚</strong></em></td>
            <td>Uncertain parameters</td>
        </tr>
        <tr class="even">
            <td>
                <blockquote>
                    <p><em><strong>𝒛</strong></em></p>
                </blockquote>
            </td>
            <td> ⊆ R𝑑<em><strong>𝒛</strong></em></td>
            <td>A feasible action</td>
        </tr>
        <tr class="odd">
            <td>
                <blockquote>
                    <p><em><strong>𝜽</strong></em></p>
                </blockquote>
            </td>
            <td>𝛩</td>
            <td>Parameters of a prediction model</td>
        </tr>
        <tr class="even">
            <td>
                <blockquote>
                    <p><em><strong>𝜽</strong></em>̂</p>
                </blockquote>
            </td>
            <td>𝛩</td>
            <td>Optimal parameter value that minimizes the estimation error</td>
        </tr>
        <tr class="odd">
            <td>
                <blockquote>
                    <p>𝑐(<em><strong>𝒛</strong></em>, <em><strong>𝒚</strong></em>)</p>
                </blockquote>
            </td>
            <td>R</td>
            <td>Cost of an action <em><strong>𝒛</strong></em> under <em><strong>𝒚</strong></em></td>
        </tr>
        <tr class="even">
            <td>
                <blockquote>
                    <p>ℎ(<em><strong>𝒛</strong></em>, Q<em><strong><sub>𝒚</sub></strong></em> )</p>
                </blockquote>
            </td>
            <td>R</td>
            <td>Expected cost of an action <em><strong>𝒛</strong></em> under Q<em><strong><sub>𝒚</sub></strong></em>
                (a distribution over <em><strong>𝒚</strong></em>)</td>
        </tr>
        <tr class="odd">
            <td>
                <blockquote>
                    <p>𝐻(𝜋, Q)</p>
                </blockquote>
            </td>
            <td>R</td>
            <td>Expected cost of a policy 𝜋 under Q (a distribution over (<em><strong>𝒙</strong></em>,
                <em><strong>𝒚</strong></em>))</td>
        </tr>
        <tr class="even">
            <td>
                <blockquote>
                    <p>𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>)</p>
                </blockquote>
            </td>
            <td>()</td>
            <td>Estimate of the conditional distribution of <em><strong>𝒚</strong></em> given
                <em><strong>𝒙</strong></em></td>
        </tr>
        <tr class="odd">
            <td>
                <blockquote>
                    <p>𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>)</p>
                </blockquote>
            </td>
            <td>R𝑑<em><strong><sub>𝒚</sub></strong></em></td>
            <td>Estimate of the conditional expectation of <em><strong>𝒚</strong></em> given
                <em><strong>𝒙</strong></em></td>
        </tr>
    </tbody>
</table>
<blockquote>
    <p>𝜋∗(<em><strong>𝒙</strong></em>)  Optimal solution of CSO under true conditional distribution P(<em><strong>𝒚
                𝒙</strong></em>)</p>
    <p>𝜋<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>)  Action prescribed by a policy
        parameterized by <em><strong>𝜽</strong></em> for context <em><strong>𝒙</strong></em></p>
    <p>𝑧∗(<em><strong>𝒙</strong></em>)  Optimal solution to the CSO problem under the true conditional distribution
        P(<em><strong>𝒚 𝒙</strong></em>)</p>
    <p>𝑧∗(<em><strong>𝒙</strong></em>, 𝑓<em><strong><sub>𝜽</sub></strong></em>)  Optimal solution to the CSO
        problem under the conditional distribution
        𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>)</p>
    <p>𝑧∗(<em><strong>𝒙</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>)  Optimal solution to the CSO
        problem under the Dirac distribution 𝛿<sub>𝑔</sub> <sub>(<em><strong>𝒙</strong></em>)</sub></p>
    <p>𝜌(𝑓<em><strong><sub>𝜽</sub></strong></em>, P̂ 𝑁 ) R Expected prediction error for distribution model
        𝑓<em><strong><sub>𝜽</sub></strong></em> based on empirical distribution P̂ 𝑁</p>
    <p>𝜌(𝑔<em><strong><sub>𝜽</sub></strong></em>, P̂ 𝑁 ) R Expected prediction error for point prediction model
        𝑔<em><strong><sub>𝜽</sub></strong></em> based on empirical distribution P̂ 𝑁</p>
</blockquote>
<p>performance from using LDRs. <a href="#_bookmark51">Ban and Rudin</a> (<a href="#_bookmark51">2019</a>) consider un-
    constrained problems because it is difficult to ensure the feasibility</p>
<p>given below:</p>
<blockquote>
    <p>𝐿</p>
</blockquote>
<p>of policies and maintain computational tractability using the ERM approach.</p>
<blockquote>
    <p><a href="#_bookmark58">Bertsimas and Kallus</a> (<a href="#_bookmark58">2020</a>) present a general theory for
        gener-</p>
    <p>𝜑 ∶  → R ∃ 𝐿 ∈ N, <em><strong>𝒗</strong></em><sub>1</sub>, <em><strong>𝒗</strong></em><sub>2</sub>, … ,
        <em><strong>𝒗</strong></em><sub>𝐿</sub> ∈ , 𝜑(<em><strong>𝒙</strong></em>) =
        𝑎<sub>𝑙</sub>𝐾(<em><strong>𝒗</strong></em><sub>𝑙</sub>, <em><strong>𝒙</strong></em>),
        ∀<em><strong>𝒙</strong></em> ∈  ,</p>
    <p>𝑙=1</p>
    <p>with the inner product of 𝜑<sub>1</sub>(<em><strong>𝒙</strong></em>) = ∑𝐿1 𝑎𝑖
        𝐾(<em><strong>𝒗</strong></em>𝑖 , <em><strong>𝒙</strong></em>) and
        𝜑<sub>2</sub>(<em><strong>𝒙</strong></em>) =</p>
</blockquote>
<p>alization bounds of decision rules based on Rademacher complexity</p>
<blockquote>
    <p>∑𝐿2 𝑎<sup>𝑗</sup> 𝐾(<em><strong>𝒗</strong></em><sup>𝑗</sup> , <em><strong>𝒙</strong></em>) given by:</p>
    <p>𝑖=1 1 1</p>
</blockquote>
<p>that goes beyond LDR, although their main examples in this context pertain to LDR. Unfortunately, LDRs may not be
    asymptotically optimal in general. To generalize LDRs, one can consider decision rules that</p>
<blockquote>
    <p>𝑗=1 2 2</p>
    <p>𝐿1 𝐿2</p>
</blockquote>
<p><a href="#_bookmark59">⟨</a>𝜑<sub>1</sub>, 𝜑<sub>2</sub><a href="#_bookmark59">⟩</a> = 𝑎<sup>𝑖</sup>
    𝑎<sup>𝑗</sup> 𝐾(<em><strong>𝒗</strong></em>𝑖 , <em><strong>𝒗</strong></em><sup>𝑗</sup> ).</p>
<p><a href="#_bookmark51">2019</a>). It is also possible to lift the covariate vector to a reproducing kernel Hilbert
    space (RKHS, <a href="#_bookmark46">Aronszajn</a>, <a href="#_bookmark46">1950</a>), as seen in the next section.
</p>
<ol start="2" type="1">
    <li>
        <blockquote>
            <p><em>RKHS-based decision rules</em></p>
        </blockquote>
    </li>
</ol>
<p><a href="#_bookmark59">Bertsimas and Koduri</a> (<a href="#_bookmark59">2022</a>) approximate the optimal policy with
    a linear policy in the RKHS, i.e. 𝜋 (<em><strong>𝒙</strong></em>) ∶= 𝜑, 𝐾(<em><strong>𝒙</strong></em>, ⋅) when
    𝑑 = 1, and show using the representer theorem (see Theorem 9 in <a href="#_bookmark122">Hofmann et al.</a>, <a
        href="#_bookmark122">2008</a>) that the solution of the following regularized problem:</p>
<blockquote>
    <p>min 𝐻(𝜋<sub>𝜑</sub>, P̂ 𝑁 ) + 𝜆‖𝜑‖2,</p>
</blockquote>
<p>𝜑∈</p>
<p>takes the form 𝜋∗(<em><strong>𝒙</strong></em>) = ∑𝑁</p>
<blockquote>
    <p>2</p>
    <p>𝐾(<em><strong>𝒙</strong></em><sub>𝑖</sub>, <em><strong>𝒙</strong></em>)𝑎∗. Hence, this reduces the decision
    </p>
</blockquote>
<p>To obtain decision rules that are more flexible than linear ones with respect to <em><strong>𝒙</strong></em>, it is
    possible to lift the covariate vector to an RKHS in which LDRs might achieve better performance. Let 𝐾 ∶  ×  → R
    be the symmetric positive definite kernel associated with the chosen</p>
<p>rule problem to:</p>
<blockquote>
    <p>min 𝐻</p>
</blockquote>
<p>(∑𝑁</p>
<blockquote>
    <p>𝑖=1</p>
    <p>𝐾(<em><strong>𝒙</strong></em><sub>𝑖</sub>, ⋅)𝑎<sub>𝑖</sub>, P̂ 𝑁 )</p>
</blockquote>
<p>+ 𝜆</p>
<blockquote>
    <p>𝑁 𝑁</p>
    <p>𝐾(<em><strong>𝒙</strong></em><sub>𝑖</sub>, <em><strong>𝒙</strong></em><sub>𝑗</sub>
        )𝑎<sub>𝑖</sub>𝑎<sub>𝑗</sub> .</p>
</blockquote>
<p>RKHS, e.g., the Gaussian kernel 𝐾(<em><strong>𝒙</strong></em> , <em><strong>𝒙</strong></em> ) ∶=
    exp(−‖<em><strong>𝒙</strong></em> − <em><strong>𝒙</strong></em> ‖2∕(2𝜎2)).</p>
<blockquote>
    <p><em><strong>𝒂</strong></em>∈R𝑁</p>
    <p>𝑖=1</p>
    <p>𝑖=1 𝑗=1</p>
</blockquote>
<p>This RKHS approach appeared earlier in <a href="#_bookmark51">Ban and Rudin</a> (<a href="#_bookmark51">2019</a>) and
    <a href="#_bookmark53">Bazier-Matte and Delage</a> (<a href="#_bookmark53">2020</a>) who respectively study the
    data-</p>
<p>extension to the scenario-based optimal policy. Mathematically,</p>
<blockquote>
    <p>min sup {𝐻(𝜋, Q) ∶ (Q, P̂ 𝑁 ) ≤ 𝑟} ≡ min sup {𝐻(𝜋, Q) ∶ (Q, P̂ 𝑁 ) ≤ 𝑟},</p>
</blockquote>
<p>driven single item newsvendor and single risky asset portfolio problems</p>
<blockquote>
    <p>𝜋∈𝛱 Q∈(×)</p>
    <p>𝜋∶̂→ Q∈(̂× )</p>
</blockquote>
<p><a href="#_bookmark59">and</a> establish bounds on the out-of-sample performance. <a
        href="#_bookmark59">Bertsimas</a></p>
<p>where ̂ ∶= ∪𝑁 {<em><strong>𝒙</strong></em> } and (̂ × ) is the set of all distribution</p>
<p><a href="#_bookmark59">and Koduri</a> (<a href="#_bookmark59">2022</a>) show the asymptotic optimality of RKHS-based
    policies. <a href="#_bookmark170">Notz and Pibernik</a> (<a href="#_bookmark170">2022</a>) study a two-stage
    capacity plan- ning problem with multivariate demand and vector-valued capacity decisions for which the underlying
    demand distribution is difficult to estimate in practice. Similar to <a href="#_bookmark53">Bazier-Matte and
        Delage</a> (<a href="#_bookmark53">2020</a>), the authors optimize over policies that are linear in the RKHS
    associated with the Gaussian kernel and identify generalization error bounds. For large dimensional problems, this
    kernel is shown to have a slow convergence rate, and as a result, the authors propose instead using a data-dependent
    random forest kernel.</p>
<ol start="3" type="1">
    <li>
        <blockquote>
            <p><em>Non-linear decision rules</em></p>
        </blockquote>
    </li>
</ol>
<p>Many non-linear decision rule approaches have been experimented <a href="#_bookmark226">with.</a> <a
        href="#_bookmark124">Huber et al.</a> (<a href="#_bookmark124">2019</a>), <a href="#_bookmark172">Oroojlooyjadid
        et al.</a> (<a href="#_bookmark172">2020</a>), and <a href="#_bookmark226">Zhang</a> <a href="#_bookmark226">and
        Gao</a> (<a href="#_bookmark226">2017</a>) study the value of training a DNN to learn the or- dering policy of a
    newsvendor problem. It is well-known that neural networks enjoy the universal approximation property; that is, they
    can approximate any continuous function arbitrarily well (<a href="#_bookmark86">Cybenko</a>, <a
        href="#_bookmark86">1989</a>; <a href="#_bookmark150">Lu et al.</a>, <a href="#_bookmark150">2017</a>). For
    constrained problems, one can use softmax as the final layer to ensure that decisions lie in a simplex, e.g., in a
    portfolio optimization problem (<a href="#_bookmark229">Zhang et al.</a>, <a href="#_bookmark229">2021</a>). Yet, in
    general, the output of a neural network might not naturally land in the feasible space . To circumvent this issue,
    <a href="#_bookmark75">Chen et al.</a> (<a href="#_bookmark75">2023</a>) introduce an application-specific
    differentiable repair layer that projects the solution back to feasibility. <a href="#_bookmark187">Rychener et
        al.</a> (<a href="#_bookmark187">2023</a>) show that the decision rule obtained by using the stochastic gradient
    descent (SGD) method to train DNN-based policies approximately minimizes the Bayesian posterior loss.</p>
<p>Exploiting the fact that the optimal solution of a newsvendor prob- lem is a quantile of the demand distribution, <a
        href="#_bookmark124">Huber et al.</a> (<a href="#_bookmark124">2019</a>) further train an additive ensemble of
    decision trees using quantile regression to produce the ordering decision. They test these algorithms on a real-
    world data set from a large German bakery chain. <a href="#_bookmark57">Bertsimas et al.</a> (<a
        href="#_bookmark57">2019</a>), <a href="#_bookmark80">Ciocan and Mišić</a> (<a href="#_bookmark80">2022</a>),
    and <a href="#_bookmark134">Keshavarz</a> (<a href="#_bookmark134">2022</a>) optimize de- cision tree-based decision
    rules to address the multi-item newsvendor, treatment planning, and optimal stopping problems, respectively. A <a
        href="#_bookmark195">tutorial</a> on DNN-based decision rule optimization is given in <a
        href="#_bookmark195">Shlezinger</a> <a href="#_bookmark195">et al.</a> (<a href="#_bookmark195">2022</a>).</p>
<p><a href="#_bookmark228">Zhang et al.</a> (<a href="#_bookmark228">2023</a>) introduce piecewise-affine decision rules
    and provide non-asymptotic and asymptotic consistency results for uncon- strained and constrained problems,
    respectively. The policy is learned through a stochastic majorization-minimization algorithm, and experi- ments on a
    constrained newsvendor problem show that piecewise-affine decision rules can outperform the RKHS-based policies.</p>
<ol start="4" type="1">
    <li>
        <blockquote>
            <p><em>Distributionally robust decision rules</em></p>
        </blockquote>
    </li>
</ol>
<blockquote>
    <p>Most of the literature on policy learning assumes a parametric form</p>
</blockquote>
<p>𝛱<em><strong>𝜽</strong></em> for the policy. A notable exception is <a href="#_bookmark230">Zhang et al.</a> (<a
        href="#_bookmark230">2023</a>), which studies a distributionally robust contextual newsvendor problem under</p>
<blockquote>
    <p>supported on ̂ 𝑖=1 . 𝑖</p>
</blockquote>
<p>Prior to the work of <a href="#_bookmark230">Zhang et al.</a> (<a href="#_bookmark230">2023</a>), many have
    considered dis- tributionally robust versions of the decision rule optimization problem <a
        href="#_bookmark60">in</a> the non-contextual setting (<a href="#_bookmark227">Yanıkoğlu et al.</a>, <a
        href="#_bookmark227">2019</a>) while <a href="#_bookmark60">Bertsimas</a> <a href="#_bookmark60">et al.</a> (<a
        href="#_bookmark60">2023</a>) use LDRs to solve dynamic optimization problems with side information.</p>
<p><a href="#_bookmark225">Yang et al.</a> (<a href="#_bookmark225">2023</a>) point out that the perturbed distributions
    in the Wasserstein ambiguity set might have a different conditional in- formation structure than the estimated
    conditional distribution. They introduce a distributionally robust optimization (DRO) problem with causal transport
    metric (<a href="#_bookmark48">Backhoff et al.</a>, <a href="#_bookmark48">2017</a>; <a
        href="#_bookmark139">Lassalle</a>, <a href="#_bookmark139">2018</a>) that places an additional causality
    constraint on the transport plan com- pared to the Wasserstein metric. Tractable reformulations of the DRO problem
    are given under LDRs as well as for one-dimensional convex cost functions. <a href="#_bookmark187">Rychener et
        al.</a> (<a href="#_bookmark187">2023</a>) present a Bayesian interpretation of decision rule optimization using
    SGD and show that their algorithm provides an unbiased estimate of the worst-case objective function of a DRO
    problem as long as a uniqueness condition is satisfied. The authors note that the Wasserstein ambiguity set violates
    this condition and thus use the Kullback–Leibler (KL) divergence (<a href="#_bookmark138">Kullback &amp;
        Leibler</a>, <a href="#_bookmark138">1951</a>) to train the models.</p>
<h2 id="sequential-learning-and-optimization">Sequential learning and optimization</h2>
<p><span id="_bookmark24" class="anchor"></span>In reviewing contextual optimization approaches that are based on SLO,
    we distinguish two settings: (i) a more traditional setting where the conditional distribution is learned from data
    and used directly in the optimization model, and (ii) a setting that attempts to produce decisions that are robust
    to model misspecification. An overview of the methods presented in this section is given in <a
        href="#_bookmark25">Table 2</a>.</p>
<ol type="1">
    <li>
        <blockquote>
            <p><em>Learning conditional distributions</em></p>
        </blockquote>
    </li>
</ol>
<p>Most of the recent literature has employed discrete models for
    𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>). This is first motivated from a
    computational perspective by the fact that the CSO Problem (<a href="#_bookmark12">5</a>) is easier to solve in this
    setting. In fact, more often than not, the CSO under a continuous distribution needs to be first replaced by its SAA
    to be solved (<a href="#_bookmark194">Shapiro et al.</a>, <a href="#_bookmark194">2014</a>). From a statistical
    viewpoint, it can also be difficult to assess the probability of outcomes that are not present in the data set, thus
    justifying fixing the support of <em><strong>𝒚</strong></em> to its observed values.</p>
<ol type="1">
    <li>
        <blockquote>
            <p><em>Residual-based distribution</em></p>
        </blockquote>
    </li>
</ol>
<p>A first approach (found in <a href="#_bookmark91">Deng &amp; Sen</a>, <a href="#_bookmark91">2022</a>; <a
        href="#_bookmark130">Kannan et al.</a>, <a href="#_bookmark130">2020</a>; <a href="#_bookmark191">Sen &amp;
        Deng</a>, <a href="#_bookmark191">2017</a>) is to use the errors of a trained regression model (i.e., its
    residuals) to construct conditional distributions. Let 𝑔<em><strong>𝜽</strong></em>̂ be a regression model trained
    to predict the response <em><strong>𝒚</strong></em> from the covariate <em><strong>𝒙</strong></em>, thus
    minimizing an estimation error 𝜌 as in (<a href="#_bookmark18">7</a>). The residual error of sample 𝑖 is given by
    <em><strong>𝝐</strong></em><sub>𝑖</sub> = <em><strong>𝒚</strong></em><sub>𝑖</sub>
    −𝑔<em><strong>𝜽</strong></em>̂ (<em><strong>𝒙</strong></em><sub>𝑖</sub>). The set of residuals measured on the
</p>
<p>historical data, {<em><strong>𝝐</strong></em><sub>𝑖</sub>}𝑁 , is then used to form the conditional distribution,
</p>
<p>the type-1 Wasserstein ambiguity set without assuming an explicit structure on the policy class. The type-𝑝
    Wasserstein distance (earth mover’s distance) between distributions P<sub>1</sub> and P<sub>2</sub> is given by:</p>
<blockquote>
    <p>𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) = PER</p>
</blockquote>
<p>𝑁</p>
<p>(<em><strong>𝒙</strong></em>) ∶=</p>
<blockquote>
    <p>𝑁 𝑖=1</p>
</blockquote>
<p>𝛿proj (𝑔<em><strong>𝜽</strong></em>̂ (<em><strong>𝒙</strong></em>)+<em><strong>𝝐</strong></em>𝑖),</p>
<blockquote>
    <p>𝑊 (P , P ) = inf ( ‖𝑦 𝑝</p>
</blockquote>
<p>1</p>
<blockquote>
    <p>– 𝑦 ‖𝑝𝛾(𝑑𝑦 , 𝑑𝑦 ) ,</p>
    <p>with proj<sub></sub> denoting the projection on the support . The residual-based</p>
    <p>CSO (<strong>rCSO</strong>) problem is now given by:</p>
</blockquote>
<h1 id="section"> </h1>
<p>where 𝛾 is a joint distribution of 𝑦<sub>1</sub> and 𝑦<sub>2</sub> with marginals P<sub>1</sub> and P<sub>2</sub>.
</p>
<p>The distributionally robust model in <a href="#_bookmark230">Zhang et al.</a> (<a href="#_bookmark230">2023</a>)
    avoids the degeneracies of ERM for generic 𝛱 by defining an optimal ‘‘Shapley’’</p>
<blockquote>
    <p><em><strong>𝒛</strong></em>∈</p>
</blockquote>
<p>The advantage of residual-based methods is that they can be applied in conjunction with any trained regression model.
    <a href="#_bookmark50">Ban et al.</a> (<a href="#_bookmark50">2019</a>)</p>
<blockquote>
    <p><span id="_bookmark25" class="anchor"></span><strong>Table 2</strong></p>
    <p><span class="underline">Overview of contextual optimization papers in the SLO framework.</span> <span
            class="underline">Method</span> <span class="underline">Regularization</span> <span
            class="underline">Learning model</span> <strong>rCSO wSAA EVB</strong> Reg. CSO DRO General Linear Kernel
        kNN DT RF</p>
    <p><a href="#_bookmark118">Hannah et al.</a> (<a href="#_bookmark118">2010</a>) ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ <a
            href="#_bookmark109">Ferreira et al.</a> (<a href="#_bookmark109">2016</a>) ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ <a
            href="#_bookmark50">Ban et al.</a> (<a href="#_bookmark50">2019</a>) ✔ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ <a
            href="#_bookmark74">Chen and Paschalidis</a> (<a href="#_bookmark74">2019</a>) ✗ ✔ ✗ ✗ ✔ ✗ ✗ ✗ ✔ ✗ ✗ <a
            href="#_bookmark58">Bertsimas and Kallus</a> (<a href="#_bookmark58">2020</a>) ✗ ✔ ✗ ✗ ✗ ✗ ✔ ✗ ✔ ✔ ✔ <a
            href="#_bookmark130">Kannan et al.</a> (<a href="#_bookmark130">2020</a>) ✔ ✗ ✗ ✗ ✗ ✔ ✔ ✔ ✔ ✔ ✔ <a
            href="#_bookmark131">Kannan et al.</a> (<a href="#_bookmark131">2021</a>) ✔ ✗ ✗ ✗ ✔ ✔ ✔ ✔ ✔ ✔ ✔ <a
            href="#_bookmark144">Liu et al.</a> (<a href="#_bookmark144">2021</a>) ✗ ✗ ✔ ✗ ✗ ✗ ✔ ✗ ✗ ✔ ✗ <a
            href="#_bookmark197">Srivastava et al.</a> (<a href="#_bookmark197">2021</a>) ✗ ✔ ✗ ✔ ✗ ✗ ✗ ✔ ✗ ✗ ✗ <a
            href="#_bookmark214">Wang et al.</a> (<a href="#_bookmark214">2021</a>) ✗ ✔ ✗ ✗ ✔ ✗ ✗ ✔ ✗ ✗ ✗ <a
            href="#_bookmark62">Bertsimas and Van Parys</a> (<a href="#_bookmark62">2022</a>) ✗ ✔ ✗ ✗ ✔ ✗ ✗ ✔ ✔ ✗ ✗ <a
            href="#_bookmark91">Deng and Sen</a> (<a href="#_bookmark91">2022</a>) ✔ ✗ ✗ ✗ ✗ ✔ ✔ ✔ ✔ ✔ ✔ <a
            href="#_bookmark103">Esteban-Pérez and Morales</a> (<a href="#_bookmark103">2022</a>) ✗ ✔ ✗ ✗ ✔ ✗ ✗ ✔ ✔ ✗ ✗
        <a href="#_bookmark132">Kannan et al.</a> (<a href="#_bookmark132">2022</a>) ✔ ✗ ✗ ✗ ✔ ✔ ✔ ✔ ✔ ✔ ✔ <a
            href="#_bookmark141">Lin et al.</a> (<a href="#_bookmark141">2022</a>) ✗ ✔ ✗ ✔ ✗ ✗ ✗ ✗ ✔ ✔ ✔ <a
            href="#_bookmark167">Nguyen et al.</a> (<a href="#_bookmark167">2021</a>) ✗ ✔ ✗ ✗ ✔ ✗ ✗ ✗ ✔ ✗ ✗ <a
            href="#_bookmark170">Notz and Pibernik</a> (<a href="#_bookmark170">2022</a>) ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✔ ✗ <a
            href="#_bookmark231">Zhu et al.</a> (<a href="#_bookmark231">2022</a>) ✗ ✗ ✔ ✗ ✔ ✔ ✔ ✔ ✔ ✔ ✔ <a
            href="#_bookmark174">Perakis et al.</a> (<a href="#_bookmark174">2023</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✔ ✗ ✗ ✗ ✗</p>
    <p>Note: we distinguish between regularized CSO models (Reg. CSO) and DRO-based regularization; an approach is
        classified as ‘‘General’’ if its learning model is not restricted to specific classes.</p>
</blockquote>
<p>and <a href="#_bookmark91">Deng and Sen</a> (<a href="#_bookmark91">2022</a>) build conditional distributions for
    two- stage and multi-stage CSO problems using the residuals obtained from</p>
<p><a href="#_bookmark163">1964</a>; <a href="#_bookmark217">Watson</a>, <a href="#_bookmark217">1964</a>) employs a
    weight function:</p>
<blockquote>
    <p>𝑤KDE(<em><strong>𝒙</strong></em>) ∶= 𝐾 ((<em><strong>𝒙</strong></em> −
        <em><strong>𝒙</strong></em><sub>𝑖</sub>)∕<em><strong>𝜽</strong></em>) )</p>
</blockquote>
<p>train the regression model 𝑔<em><strong><sub>𝜽</sub></strong></em>, and to measure the residuals
    <em><strong>𝝐</strong></em><sub>𝑖</sub>. This can lead to an underestimation of the distribution of the residual
    error. To remove this bias, <a href="#_bookmark130">Kannan et al.</a> (<a href="#_bookmark130">2020</a>) propose a
    leave-one-out model (also known as <em>jackknife</em>). They measure the residuals as
    <em><strong>𝝐</strong></em>̃<sub>𝑖</sub> =</p>
<p><em><strong>𝒚</strong></em><sub>𝑖</sub> −𝑔<em><strong>𝜽</strong></em>̂−𝑖
    (<em><strong>𝒙</strong></em><sub>𝑖</sub>), where <em><strong>𝜽</strong></em>̂−𝑖 is trained using all the
    historical data except the</p>
<p>𝑖th sample (<em><strong>𝒙</strong></em><sub>𝑖</sub>, <em><strong>𝒚</strong></em><sub>𝑖</sub>). This idea can
    also be applied to the heteroskedastic</p>
<p>case studied in <a href="#_bookmark131">Kannan et al.</a> (<a href="#_bookmark131">2021</a>), where the following
    conditional distribution is obtained by first estimating the conditional covariance</p>
<p>matrix 𝑄̂ (<em><strong>𝒙</strong></em>) (a positive definite matrix for almost every <em><strong>𝒙</strong></em>)
    and then</p>
<blockquote>
    <p>forming the residuals <em><strong>𝝐</strong></em>̂<sub>𝑖</sub> = [𝑄̂
        (<em><strong>𝒙</strong></em><sub>𝑖</sub>)]−1(<em><strong>𝒚</strong></em><sub>𝑖</sub> −
        𝑔<em><strong>𝜽</strong></em>̂ (<em><strong>𝒙</strong></em><sub>𝑖</sub>)):</p>
</blockquote>
<p>where 𝐾 is a kernel function and <em><strong>𝜽</strong></em> denotes its bandwidth parameter. Different kernel
    functions can be used, e.g., the Gaussian kernel defined as 𝐾(<em><strong>𝜟</strong></em>) ∝ exp(−
    <em><strong>𝜟</strong></em> <sup>2</sup>). <a href="#_bookmark118">Hannah et al.</a> (<a
        href="#_bookmark118">2010</a>) also use a Bayesian approach that exploits the Dirichlet process mixture to
    assign sample weights.</p>
<p><em>Weights based on random forest.</em> Weights can also be designed based on random forest regressors (<a
        href="#_bookmark58">Bertsimas &amp; Kallus</a>, <a href="#_bookmark58">2020</a>). In its simplest setting, the
    weight function of a decision tree regressor is given by:</p>
<blockquote>
    <p><sub>𝑡</sub>( ) ∶= 1 [𝑡(<em><strong>𝒙</strong></em>) =  (<em><strong>𝒙</strong></em><sub>𝑖</sub>)]</p>
</blockquote>
<h1 id="section-1"> </h1>
<p><span class="underline">1</span> ∑</p>
<p>𝑖 𝑁</p>
<p>𝑗=1</p>
<p>1 [𝑡(<em><strong>𝒙</strong></em>) =  (<em><strong>𝒙</strong></em><sub>𝑗</sub> )]</p>
<blockquote>
    <p>𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) ∶= 𝑁</p>
</blockquote>
<ol start="2" type="1">
    <li>
        <blockquote>
            <p><span id="bookmark26" class="anchor"></span><em>Weight-based distribution</em></p>
        </blockquote>
    </li>
</ol>
<blockquote>
    <p>𝑖=1</p>
</blockquote>
<p>𝛿proj (𝑔<em><strong>𝜽</strong></em>̂ (<em><strong>𝒙</strong></em>)+𝑄̂
    (<em><strong>𝒙</strong></em>)<em><strong>𝝐</strong></em>̂𝑖 ).</p>
<p>where  (<em><strong>𝒙</strong></em>) denotes the terminal node of tree 𝑡 that contains covari-</p>
<p>ate <em><strong>𝒙</strong></em>. Thus, a decision tree assigns equal weights to all the historical</p>
<p>samples that end in the same leaf node as <em><strong>𝒙</strong></em>. The random forest weight</p>
<p>A typical approach for formulating the CSO problem is to assign weights to the observations of the uncertain
    parameters in the historical <a href="#_bookmark58">data</a> and solving the weighted SAA problem
    (<strong>wSAA</strong>) given by <a href="#_bookmark58">Bertsimas</a></p>
<p>function generalizes this idea over many random decision trees. Its weight function is defined as:</p>
<blockquote>
    <p>𝑤<sup>RF</sup>(<em><strong>𝒙</strong></em>) ∶= <span class="underline">1</span> ∑
        𝑤<sup>𝑡</sup>(<em><strong>𝒙</strong></em>),</p>
    <p>(𝚠𝚂𝙰𝙰) min</p>
</blockquote>
<p>𝑧∈</p>
<blockquote>
    <p>(<em><strong>𝒛</strong></em>,</p>
    <p>𝑁</p>
</blockquote>
<p>𝑖=1</p>
<p>𝑤<sub>𝑖</sub>(<em><strong>𝒙</strong></em>)𝛿<em><strong><sub>𝒚</sub></strong></em>𝑖 )</p>
<p>. <span id="_bookmark27" class="anchor"></span>(11)</p>
<blockquote>
    <p>where 𝑤𝑡 is the weight function of tree 𝑡. Random forests are typi- cally trai<sup>𝑖</sup>ned in order to
        perform an inference task, e.g. regression, or classification, but can also be used and interpreted as
        non-parametric</p>
</blockquote>
<p>In this case, the conditional distribution 𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) =
    ∑𝑁 𝑤<sub>𝑖</sub>(<em><strong>𝒙</strong></em>)𝛿<em><strong><sub>𝒚</sub></strong></em> is fully</p>
<p>conditional density estimators.</p>
<p>determined by the function used to assign a weigh<sup>𝑖=</sup>t <sup>1</sup>to the h<sup>𝑖</sup>istorical samples.
    Different approaches have been proposed to determine the sample weights with ML methods.</p>
<p><em>Weights based on proximity.</em> Sample weights can be assigned based on the distance between a covariate
    <em><strong>𝒙</strong></em> and each historical sample <em><strong>𝒙</strong></em><sub>𝑖</sub>. For instance, a
    𝑘-nearest neighbor (kNN) estimation gives equal weight to the 𝑘 closest samples in the data set and zero weight to
    all the other samples. That is, 𝑤kNN(<em><strong>𝒙</strong></em>) ∶=
    (1∕𝑘)1[<em><strong>𝒙</strong></em><sub>𝑖</sub> ∈ 𝑘(<em><strong>𝒙</strong></em>)], where 
    (<em><strong>𝒙</strong></em>)</p>
<p>denotes the set of 𝑘 nearest neighbors of <em><strong>𝒙</strong></em> and 1[⋅] is the indicator func-</p>
<p>tion. Even though it may appear simple, this non-parametric approach benefits from asymptotic consistency guarantees
    on its prescriptive per- formance. Another method to determine sample weights is to use kernel <a
        href="#_bookmark197">density</a> estimators (<a href="#_bookmark51">Ban &amp; Rudin</a>, <a
        href="#_bookmark51">2019</a>; <a href="#_bookmark118">Hannah et al.</a>, <a href="#_bookmark118">2010</a>; <a
        href="#_bookmark197">Srivastava</a> <a href="#_bookmark197">et al.</a>, <a href="#_bookmark197">2021</a>). The
    Nadaraya–Watson (NW) kernel estimator (<a href="#_bookmark163">Nadaraya</a>,</p>
<p><a href="#_bookmark58">Bertsimas and Kallus</a> (<a href="#_bookmark58">2020</a>) provide conditions for the asymp-
    totic optimality and consistency of prescriptions obtained by solving Problem (<a href="#_bookmark27">11</a>) with
    the weights functions given by kNN, NW kernel estimator, and local linear regression.</p>
<ol start="3" type="1">
    <li>
        <blockquote>
            <p><em>Expected value-based models</em></p>
        </blockquote>
    </li>
</ol>
<p>As described in <a href="#_bookmark15">Definition</a> <a href="#_bookmark15">1</a>, when the cost function is linear,
    the training pipeline of SLO reduces to conditional mean estimation. For instance, <a href="#_bookmark109">Ferreira
        et al.</a> (<a href="#_bookmark109">2016</a>) train regression trees to forecast daily expected sales for
    different product categories in an inventory and pricing problem for an online retailer. Alternatively, one may
    attempt to approximate the conditional density
    𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) using a point prediction</p>
<p>𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>). For example, <a href="#_bookmark144">Liu et
        al.</a> (<a href="#_bookmark144">2021</a>) study a last-mile delivery problem,</p>
<p>where customer orders are assigned to drivers, and replace the condi- tional distribution of the stochastic travel
    time with a point predictor</p>
<p>(e.g. a linear regression or decision tree) that accounts for the number of stops, total distance of the trip, etc.
</p>
<ol start="2" type="1">
    <li>
        <blockquote>
            <p><em>Regularization and distributionally robust optimization</em></p>
        </blockquote>
    </li>
</ol>
<p>While non-parametric conditional density estimation methods ben- <a href="#_bookmark170">efit</a> from asymptotic
    consistency (<a href="#_bookmark58">Bertsimas &amp; Kallus</a>, <a href="#_bookmark58">2020</a>; <a
        href="#_bookmark170">Notz &amp;</a> <a href="#_bookmark170">Pibernik</a>, <a href="#_bookmark170">2022</a>),
    they are known to produce overly optimistic poli- cies when the size of the covariate vector is large (see
    discussions in <a href="#_bookmark62">Bertsimas &amp; Van Parys</a>, <a href="#_bookmark62">2022</a>). To circumvent
    this issue, authors have proposed to either regularize the CSO problem (<a href="#_bookmark141">Lin et al.</a>, <a
        href="#_bookmark141">2022</a>; <a href="#_bookmark197">Srivastava et al.</a>, <a href="#_bookmark197">2021</a>)
    or to cast it as a DRO problem. In the latter case, one attempts to minimize the worst-case expected cost over the
    set of distributions  (𝑓 (<em><strong>𝒙</strong></em>)) that lie at a distance 𝑟 from the estimated distribution
    𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>):</p>
<blockquote>
    <p>min sup ℎ(<em><strong>𝒛</strong></em>, Q<sub>𝑦</sub>). (12)</p>
</blockquote>
<p>with</p>
<blockquote>
    <p>̂ ̂</p>
</blockquote>
<p>They show how finite-dimensional convex reformulations can be ob- tained when
    𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) ∶= <em><strong>𝜽</strong></em>𝑇
    <em><strong>𝒙</strong></em>, and promote the use of a ‘‘robustness optimization’’ form.</p>
<h2 id="integrated-learning-and-optimization">Integrated learning and optimization</h2>
<p><span id="_bookmark28" class="anchor"></span>As discussed previously, ILO is an end-to-end framework that in- cludes
    three components in the training pipeline: (i) a prediction model that maps the covariate to a predicted
    distribution (or possibly a point prediction), (ii) an optimization model that takes as input a prediction and
    returns a decision, and (iii) a task-based loss function that captures the downstream optimization problem. The
    parameters of the predic- tion model are trained to maximize the prescriptive performance of the policy, i.e., it is
    trained on the task loss incurred by this induced policy</p>
<blockquote>
    <p>𝑧∈ Q𝑦∈𝑟(𝑓<em><strong>𝜽</strong></em> (<em><strong>𝒙</strong></em>))</p>
</blockquote>
<p><a href="#_bookmark62">Bertsimas and Van Parys</a> (<a href="#_bookmark62">2022</a>) generate bootstrap data from the
    training set and use it as a proxy for the ‘‘out-of-sample disappoint- ment’’ of an action
    <em><strong>𝒛</strong></em> resulting from the out-of-sample cost exceeding</p>
<p>the budget given by sup<sub>Q</sub>𝑦∈𝑟(𝑓<em><strong>𝜽</strong></em> (<em><strong>𝒙</strong></em>))
    ℎ(<em><strong>𝒛</strong></em>, Q<sub>𝑦</sub>). They show that for the NW kernel estimator and KNN estimator, the
    DRO, under a range of ambi-</p>
<p>guity sets, can be reformulated as a convex optimization problem. Using KL divergence to measure the distance between
    the probability distribu- tions, they obtain guarantees (‘‘bootstrap robustness’’) with respect to the
    estimate-then-optimize model taking bootstrap data as a proxy for out-of-sample data. Taking the center of
    Wasserstein ambiguity set (see <a href="#_bookmark214">Kantorovich</a> <a href="#_bookmark133">&amp;
        Rubinshtein</a>, <a href="#_bookmark133">1958</a>) to be NW kernel estimator, <a href="#_bookmark214">Wang</a>
    <a href="#_bookmark214">et al.</a> (<a href="#_bookmark214">2021</a>) show that the distributionally robust
    newsvendor and conditional value at risk (CVaR) portfolio optimization problems can be reformulated as convex
    programs. They provide conditions to obtain asymptotic convergence and out-of-sample guarantees on the solutions of
    the DRO model.</p>
<p><a href="#_bookmark74">Chen and Paschalidis</a> (<a href="#_bookmark74">2019</a>) study a distributionally robust kNN
    regression problem by combining point estimation of the outcome <a href="#_bookmark73">with</a> a DRO model over a
    Wasserstein ambiguity set (<a href="#_bookmark73">Chen &amp; Pascha-</a> <a href="#_bookmark73">lidis</a>, <a
        href="#_bookmark73">2018</a>) and then using kNN to predict the outcome based on the weighted distance metric
    constructed from the estimates. Extending the methods developed in <a href="#_bookmark167">Nguyen et al.</a> (<a
        href="#_bookmark167">2021</a>), and <a href="#_bookmark166">Nguyen et al.</a> (<a
        href="#_bookmark166">2020</a>), study a distributionally robust contextual portfolio allocation problem where
    worst-case conditional return–risk tradeoff is computed over an optimal transport ambiguity set consisting of
    perturbations of the joint distribution of covariates and returns. Their approach generalizes the mean–variance and
    mean-CVaR model, for which the distributionally</p>
<p>robust models are shown to be equivalent to semi-definite or second-</p>
<p>rather than the estimation loss.</p>
<p>Next, we discuss several methods for implementing the ILO ap- proach. We start by describing the different models
    that are used in ILO (Section <a href="#_bookmark29">5.1</a>), and then we present the algorithms used to perform
    the training. We divide the algorithms into four categories. Namely, training using unrolling (Section <a
        href="#_bookmark32">5.2</a>), implicit differentiation (Section <a href="#_bookmark33">5.3</a>), a surrogate
    differentiable loss function (Section <a href="#_bookmark34">5.4</a>), and a differentiable optimizer (Section <a
        href="#bookmark38">5.5</a>). An overview of the methods presented in this section is given in <a
        href="#_bookmark30">Table 3</a>.</p>
<ol type="1">
    <li>
        <blockquote>
            <p><em>Models</em></p>
        </blockquote>
    </li>
</ol>
<p><span id="_bookmark29" class="anchor"></span><a href="#_bookmark54">Bengio</a> (<a href="#_bookmark54">1997</a>)
    appears to be the first to train a prediction model using a loss that is influenced by the performance of an action
    pre- scribed by a conditional expected value-based decision rule. This was done in the context of portfolio
    management, where an investment decision rule exploits a point prediction of asset returns. Effective wealth
    accumulation is used to steer the predictor toward predictions that lead to good investments. More recent works
    attempt to integrate a full optimization model, rather than a rule, into the training pipeline. Next, we summarize
    how ILO is applied to the two types of contextual optimization models and introduce two additional popular task
    models that have been considered under ILO, replacing the traditional expected cost task.</p>
<p><em>Expected value-based model.</em> To this date, most of the literature has considered performing ILO on an
    expected value-based optimization model. Namely, following the notation presented in <a
        href="#_bookmark15">Definition 1</a> (Sec- tion <a href="#bookmark11">2.2.2</a>), this training pipeline is
    interested in the loss (<em><strong>𝜽</strong></em>) ∶=</p>
<p>order cone representable programs. <a href="#_bookmark103">Esteban-Pérez and Morales</a> (<a
        href="#_bookmark103">2022</a>)</p>
<blockquote>
    <p>𝐻(𝑧∗(⋅, 𝑔<em><strong><sub>𝜽</sub></strong></em>), P̂ 𝑁 ) = EP̂ [𝑐(𝑧∗(<em><strong>𝒙</strong></em>,
        𝑔<em><strong><sub>𝜽</sub></strong></em>), <em><strong>𝒚</strong></em>)] with
        𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) as a point predictor</p>
</blockquote>
<p>solve a DRO problem with a novel ambiguity set that is based on trim- ming the empirical conditional distribution,
    i.e., reducing the weights over the support points. The authors show the link between trimming a distribution and
    partial mass transportation problem, and an interested reader can refer to <a href="#_bookmark104">Esteban-Pérez and
        Morales</a> (<a href="#_bookmark104">2023</a>) for an application in the optimal power flow problem.</p>
<p>for <em><strong>𝒚</strong></em>, which we interp<sup>𝑁</sup>ret as a prediction of E[<em><strong>𝒚
            𝒙</strong></em>]. This already raises challenges related to the non-convexity of the integrated loss
    function</p>
<p>(<em><strong>𝜽</strong></em>) and its differentiation with respect to <em><strong>𝜽</strong></em>:</p>
<blockquote>
    <p>𝑁</p>
    <p>∇ (<em><strong>𝜽</strong></em>) = ∇ 𝑐(𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em> , 𝑔 ),
        <em><strong>𝒚</strong></em> )</p>
    <p>𝑁 𝑖=1</p>
    <p>A distributionally robust extension of the <strong>rCSO</strong> model is presented</p>
    <p>𝑁 𝑑<em><strong>𝒛</strong></em> 𝑑<em><strong>𝒚</strong></em></p>
    <p>𝜕𝑐(𝑧∗(<em><strong>𝒙</strong></em> , 𝑔 ), <em><strong>𝒚</strong></em> )
        𝜕𝑧∗(<em><strong>𝒙</strong></em>𝑖, <em><strong>𝒚</strong></em>̂)</p>
</blockquote>
<p>all distributions that lie in the 𝑟 radius of the (Wasserstein) ambiguity <span class="underline"> </span></p>
<blockquote>
    <p>𝑖=1 𝑗=1 𝑘=1</p>
    <p>|<em><strong>𝒚</strong></em>̂=𝑔𝜃 (<em><strong>𝒙</strong></em>𝑖 )</p>
</blockquote>
<p>ball centered at the estimated distribution PER(<em><strong>𝒙</strong></em>). <a href="#_bookmark174">Perakis et
        al.</a> (<a href="#_bookmark174">2023</a>)</p>
<p>propose a DRO model to solve a two-stage multi-item joint production</p>
<p>with <sup>𝜕𝑧</sup>∗(<em><strong>𝒙</strong></em>𝑖,<em><strong>𝒚</strong></em>̂) as the most problematic
    evaluation. For instance, when</p>
<p>𝑧∗(<em><strong>𝒙</strong></em> , 𝑔 𝜕𝑦̂i<sup>𝑘</sup>s the solution of a linear program (LP), it is well known
    that</p>
<p>and pricing problem with a partitioned-moment-based ambiguity set</p>
<blockquote>
    <p>𝑖 <em><strong>𝜽</strong></em>)</p>
</blockquote>
<p>constructed by clustering the residuals estimated from an additive demand model.</p>
<p><a href="#_bookmark231">Zhu et al.</a> (<a href="#_bookmark231">2022</a>) considers an expected value-based model and
    suggests an ambiguity set that is informed by the estimation metric used to train 𝑔<em><strong>𝜽</strong></em>̂ .
    Namely, they consider:</p>
<blockquote>
    <p>min sup 𝑐(<em><strong>𝒛</strong></em>,
        𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>)),</p>
</blockquote>
<p>its gradient is either null or non-existent as it jumps between extreme points of the feasible polyhedron as the
    objective is perturbed.</p>
<p><em>Conditional distribution-based model.</em> In the context of learning a con- ditional distribution model
    𝑓<sub>𝜃</sub> (<em><strong>𝒙</strong></em>), <a href="#_bookmark94">Donti et al.</a> (<a
        href="#_bookmark94">2017</a>) appear to be the first to study the ILO problem. They model the distribution of
    the uncertain parameters using parametric distributions (exponential</p>
<p><em><strong>𝒛</strong></em>∈ <em><strong>𝜽</strong></em>∈ (<em><strong>𝜽</strong></em>̂,𝑟)</p>
<blockquote>
    <p>and normal). For the newsvendor problem, it is shown that the ILO</p>
    <p><span id="_bookmark30" class="anchor"></span><strong>Table 3</strong></p>
    <p><span class="underline">Overview of contextual optimization papers in the ILO framework.</span> <span
            class="underline">Objective</span> <span class="underline">Feasible domain</span> <span
            class="underline">Training</span> LP QP Convex Non convex Integer Uncertain Implicit diff. Surr. loss Surr.
        optim.</p>
    <p><a href="#_bookmark45">Amos and Kolter</a> (<a href="#_bookmark45">2017</a>) ✗ ✔ ✗ ✗ ✗ ✔ ✔ ✗ ✔ <a
            href="#_bookmark94">Donti et al.</a> (<a href="#_bookmark94">2017</a>) ✗ ✔ ✔ ✗ ✗ ✔ ✔ ✗ ✗ <a
            href="#_bookmark42">Agrawal et al.</a> (<a href="#_bookmark42">2019</a>) ✗ ✔ ✔ ✗ ✗ ✔ ✔ ✗ ✔ <a
            href="#_bookmark210">Vlastelica et al.</a> (<a href="#_bookmark210">2019</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✗ ✔ ✗ <a
            href="#_bookmark218">Wilder et al.</a> (<a href="#_bookmark218">2019</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✔ ✗ ✗ <a
            href="#_bookmark219">Wilder et al.</a> (<a href="#_bookmark219">2019</a>) ✗ ✔ ✗ ✗ ✔ ✗ ✗ ✔ ✗ <a
            href="#_bookmark56">Berthet et al.</a> (<a href="#_bookmark56">2020</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✔ <a
            href="#_bookmark102">Elmachtoub et al.</a> (<a href="#_bookmark102">2020</a>) ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ <a
            href="#_bookmark108">Ferber et al.</a> (<a href="#_bookmark108">2020</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✔ ✗ ✗ <a
            href="#_bookmark152">Mandi and Guns</a> (<a href="#_bookmark152">2020</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✔ ✗ ✗ <a
            href="#_bookmark154">Mandi et al.</a> (<a href="#_bookmark154">2020</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✗ ✔ ✗ <a
            href="#_bookmark113">Grigas et al.</a> (<a href="#_bookmark113">2021</a>) ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✔ <a
            href="#_bookmark161">Mulamba et al.</a> (<a href="#_bookmark161">2021</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✔ <a
            href="#_bookmark79">Chung et al.</a> (<a href="#_bookmark79">2022</a>) ✗ ✗ ✔ ✗ ✔ ✗ ✗ ✔ ✗ <a
            href="#_bookmark85">Cristian et al.</a> (<a href="#_bookmark85">2022</a>) ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✔ <a
            href="#_bookmark87">Dalle et al.</a> (<a href="#_bookmark87">2022</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✔ <a
            href="#_bookmark100">Elmachtoub and Grigas</a> (<a href="#_bookmark100">2022</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✗ ✔ ✗ <a
            href="#_bookmark127">Jeong et al.</a> (<a href="#_bookmark127">2022</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✗ ✔ ✗ <a
            href="#_bookmark128">Kallus and Mao</a> (<a href="#_bookmark128">2022</a>) ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✔ ✗ <a
            href="#_bookmark135">Kong et al.</a> (<a href="#_bookmark135">2022</a>) ✗ ✔ ✔ ✔ ✔ ✗ ✗ ✗ ✔ <a
            href="#_bookmark140">Lawless and Zhou</a> (<a href="#_bookmark140">2022</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✗ ✔ ✗ <a
            href="#_bookmark149">Loke et al.</a> (<a href="#_bookmark149">2022</a>) ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ <a
            href="#_bookmark151">Mandi et al.</a> (<a href="#_bookmark151">2022</a>) ✔ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✔ <a
            href="#_bookmark162">Muñoz et al.</a> (<a href="#_bookmark162">2022</a>) ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ <a
            href="#_bookmark193">Shah et al.</a> (<a href="#_bookmark193">2022</a>) ✔ ✔ ✔ ✔ ✔ ✗ ✗ ✗ ✔ <a
            href="#_bookmark68">Butler and Kwon</a> (<a href="#_bookmark68">2023a</a>) ✗ ✔ ✗ ✗ ✗ ✗ ✔ ✗ ✗ <a
            href="#_bookmark84">Costa and Iyengar</a> (<a href="#_bookmark84">2023</a>) ✗ ✔ ✔ ✗ ✗ ✔ ✔ ✔ ✗ <a
            href="#_bookmark105">Estes and Richard</a> (<a href="#_bookmark105">2023</a>) ✔ ✗ ✔ ✗ ✗ ✗ ✗ ✔ ✗ <a
            href="#_bookmark137">Kotary et al.</a> (<a href="#_bookmark137">2023</a>) ✔ ✔ ✔ ✔ ✗ ✗ ✔ ✗ ✗ <a
            href="#_bookmark157">McKenzie et al.</a> (<a href="#_bookmark157">2023</a>) ✔ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ <a
            href="#_bookmark201">Sun et al.</a> (<a href="#_bookmark201">2023</a>) ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ <a
            href="#_bookmark203">Sun et al.</a> (<a href="#_bookmark203">2023</a>) ✗ ✔ ✗ ✔ ✗ ✗ ✔ ✗ ✗</p>
    <p>Notes: an approach has a ‘‘Convex’’ objective if it can handle general convex objective functions that are not
        linear or quadratic such as convex piecewise-linear objective functions; an ‘‘Uncertain’’ feasible domain
        denotes that some constraints are subject to uncertainty. Implicit diff., surr. loss and surr. optim. denote
        implicit differentiation, surrogate differentiable loss function and surrogate differentiable optimizer,
        respectively. This table covers papers that introduced an ILO framework to solve a class of CSO problems, papers
        focused on deriving theoretical guarantees and specific applications are therefore intentionally excluded. The
        tick marks reflect our understanding of the claimed scope of the contributions.</p>
</blockquote>
<p>model outperforms decision rule optimization with neural networks and SLO with maximum likelihood estimation (MLE)
    when there is model misspecification. Since then, it has become more common to formulate the CSO problem as a
    weighted SAA model (as discussed in Section <a href="#bookmark26">4.1.2</a>). The prediction model then amounts to
    identifying a vector of weights to assign to each historical sample. This approach is taken by <a
        href="#_bookmark128">Kallus and Mao</a> (<a href="#_bookmark128">2022</a>), who train a random forest regressor
    in an integrated fashion to assign weights, and by <a href="#_bookmark113">Grigas et al.</a> (<a
        href="#_bookmark113">2021</a>), who show how to train general differentiable models to predict the probabilities
    of an uncertain parameter <em><strong>𝒚</strong></em> with finite support.</p>
<p><em>Optimal action imitation task.</em> ILO has some connections to inverse optimization, i.e., the problem of
    learning the parameters of an opti- mization model given data about its optimal solution (see <a
        href="#_bookmark201">Sun et al.</a>, <a href="#_bookmark201">2023</a>, where both problems are addressed using
    the same method). Indeed, one can replace the original objective of ILO with an objective that seeks to produce a
    𝑧∗(<em><strong>𝒙</strong></em>, 𝑓<em><strong><sub>𝜽</sub></strong></em>) that is as close as possible to the
    optimal hindsight action and, therefore, closer to the regret objective. Specifically, to learn a policy that
    ‘‘imitates’’ the optimal hindsight action, one can first augment the data set with <em><strong>𝒛</strong></em>∗ ∶=
    𝑧∗(<em><strong>𝒙</strong></em><sub>𝑖</sub>, <em><strong>𝒚</strong></em><sub>𝑖</sub>) to get</p>
<p>{(<em><strong>𝒙</strong></em> , <em><strong>𝒚</strong></em> , <em><strong>𝒛</strong></em>∗)}𝑁 . Thereafter, a
    prediction model 𝑓 𝑖 is learned in a way</p>
<p>tha𝑖t th𝑖 e a<sup>𝑖</sup> cti<sup>𝑖=</sup>o<sup>1</sup>n</p>
<blockquote>
    <p>∗(<em><strong>𝒙</strong></em> )</p>
</blockquote>
<h3 id="𝜽-𝒙"><br />
    <sub>𝜽</sub> 𝒙
</h3>
<blockquote>
    <p>∗ for all samples in</p>
    <p>𝑧 <sub>𝑖</sub>, 𝑓<em><strong><sub>𝜽</sub></strong></em> is as close as possible to
        <em><strong>𝒛</strong></em>𝑖</p>
</blockquote>
<p><em>Regret minimization task.</em> A recent line of work has tackled the ILO <a href="#_bookmark100">problem</a> from
    the point of view of regret. Indeed, in <a href="#_bookmark100">Elmachtoub</a></p>
<p>the training set (<a href="#_bookmark135">Kong et al.</a>, <a href="#_bookmark135">2022</a>):</p>
<blockquote>
    <p>𝐻<sub>Imitation</sub>(𝜋, P̂ ′ ) ∶= EP̂ ′ [𝑑(𝜋(<em><strong>𝒙</strong></em>), <em><strong>𝒛</strong></em>∗)]
        = EP̂</p>
    <p>[𝑑(𝜋(<em><strong>𝒙</strong></em>), 𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>,
        <em><strong>𝒚</strong></em>))] (14)</p>
</blockquote>
<p><a href="#_bookmark100">and Grigas</a> (<a href="#_bookmark100">2022</a>), a contextual point predictor
    𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) is learned by</p>
<blockquote>
    <p>𝑁 𝑁 𝑁</p>
</blockquote>
<p>minimizing the regret associated with implementing the prescribed</p>
<p>with P̂ ′ as the empirical distribution on the lifted tuple (<em><strong>𝒙</strong></em>,
    <em><strong>𝒚</strong></em>, 𝑧∗(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>))</p>
<p>action based on the mean estimator 𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) instead of
    based on the realized parameters <em><strong>𝒚</strong></em> (a.k.a. the optimal hindsight or wait-and-see
    decision). Specifically, the value of an expected value-based policy</p>
<p>𝜋<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) ∶= 𝑧∗(<em><strong>𝒙</strong></em>,
    𝑔<em><strong><sub>𝜽</sub></strong></em>) is measured as the expected regret defined as:</p>
<blockquote>
    <p>𝐻<sub>Regret</sub>(𝜋<em><strong><sub>𝜽</sub></strong></em>, P) ∶=
        E<sub>P</sub>[𝑐(𝜋<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>),
        <em><strong>𝒚</strong></em>) − 𝑐(𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>),
        <em><strong>𝒚</strong></em>)]. <span id="_bookmark31" class="anchor"></span>(13)</p>
</blockquote>
<p>Minimizing the expected regret returns the same optimal parameter vector <em><strong>𝜽</strong></em> as the ILO
    problem (<a href="#_bookmark16">9</a>). This is due to the fact that:</p>
<p>based o<sup>𝑁</sup>n the augmented data set and a distance function 𝑑(<em><strong>𝒛</strong></em>,
    <em><strong>𝒛</strong></em>∗). We note that there is no reason to believe that the best imitator under a general
    distance function, e.g., <em><strong>𝒛</strong></em> − <em><strong>𝒛</strong></em>∗ 2, performs well under our</p>
<p>original metric 𝐻(𝜋, P̂ 𝑁 ). One exception is for 𝑑(<em><strong>𝒛</strong></em>, <em><strong>𝒛</strong></em>∗)
    ∶= 𝑐(<em><strong>𝒛</strong></em>, <em><strong>𝒚</strong></em>) −</p>
<p>𝑐(<em><strong>𝒛</strong></em>∗, <em><strong>𝒚</strong></em>), where we allow the distance to also depend on
    <em><strong>𝒚</strong></em>, for which</p>
<p>we recover the regret minimization approach, and therefore the same solution as with 𝐻(𝜋, P̂ 𝑁 ). Readers that
    have an interest in general inverse optimization methods should consult (<a href="#_bookmark71">Chan et al.</a>, <a
        href="#_bookmark71">2021</a>) for an</p>
<p>extensive recent review of the field.</p>
<blockquote>
    <p>𝐻Regret(𝜋, P̂</p>
</blockquote>
<p><sub>𝑁</sub> ) = E<sub>P</sub>̂𝑁</p>
<p>[𝑐(𝜋(<em><strong>𝒙</strong></em>), <em><strong>𝒚</strong></em>) − 𝑐(𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>,
    <em><strong>𝒚</strong></em>), <em><strong>𝒚</strong></em>)] = 𝐻(𝜋, P̂</p>
<p><sub>𝑁</sub> ) − E<sub>P</sub>̂𝑁</p>
<p>[𝑐(𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>), <em><strong>𝒚</strong></em>)].</p>
<ol start="2" type="1">
    <li>
        <blockquote>
            <p><em><br />
                    Training by unrolling</em></p>
        </blockquote>
    </li>
</ol>
<blockquote>
    <p><span id="_bookmark32" class="anchor"></span>An approach to obtain the Jacobian matrix <span
            class="underline">𝜕𝑧∗(<em><strong>𝒙</strong></em>,<em><strong>𝒚</strong></em>̂</span>)</p>
    <p>is unrolling</p>
</blockquote>
<p>Hence, both 𝐻<sub>Regret</sub>(𝜋, P̂</p>
<p>ers.</p>
<p><sub>𝑁</sub> ) and 𝐻(𝜋, P̂</p>
<p><sub>𝑁</sub> ) have the same set of minimiz-</p>
<p>(<a href="#_bookmark92">Domke</a>, <a href="#_bookmark92">2012</a>), which involves approximating the
    op<sup>𝜕<em><strong>𝒚</strong></em>̂</sup>timization prob- lem with an iterative solver (e.g., first-order
    gradient-based method).</p>
<p>Each operation is stored on the computational graph, which then allows, in principle, for computing gradients through
    classical back- propagation methods. Unfortunately, this approach requires extensive amounts of memory. Besides
    this, the large size of the computational graph exacerbates the vanishing and exploding gradient problems typ-
    ically associated with training neural networks (<a href="#_bookmark160">Monga et al.</a>, <a
        href="#_bookmark160">2021</a>).</p>
<ol start="3" type="1">
    <li>
        <blockquote>
            <p><em>Training using implicit differentiation</em></p>
        </blockquote>
    </li>
</ol>
<p><span id="_bookmark33" class="anchor"></span>Implicit differentiation allows for a memory-efficient backpropaga- tion
    as opposed to unrolling (we refer to <a href="#_bookmark49">Bai et al.</a>, <a href="#_bookmark49">2019</a>, for
    discussion on training constant memory implicit models using a fixed-point – <a href="#_bookmark45">FP</a> –
    equation and feedforward networks of infinite depths). <a href="#_bookmark45">Amos</a> <a href="#_bookmark45">and
        Kolter</a> (<a href="#_bookmark45">2017</a>) appear to be the first to have employed implicit differentiation
    methods to train an ILO model, which they refer to as <strong>OptNet</strong>. They consider expected value-based
    optimization models that take the form of constrained quadratic programs (QP) with equality and inequality
    constraints. They show how the implicit function theorem (IFT – <a href="#_bookmark117">Halkin</a>, <a
        href="#_bookmark117">1974</a>) can be used to differentiate 𝑧∗(<em><strong>𝒙</strong></em>,
    𝑔<em><strong><sub>𝜽</sub></strong></em>) with respect to <em><strong>𝜽</strong></em> using the Karush–Kuhn–Tucker
    (KKT) conditions that are satisfied at optimality. Further, they provide a custom solver based on a primal– dual
    interior method to simultaneously solve multiple QPs on GPUs in batch form, permitting 100-times speedups compared
    to Gurobi and CPLEX. This approach is extended to conditional stochastic and strongly convex optimization models in
    <a href="#_bookmark94">Donti et al.</a> (<a href="#_bookmark94">2017</a>). They use sequential quadratic programming
    (<strong>SQP</strong>) to obtain quadratic approximations of the objective functions of the convex program at each
    iteration until convergence to the solution and then differentiate the last iteration of <strong>SQP</strong> to
    obtain the Jacobian. For a broader view of implicit differenti- <a href="#_bookmark97">ation,</a> we refer to the
    surveys by <a href="#_bookmark65">Blondel et al.</a> (<a href="#_bookmark65">2022</a>) and <a
        href="#_bookmark97">Duvenaud</a> <a href="#_bookmark97">et al.</a> (<a href="#_bookmark97">2020</a>).</p>
<p>To solve large-scale QPs with linear equality and box inequality constraints, <a href="#_bookmark68">Butler and
        Kwon</a> (<a href="#_bookmark68">2023a</a>) use the ADMM algorithm to decouple the differentiation procedure for
    primal and dual variables, thereby decomposing the large problem into smaller subproblems. Their procedure relies on
    implicit differentiation of the FP equations of the alternating direction method of multipliers (ADMM) algorithm
    (<strong>ADMM-FP</strong>). They show that unrolling the iterations of the ADMM algo- rithm on the computational
    graph (<a href="#_bookmark200">Sun et al.</a>, <a href="#_bookmark200">2016</a>; <a href="#_bookmark220">Xie et
        al.</a>, <a href="#_bookmark220">2019</a>) results in higher computation time than <strong>ADMM-FP</strong>.
    Their empirical results on a portfolio optimization problem with 254 assets suggest that computational time can be
    reduced by a factor of almost five by using <strong>ADMM-FP</strong> compared to <strong>OptNet</strong>, mostly due
    to the use of the ADMM <a href="#_bookmark68">algorithm</a> in the forward pass. Note that the experiments in <a
        href="#_bookmark68">Butler and</a> <a href="#_bookmark68">Kwon</a> (<a href="#_bookmark68">2023a</a>) were
    conducted on a CPU.</p>
<p>To extend <strong>OptNet</strong> to a broader class of problems, <a href="#_bookmark42">Agrawal et al.</a> (<a
        href="#_bookmark42">2019</a>) introduce <strong>cvxpylayers</strong> that relies on converting disciplined
    convex programs in the domain-specific language used by CVXPY into conic programs. They implicitly differentiate the
    residual map of the homogeneous self-dual embedding associated with the conic program. <a
        href="#_bookmark157">McKenzie et al.</a> (<a href="#_bookmark157">2023</a>) note that using KKT conditions for
    con- strained optimization problems with DNN-based policies is compu- tationally costly as
    ‘‘<strong>cvxpylayers</strong> struggles with solving problems containing more than 100 variables’’ (see also <a
        href="#_bookmark68">Butler &amp; Kwon</a>, <a href="#_bookmark68">2023a</a>).</p>
<p>An alternative is to use projected gradient descent (<strong>PGD</strong>) where DNN-</p>
<p>based policies are updated using an iterative solver and projected onto the constraint set  at each iteration and
    the associated FP system (<a href="#_bookmark65">Blondel et al.</a>, <a href="#_bookmark65">2022</a>; <a
        href="#_bookmark72">Chen et al.</a>, <a href="#_bookmark72">2021</a>; <a href="#_bookmark95">Donti et al.</a>,
    <a href="#_bookmark95">2021</a>) is used to obtain the Jacobian.</p>
<p>Since a closed-form solution for the projection onto  is unavailable in many cases, the projection step may be
    costly, and in some cases,</p>
<p><strong>PGD</strong> may not even converge to a feasible point (<a href="#_bookmark187">Rychener et al.</a>, <a
        href="#_bookmark187">2023</a>). To avoid computing the projection in the forward pass, <a
        href="#_bookmark157">McKenzie et al.</a></p>
<p>Jacobian matrix is replaced with an identity matrix <a href="#_bookmark188">Sahoo et al.</a> (see also <a
        href="#_bookmark188">2023</a>, where a similar approach is used for expected value-based models).</p>
<p>To mitigate the issues with unrolling, <a href="#_bookmark137">Kotary et al.</a> (<a href="#_bookmark137">2023</a>)
    propose FP folding (<strong>fold-opt</strong>) that allows analytically differentiating the FP system of general
    iterative solvers, e.g., <strong>ADMM</strong>, <strong>SQP</strong>, and <strong>PGD</strong>. By unfold- ing
    (i.e., partial unrolling), some of the steps of unrolling are grouped in analytically differentiable update function
     ∶ R<sup>𝑑</sup><em><strong>𝒚</strong></em> → R<sup>𝑑</sup><em><strong>𝒚</strong></em> :</p>
<blockquote>
    <p>𝑧<sub>𝑘+1</sub>(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>̂) = 
        (𝑧<sub>𝑘</sub>(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>̂), <em><strong>𝒚</strong></em>̂).
    </p>
</blockquote>
<p>Realizing that 𝑧∗(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>̂) is the FP of the above system, they
    use the IFT to obtain a linear system (a differential FP condition) that can be solved to obtain the Jacobian. This
    effectively decouples the forward and backward pass enabling the use of black box solvers like Gurobi for the
    forward pass while <strong>cvxpylayers</strong> is restricted to operator splitting solvers like ADMM. An added
    benefit of using <strong>fold-opt</strong> is that it can solve non-convex problems. In the case of portfolio
    optimization, the authors note that the superior performance of their model with respect to
    <strong>cvxpylayers</strong> can be explained by the precise calculations made in the forward pass by Gurobi.</p>
<p>While speedups can be obtained for sparse problems, <a href="#_bookmark203">Sun et al.</a> (<a
        href="#_bookmark203">2023</a>) remark that the complexity associated with differentiating the KKT conditions is
    cubic in the total number of decision variables and constraints in general. They propose an alternating differenti-
    ation framework (called <strong>Alt-Diff</strong>) to solve parameterized convex optimization problems with
    polyhedral constraints using ADMM that decouples the objective and constraints. This procedure results in a smaller
    Jacobian matrix when there are many constraints since the gradient computations for primal, dual, and slack
    variables are done alternatingly. The gradients are shown to converge to those obtained by differentiating the KKT
    conditions. The authors employ trunca- tion of iterations to compensate for the slow convergence of ADMM when
    compared to interior-point methods and provide theoretical up- per bounds on the error in the resulting gradients.
    <strong>Alt-Diff</strong> is shown to achieve the same accuracy with truncation and lower computa- tional time when
    compared to <strong>cvxpylayers</strong> for an energy generation scheduling problem.</p>
<p>Motivated by <strong>OptNet</strong>, several extensions have been proposed to solve linear and combinatorial
    problems. <a href="#_bookmark218">Wilder et al.</a> (<a href="#_bookmark218">2019</a>) solve LP- representable
    combinatorial optimization problems and LP relaxations of combinatorial problems during the training phase. Their
    model, re- ferred to as <strong>QPTL</strong> (Quadratic Programming Task Loss), adds a quadratic penalty term to
    the objective function of the linear problem. This has two advantages: it recovers a differentiable linear–quadratic
    program, and the added term acts as a regularizer, which might avoid overfitting. To solve a general mixed-integer
    LP (MILP), <a href="#_bookmark108">Ferber et al.</a> (<a href="#_bookmark108">2020</a>) develop a cutting plane
    method <strong>MIPaal</strong>, which adds a given number of cutting planes in the form of constraints
    𝑆<em><strong>𝒛</strong></em> ≤ <em><strong>𝒔</strong></em> to the LP relaxation of the MILP. Instead of adding a
    quadratic term, <a href="#_bookmark152">Mandi and Guns</a> (<a href="#_bookmark152">2020</a>) propose
    <strong>IntOpt</strong> based on the interior point method to solve LPs that adds a log barrier term to the
    objective function and differentiates the homogeneous self-dual formulation of the LP. Their experimental analyses
    show that this approach performs better on energy cost-aware scheduling problems than <strong>QPTL</strong>.</p>
<p><a href="#_bookmark84">Costa and Iyengar</a> (<a href="#_bookmark84">2023</a>) introduce an ILO framework with the
    weighted average of Sharpe ratio and MSE loss as a task loss and replace the optimization problem with a surrogate
    DRO problem. By using convex duality, they reformulate the minimax problem as a minimization problem and learn the
    parameters (e.g., size of ambiguity set) using implicit differentiation instead of cross-validation (CV). More
    specifically, the DRO model uses a deviation risk measure (e.g., vari- ance) to control variability in the portfolio
    returns associated with the prediction errors <em><strong>𝝐</strong></em> = <em><strong>𝒚</strong></em> − 𝑔
    (<em><strong>𝒙</strong></em> ):</p>
<p>(<a href="#_bookmark157">2023</a>) solve the expected value-based CSO problem using Davis–Yin</p>
<p>operator splitting (<a href="#_bookmark88">Davis &amp; Yin</a>, <a href="#_bookmark88">2017</a>) while the backward
    pass uses</p>
<blockquote>
    <p>𝑖 𝑖</p>
</blockquote>
<p>argmin</p>
<p>𝜃</p>
<blockquote>
    <p>max</p>
    <p>𝑖</p>
    <p>E<sub>Q</sub> [(<em><strong>𝝐</strong></em>⊤<em><strong>𝒛</strong></em> −
        E<sub>Q</sub>[<em><strong>𝝐</strong></em>⊤<em><strong>𝒛</strong></em>])2] ,</p>
</blockquote>
<p>where the distribution of errors lies in 𝜙-divergence (e.g., Hellinger loss with a convex upper bound called
    <strong>SPO+</strong>:</p>
<p>distance) based ambiguity set <sup>𝜙</sup>(P̂</p>
<p><sub>𝑁</sub> ) = {Q ∶ EP̂ [𝜙(Q∕P̂ )] ≤ 𝑟} centered</p>
<blockquote>
    <p>𝑇 𝑇 ∗</p>
    <p>𝑇 ∗</p>
    <p>at P̂ 𝑁 = <span class="underline"><sup> 1</sup></span> ∑𝑁 𝛿<em><strong><sub>𝝐</sub></strong></em> .</p>
    <p>𝓁SP𝙾+(<em><strong>𝒚</strong></em>̂, <em><strong>𝒚</strong></em>) ∶= sup(<em><strong>𝒚</strong></em> −
        2<em><strong>𝒚</strong></em>̂) <em><strong>𝒛</strong></em> + 2<em><strong>𝒚</strong></em>̂ 𝑧
        (<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>) − <em><strong>𝒚</strong></em> 𝑧
        (<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>),</p>
</blockquote>
<p>For convex problems, the optimality conditions are given by KKT conditions, which can be represented as 𝐹
    (<em><strong>𝜽</strong></em>, <em><strong>𝒛</strong></em>) = 0 where 𝐹 ∶ R𝑑<em><strong>𝒙</strong></em> ×
    R𝑑<em><strong>𝒛</strong></em> → R𝑚, where 𝑚 is proportional to the number of constraints that define . From the
    classical IFT (<a href="#_bookmark93">Dontchev et al.</a>, <a href="#_bookmark93">2009</a>), we know that if</p>
<p>𝐹 is continuously differentiable and the Jacobian matrix with respect to</p>
<p><em><strong>𝒛</strong></em>, denoted by ∇<em><strong><sub>𝒛</sub></strong></em>𝐹 (<em><strong>𝜽</strong></em>,
    <em><strong>𝒛</strong></em>(<em><strong>𝜽</strong></em>)), is non-singular at the point
    (<em><strong>𝜽</strong></em>̄, <em><strong>𝒛</strong></em>̄), then there exists a neighborhood around
    <em><strong>𝜽</strong></em>̄ for which the gradient of the optimal</p>
<p>solution with respect to the parameters is given by:</p>
<blockquote>
    <p>𝜕<em><strong>𝒛</strong></em>∗(<em><strong>𝜽</strong></em>) = −(∇ 𝐹 (<em><strong>𝜽</strong></em>,
        <em><strong>𝒛</strong></em>(<em><strong>𝜽</strong></em>)))−1∇ 𝐹 (<em><strong>𝜽</strong></em>,
        <em><strong>𝒛</strong></em>(<em><strong>𝜽</strong></em>)).</p>
    <p>𝜕<em><strong>𝜽</strong></em></p>
</blockquote>
<p>When the Jacobian matrix ∇<em><strong><sub>𝒛</sub></strong></em>𝐹 (<em><strong>𝜽</strong></em>,
    <em><strong>𝒛</strong></em>(<em><strong>𝜽</strong></em>)) is singular, classical IFT can- not be applied. This
    occurs in linear programs and can also arise in smooth QPs as shown in <a href="#_bookmark67">Bolte et al.</a> (<a
        href="#_bookmark67">2021</a>). <a href="#_bookmark67">Bolte et al.</a> (<a href="#_bookmark67">2021</a>) obtain
    a generalization of IFT to non-smooth functions using conservative Jacobians that generalize Clarke Jacobians (<a
        href="#_bookmark81">Clarke</a>, <a href="#_bookmark81">1990</a>) for locally Lipschitz function 𝐹 . They also
    derive conservative Jacobians for conic optimization layers (<a href="#_bookmark42">Agrawal et al.</a>, <a
        href="#_bookmark42">2019</a>).</p>
<p>Further, <a href="#_bookmark67">Bolte et al.</a> (<a href="#_bookmark67">2021</a>) illustrate using
    <strong>cvxpylayers</strong> that in a bilevel program which is a composition of a quadratic function with the
    solution map of a linear program, gradient descent does not converge but gets stuck in a ‘‘limit cycle of
    non-critical points’’ even though invertibility condition does not hold only on a set of measure 0 (defined by a
    line) where the solution map moves from extreme point to another. As this example illustrates, the convergence of
    gradient methods based on IFT can be impacted by the non-invertibility of the Jacobian matrix and non-smoothness
    which is difficult to verify a priori. As a result, research efforts have been directed toward designing surrogate
    loss functions and perturbation-based models for CSO problems that could circumvent the need to use the IFT.</p>
<ol start="4" type="1">
    <li>
        <blockquote>
            <p><em>Training using a surrogate differentiable loss function</em></p>
        </blockquote>
    </li>
</ol>
<p><span id="_bookmark34" class="anchor"></span>As discussed in Section <a href="#_bookmark29">5.1</a>, minimizing
    directly the task loss in (<a href="#_bookmark16">9</a>) or the regret in (<a href="#_bookmark31">13</a>) is
    computationally difficult in most cases. For instance, the loss may be piecewise-constant as a function of the
    parameters of a prediction model and, thus, may have no informa- tive gradient. To address this issue, several
    surrogate loss functions with good properties, e.g., differentiability and convexity, have been proposed to train
    ILO models.</p>
<ol type="1">
    <li>
        <blockquote>
            <p><em><strong>SPO+</strong></em></p>
        </blockquote>
    </li>
</ol>
<p>In CSO problems, <a href="#_bookmark100">Elmachtoub and Grigas</a> (<a href="#_bookmark100">2022</a>) first tackle
    the potential non-uniqueness of 𝑧∗(<em><strong>𝒙</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>) by
    introducing a Smart ‘‘Predict, then Optimize’’ (<strong>SPO</strong>) model where the decision-maker chooses to
    minimize the empirical average of the regret under the worst-case optimal solution as defined below:</p>
<blockquote>
    <p>(SP𝙾) min max 𝐻<sub>Regret</sub>(𝜋, P̂ 𝑁 ),</p>
</blockquote>
<p>which has a closed-form expression for its subgradient</p>
<blockquote>
    <p>2 𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>) −
        𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>, 2<em><strong>𝒚</strong></em>̂ − <em><strong>𝒚</strong></em>) ∈
        ∇<sub><em><strong>𝒚</strong></em>̂</sub>𝓁SP𝙾+(<em><strong>𝒚</strong></em>̂, <em><strong>𝒚</strong></em>).
        <span id="_bookmark35" class="anchor"></span>(16)</p>
</blockquote>
<p><a href="#_bookmark149">Loke et al.</a> (<a href="#_bookmark149">2022</a>) propose a decision-driven regularization
    model (<strong>DDR</strong>) that combines prediction accuracy and decision quality in a single optimization problem
    with loss function as follows:</p>
<blockquote>
    <p>𝓁𝙳𝙳(<em><strong>𝒚</strong></em>̂, <em><strong>𝒚</strong></em>) = 𝑑(<em><strong>𝒚</strong></em>̂,
        <em><strong>𝒚</strong></em>) − 𝜆 min{𝜇<em><strong>𝒚</strong></em>⊤<em><strong>𝒛</strong></em> + (1 −
        𝜇)<em><strong>𝒚</strong></em>̂<sup>⊤</sup><em><strong>𝒛</strong></em>},</p>
    <p>and <strong>SPO+</strong> being a special case with 𝜇 = −1, 𝜆 = 1, and 𝑑(<em><strong>𝒚</strong></em>̂,
        <em><strong>𝒚</strong></em>) = 2<em><strong>𝒚</strong></em>̂<sup>⊤</sup>𝑧∗(<em><strong>𝒙</strong></em>,
        <em><strong>𝒚</strong></em>) − <em><strong>𝒚</strong></em>𝑇 𝑧∗(<em><strong>𝒙</strong></em>,
        <em><strong>𝒚</strong></em>).</p>
</blockquote>
<p><em><strong>SPO+</strong> for combinatorial problems.</em> Evaluating the gradient of <strong>SPO+</strong> loss in
    (<a href="#_bookmark35">16</a>) requires solving the optimization problem (<a href="#_bookmark19">8</a>) to obtain
    𝑧∗(<em><strong>𝒙</strong></em>, 2<em><strong>𝒚</strong></em>̂ − <em><strong>𝒚</strong></em>) for each data
    point. This can be computationally demanding when the optimization model in (<a href="#_bookmark19">8</a>) is an
    NP-hard problem. <a href="#_bookmark154">Mandi et al.</a> (<a href="#_bookmark154">2020</a>) propose a
    <strong>SPO-relax</strong> approach that computes the gradient of <strong>SPO+</strong> loss by solving instead a
    continuous relaxation when (<a href="#_bookmark19">8</a>) is a MILP. They also suggest speeding up the resolution
    using a warm-start for learning with a pre-trained model that uses MSE as the loss function. Another way proposed to
    speed up the computation is warm-starting the solver. For example, 𝑧∗(<em><strong>𝒙</strong></em>,
    <em><strong>𝒚</strong></em>) can be used as a starting point for MILP solvers or to cut away a large part of the
    feasible space. <a href="#_bookmark154">Mandi et al.</a> (<a href="#_bookmark154">2020</a>) show that for weighted
    and unweighted knapsack problems as well as energy-cost aware scheduling problems, <strong>SPO-relax</strong>
    results in faster convergence and similar performance compared to <strong>SPO+</strong> loss. Also,
    <strong>SPO-relax</strong> provides low regret solutions and faster convergence compared to <strong>QPTL</strong> in
    the aforementioned three problems, except in the weighted knapsack problem with low capacity.</p>
<p>With a focus on exact solution approaches, <a href="#_bookmark127">Jeong et al.</a> (<a
        href="#_bookmark127">2022</a>) study the problem of minimizing the regret in (<a href="#_bookmark31">13</a>)
    assuming a linear predic-</p>
<blockquote>
    <p>tion model 𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) =
        <em><strong>𝜽𝒙</strong></em> with <em><strong>𝜽</strong></em> ∈ R𝑑<em><strong>𝒛</strong></em>
        ×𝑑<em><strong>𝒙</strong></em> . Under the assumption that</p>
</blockquote>
<p>𝑧∗(<em><strong>𝒙</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>) is unique for all
    <em><strong>𝜽</strong></em> and <em><strong>𝒙</strong></em>, the authors reformulate the bilevel</p>
<p><strong>SPO</strong> problem as a single-level MILP using symbolic variable elimination.</p>
<p>They show that their model can achieve up to two orders of magnitude improvement in expected regret compared to
    <strong>SPO+</strong> on the training set. <a href="#_bookmark162">Muñoz et al.</a> (<a
        href="#_bookmark162">2022</a>) applies a similar idea of representing the set of optimal solutions with a MILP.
    They rely on the KKT conditions of the problem defining 𝑧∗(<em><strong>𝒙</strong></em>,
    𝑔<em><strong><sub>𝜽</sub></strong></em>) to transform the bilevel integrated problem into a single-level MILP.
    Finally, <a href="#_bookmark105">Estes and Richard</a> (<a href="#_bookmark105">2023</a>) use the
    <strong>SPO</strong> loss function to solve a two-stage LP with right-hand side uncertainty. They propose a
    lexicographical ordering rule to select the minimal solution when there are multiple optima and approximate the
    resulting piecewise-linear loss function, <strong>lex-SPO</strong>, by a convex surrogate to find the point
    predictor.</p>
<p><em><strong>SPO</strong> trees.</em> <a href="#_bookmark102">Elmachtoub et al.</a> (<a href="#_bookmark102">2020</a>)
    propose a model (<strong>SPOT</strong>) to con- struct decision trees that segment the covariates based on the
    <strong>SPO</strong> loss function while retaining the interpretability in the end-to-end learning framework. Their
    model outperforms classification and regression trees</p>
<p><em><strong>𝜽</strong></em> 𝜋</p>
<blockquote>
    <p>s.t. 𝜋(<em><strong>𝒙</strong></em>) ∈ argmin 𝑐(<em><strong>𝒛</strong></em>,
        𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>)), ∀<em><strong>𝒙</strong></em>.</p>
</blockquote>
<p><em><strong>𝒛</strong></em>∈</p>
<p>(15)</p>
<blockquote>
    <p>(CART) in the numerical experiments on a news recommendation prob-</p>
    <p>lem using a real-world data set and on the shortest path problem with synthetic data (also used in <a
            href="#_bookmark100">Elmachtoub and Grigas</a> (<a href="#_bookmark100">2022</a>)).</p>
</blockquote>
<p>In the expected value-based model, they show that the <strong>SPO</strong> objective</p>
<p>reduces to training the prediction model according to the ERM problem:</p>
<p><em>Guarantees.</em> <a href="#_bookmark100">Elmachtoub and Grigas</a> (<a href="#_bookmark100">2022</a>) show that
    under certain</p>
<p>with:</p>
<blockquote>
    <p><em><strong>𝜽</strong></em>⋆ ∈ argmin 𝜌</p>
    <p><em><strong>𝜽</strong></em></p>
</blockquote>
<p>SP𝙾</p>
<p>(𝑔<em><strong><sub>𝜽</sub></strong></em>, P̂</p>
<p><sub>𝑁</sub> ) ∶= E</p>
<p>P̂ 𝑁</p>
<p>[𝓁SP𝙾</p>
<p>(𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>), <em><strong>𝒚</strong></em>)],</p>
<p>conditions, the minimizers of the <strong>SPO</strong> loss, <strong>SPO+</strong> loss and MSE loss are almost
    always equal to E <sub>(</sub> <sub>)</sub>[<em><strong>𝒚</strong></em>] given that E <sub>(</sub>
    <sub>)</sub>[<em><strong>𝒚</strong></em>] ∈ . Thus, <strong>SPO+</strong> is Fisher consistent with respect to the
    <strong>SPO</strong> loss. This means that <a href="#_bookmark121">minimizing</a> the surrogate loss also minimizes
    the true loss function. <a href="#_bookmark121">Ho-</a></p>
<blockquote>
    <p>𝓁SP𝙾(<em><strong>𝒚</strong></em>̂, <em><strong>𝒚</strong></em>) ∶= sup</p>
    <p><em><strong>𝒛</strong></em>̄∈argmin<em><strong>𝒛</strong></em>∈
        𝑐(<em><strong>𝒛</strong></em>,<em><strong>𝒚</strong></em>̂)</p>
</blockquote>
<p>𝑐(<em><strong>𝒛</strong></em>̄, <em><strong>𝒚</strong></em>) − 𝑐(𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>,
    <em><strong>𝒚</strong></em>), <em><strong>𝒚</strong></em>).</p>
<blockquote>
    <p><a href="#_bookmark121">Nguyen and Kılınç-Karzan</a> (<a href="#_bookmark121">2022</a>) show that for some
        examples of a multiclass classification problem, <strong>SPO+</strong> is Fisher inconsistent, while</p>
</blockquote>
<p>Since the <strong>SPO</strong> loss function is nonconvex and discontinuous in <em><strong>𝒚</strong></em>̂ (<a
        href="#_bookmark121">Ho-</a> <a href="#_bookmark121">Nguyen &amp; Kılınç-Karzan</a>, <a
        href="#_bookmark121">2022</a>, Lemma 1), <a href="#_bookmark100">Elmachtoub and Grigas</a> (<a
        href="#_bookmark100">2022</a>) focus on the linear objective 𝑐(<em><strong>𝒛</strong></em>,
    <em><strong>𝒚</strong></em>) ∶= <em><strong>𝒚</strong></em>𝑇 <em><strong>𝒛</strong></em> and replace the
    <strong>SPO</strong></p>
<p>MSE loss is consistent. However, complete knowledge of the distribu- tion is a limitation in practice where the
    decision-maker has access to <a href="#_bookmark121">only</a> the samples from the distribution. As a result, <a
        href="#_bookmark121">Ho-Nguyen and</a></p>
<p><a href="#_bookmark121">Kılınç-Karzan</a> (<a href="#_bookmark121">2022</a>) and <a href="#_bookmark142">Liu and
        Grigas</a> (<a href="#_bookmark142">2021</a>) provide calibration realization of the uncertain parameter, they
    obtain the conditional</p>
<p>bounds that hold for a class of distributions  on  ×  and ensure</p>
<p>distribution of uncertain parameter, 𝑓<em><strong><sub>𝜽</sub></strong></em></p>
<blockquote>
    <p>= <span class="underline">1</span> ∑𝑇</p>
    <p>𝛿<em><strong>𝒚</strong></em>̂𝑡 , where <em><strong>𝒚</strong></em>̂<sup>𝑡</sup> is the</p>
</blockquote>
<p>translates to lower excess <strong>SPO</strong> risk.</p>
<blockquote>
    <p>𝑖 <em><strong>𝜽</strong></em>0</p>
</blockquote>
<p>they obtain an optimal allocation, 𝑧<sup>𝑗</sup> = 𝑧∗(<em><strong>𝒙</strong></em>, 𝑓 ) for facility 𝑗</p>
<p>In many ML applications, one seeks to derive finite-sample guaran- tees, which are given in the form of a
    generalization bound, i.e., an</p>
<p>minimizes the average unmet demand<sup>𝑖</sup>. In the last s<sup>0</sup>tep, they retrain the random forest to
    minimize the reweighted MSE loss function:</p>
<p>upper bound on the difference between the true risk of a loss function</p>
<blockquote>
    <p>𝑁 𝑀 𝑇</p>
</blockquote>
<p>and its empirical risk estimate for a given sample size 𝑁. A general-</p>
<p>min ∑ ∑ ∑ 1 [𝑦̂<sup>𝑡,𝑗</sup> ≥ 𝑠<sup>𝑗</sup> + 𝑧<sup>𝑗</sup> ]
    |𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em><sub>𝑖</sub>) − 𝑦̂<sup>𝑡,𝑗</sup> |, <span
        id="_bookmark36" class="anchor"></span>(17)</p>
<p>(<a href="#_bookmark99">2022</a>) (extension of <a href="#_bookmark98">El Balghiti et al.</a>, <a
        href="#_bookmark98">2019</a>) based on Rademacher</p>
<p>complexity of the <strong>SPO</strong> loss composed with the prediction functions</p>
<p>𝑔<em><strong><sub>𝜽</sub></strong></em> ∈ . More <span class="underline"> speci</span>fically, the bound achieved
    in <a href="#_bookmark98">El Balghiti et al.</a></p>
<blockquote>
    <p><span class="underline">log(𝑁</span>)</p>
    <p>𝑁</p>
</blockquote>
<p>feature dimension are obtained using <strong>SPO</strong> function’s structure and if </p>
<p>satisfies a ‘‘strength’’ property. <a href="#_bookmark123">Hu et al.</a> (<a href="#_bookmark123">2022</a>) show that
    for linear CSO p<span class="underline">ro</span>blems, the generalization bound for MSE loss and
    <strong>SPO</strong> loss is O <span class="underline"> 1</span> while faster convergence rates for the SLO model
    compared</p>
<p><a href="#_bookmark101">to</a> ILO model are obtained under certain low-noise assumptions. <a
        href="#_bookmark101">El-</a></p>
<p>where 𝑀 is the number of facilities, 𝑦̂<sup>𝑡,𝑗</sup> and 𝑠<sup>𝑗</sup> denote the demand and inventory levels,
    respectively, at fac<sup>𝑖</sup>ility 𝑗. T<sup>𝑖</sup>he above model (<a href="#_bookmark36">17</a>) solves the
    optimization problem once during training, and is shown to be scalable for a medical allocation problem in Sierra
    Leone when compared to <a href="#_bookmark128">Kallus and Mao</a> (<a href="#_bookmark128">2022</a>) where splitting
    of the feature space is done based on the task loss.</p>
<p><a href="#_bookmark140">Lawless and Zhou</a> (<a href="#_bookmark140">2022</a>) introduce a loss function similar to
    <a href="#_bookmark79">Chung</a> <a href="#_bookmark79">et al.</a> (<a href="#_bookmark79">2022</a>) that weighs the
    prediction error with a regret term as follows:</p>
<blockquote>
    <p>∗ ∗ 2</p>
</blockquote>
<p><a href="#_bookmark101">machtoub et al.</a> (<a href="#_bookmark101">2023</a>) show that for non-linear optimization
    problems, SLO models stochastically dominate ILO in terms of their asymptotic optimality gaps when the hypothesis
    class covers the true distribution. When the model is misspecified, they show that ILO outperforms SLO
    asymptotically in a general nonlinear setting.</p>
<ol start="2" type="1">
    <li>
        <blockquote>
            <p><em>Surrogate loss for a stochastic forest</em></p>
        </blockquote>
    </li>
</ol>
<p><a href="#_bookmark128">Kallus and Mao</a> (<a href="#_bookmark128">2022</a>) propose an algorithm called
    <strong>StochOpt Forest</strong>, which generalizes the random-forest based local parameter estimation procedure in
    <a href="#_bookmark47">Athey et al.</a> (<a href="#_bookmark47">2019</a>). A second-order perturba- tion analysis of
    stochastic optimization problems allows them to scale to larger CSO problems since they can avoid solving an
    optimization problem at each candidate split. The policies obtained using their model are shown to be asymptotically
    consistent, and the benefit of ILO is illustrated by comparing their approach to the random forests of <a
        href="#_bookmark58">Bertsimas and Kallus</a> (<a href="#_bookmark58">2020</a>) on a set of problems with
    synthetic and real-world data.</p>
<ol start="3" type="1">
    <li>
        <blockquote>
            <p><em>Other surrogates</em></p>
        </blockquote>
    </li>
</ol>
<p><a href="#_bookmark219">Wilder et al.</a> (<a href="#_bookmark219">2019</a>) introduce <strong>ClusterNet</strong> to
    solve hard combina- torial graph optimization problems by learning incomplete graphs. The model combines graph
    convolution networks to embed the graphs in a continuous space and uses a soft version of k-means clustering to
    obtain a differential proxy for the combinatorial problems, e.g., community detection and facility location.
    Numerical experiments on a synthetic data set show that <strong>ClusterNet</strong> outperforms the two-stage SLO
    ap- proach of first learning the graph and then optimizing, as well as other baselines used in community detection
    and facility location.</p>
<p>Focusing on combinatorial problems, <a href="#_bookmark210">Vlastelica et al.</a> (<a href="#_bookmark210">2019</a>)
    pro- pose a differentiable black-box (<strong>DBB</strong>) approach to tackle the issue that</p>
<p>the Jacobian of 𝑧∗(<em><strong>𝒙</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>) is zero almost
    everywhere by approximating</p>
<p>the true loss function using an interpolation controlled in a way that balances between ‘‘informativeness of the
    gradient’’ and ‘‘faithfulness to the original function’’. Algorithmically, this is done by perturbing</p>
<p>the prediction 𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) in the direction
    ∇<em><strong><sub>𝒛</sub></strong></em>𝑐(𝑧∗(<em><strong>𝒙</strong></em>,
    𝑔<em><strong><sub>𝜽</sub></strong></em>), <em><strong>𝒚</strong></em>) and obtaining a</p>
<p>gradient of the surrogate loss based on the effect of this perturbation on the resulting perturbed action.</p>
<p><a href="#_bookmark79">Chung et al.</a> (<a href="#_bookmark79">2022</a>) introduce a computationally tractable ILO
    model to solve non-linear CSO problems. Using the first-order Taylor expansion of the task loss around the
    prediction, they introduce a</p>
<p>reweighted MSE loss function where weights are determined by taking</p>
<blockquote>
    <p>𝑑(𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>), <em><strong>𝒚</strong></em>) =
        [𝑐(𝑧 (<em><strong>𝒙</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>), <em><strong>𝒚</strong></em>)
        − 𝑐(𝑧 (<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>),
        <em><strong>𝒚</strong></em>)](<em><strong>𝒚</strong></em> −
        𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>)) . <span id="_bookmark37"
            class="anchor"></span>(18)</p>
</blockquote>
<p>Learning optimal <em><strong>𝜽</strong></em> from the above formulation involves an argmin differentiation. So, the
    authors provide a two-step polynomial time algorithm to approximately solve the above problem. It first computes</p>
<p>a pilot estimator 𝑔<em><strong><sub>𝜽</sub></strong></em>0 by solving (<a href="#_bookmark18">7</a>) with
    𝑑(𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>), <em><strong>𝒚</strong></em>) =
    (𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) − <em><strong>𝒚</strong></em>)2 and then
    solving (<a href="#_bookmark18">7</a>) with the distance function in (<a href="#_bookmark37">18</a>) where
    𝑐(𝑧∗(<em><strong>𝒙</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>), <em><strong>𝒚</strong></em>) is
    substituted with 𝑐(𝑧∗(<em><strong>𝒙</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>0 ),
    <em><strong>𝒚</strong></em>). The authors show that their simple algorithm performs comparably to
    <strong>SPO+</strong>.</p>
<p>We conclude this subsection on surrogate loss functions by mention- ing the efforts in <a href="#_bookmark201">Sun et
        al.</a> (<a href="#_bookmark201">2023</a>) to learn a cost point estimator (in an expected value-based model) to
    imitate the hindsight optimal solution. This is done by designing a surrogate loss function that penalizes how much
    the optimal basis optimality conditions are violated. They derive generalization error bounds for this new loss
    function and employ them to provide a bound on the sub-optimality of the minimal <em><strong>𝜽</strong></em>.</p>
<ol start="5" type="1">
    <li>
        <blockquote>
            <p><em>Training using a surrogate differentiable optimizer</em></p>
        </blockquote>
        <ol type="1">
            <li>
                <blockquote>
                    <p><span id="bookmark38" class="anchor"></span><em>Differentiable perturbed optimizer</em></p>
                </blockquote>
            </li>
        </ol>
    </li>
</ol>
<p>One way of obtaining a differentiable optimizer is to apply a stochastic perturbation to the parameters predicted by
    the ML model. Taking the case of expected value-based models as an example, the key idea is that although the
    gradient of the solution of the contextual problem with respect to the predicted parameters
    <em><strong>𝒚</strong></em>̂ ∶= 𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) is zero
    almost everywhere, if we perturb the predictor using a noise with differentiable density, then the expectation of
    the solution of the perturbed contextual problem,</p>
<blockquote>
    <p>𝑧̄<sup>𝜀</sup>(<em><strong>𝒙</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>) = E<sub>𝛹</sub>
        [𝑧̃<sup>𝜀</sup>(<em><strong>𝒙</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>, 𝛹 )] with
        𝑧̃<sup>𝜀</sup>(<em><strong>𝒙</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>, 𝛹 ) ∶= argmin
        𝑐(<em><strong>𝒛</strong></em>, 𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) + 𝜀𝛹
        ),</p>
</blockquote>
<p><em><strong>𝒛</strong></em>∈</p>
<p>where 𝜀 &gt; 0 controls the amount of perturbation, and more generally of the expected cost of the associated random
    policy E<sub>𝛹</sub> [𝐻(𝑧̃𝜀(⋅, 𝑔<em><strong><sub>𝜽</sub></strong></em>, 𝛹 ), P̂ 𝑁 )] can be shown to be
    smooth and differentiable. This idea is pro-</p>
<p>posed and exploited in <a href="#_bookmark56">Berthet et al.</a> (<a href="#_bookmark56">2020</a>), which focus on a
    bi- linear cost 𝑐(<em><strong>𝒛</strong></em>, <em><strong>𝒚</strong></em>) ∶= <em><strong>𝒚</strong></em>𝑇
    <em><strong>𝒛</strong></em> thus simplifying E<sub>𝛹</sub> [𝐻(𝑧̃𝜀(⋅, 𝑔<em><strong><sub>𝜽</sub></strong></em>,
    𝛹 ), P̂ 𝑁 )] =</p>
<p>𝐻(𝑧̄𝜀(⋅, 𝑔<em><strong><sub>𝜽</sub></strong></em>), P̂ 𝑁 ). Further, they show that when an imitation ILO model
</p>
<p>is used with a special form of Bregman divergence to capture the difference between 𝑧∗(<em><strong>𝒙</strong></em>,
    <em><strong>𝒚</strong></em>) and 𝑧̃𝜀(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>̂, 𝛹 ), the
    gradient of 𝐻<sub>Imitation</sub> (𝑧̃𝜀(⋅, 𝑔<em><strong><sub>𝜽</sub></strong></em>, 𝛹 ), P̂ ′ ) can be computed
    directly without needing to determine</p>
<p>the Jacobian <sup>𝑁</sup>of 𝑧̄𝜀(<em><strong>𝒙</strong></em>, 𝑔 ) (<a href="#_bookmark66">Blondel et al.</a>, <a
        href="#_bookmark66">2020</a>):</p>
<blockquote>
    <p>𝐻<sub>Imitation</sub>(𝑧̃<sup>𝜀</sup>(⋅, 𝑔<em><strong><sub>𝜽</sub></strong></em>, 𝛹 ), P̂ ′ ) ∶= EP̂
        [𝓁P𝙵𝙻(𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>), <em><strong>𝒚</strong></em>)]
    </p>
</blockquote>
<p>the gradient of task loss with respect to the prediction. To solve a large-scale multi-facility inventory allocation
    problem with few samples for each facility, they use a single random forest that can predict the demand across
    facilities and products. Assuming that each tree in the random forest provides an independent and identically
    distributed</p>
<p>where 𝓁P𝙵𝙻 is a perturbed Fenchel–Young loss (<strong>PFYL</strong>) given by:</p>
<blockquote>
    <p>𝓁P𝙵𝙻(<em><strong>𝒚</strong></em>̂, <em><strong>𝒚</strong></em>) ∶=
        <em><strong>𝒚</strong></em>̂<sup>𝑇</sup> 𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>,
        <em><strong>𝒚</strong></em>) − E<sub>𝛹</sub> [(<em><strong>𝒚</strong></em>̂ + 𝜀𝛹 )𝑇
        <em><strong>𝒛</strong></em>̃<sup>𝜀</sup>(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>̂, 𝛹 )] +
        𝜀𝛺<sub>P𝙵𝙻</sub>(𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>)),</p>
</blockquote>
<p>and 𝛺<sub>P𝙵𝙻</sub>(<em><strong>𝒛</strong></em>) is the Fenchel dual of 𝐹 (<em><strong>𝒚</strong></em>) ∶=
    −E<sub>𝛹</sub> [(<em><strong>𝒚</strong></em>+𝛹)𝑇 𝑧̃𝜀(<em><strong>𝒙</strong></em>,
    <em><strong>𝒚</strong></em>, 𝛹 )]. The gradient of the Fenchel–Young loss with respect to the model prediction</p>
<p>is given by:</p>
<blockquote>
    <p>∇<sub><em><strong>𝒚</strong></em>̂</sub>𝓁P(<em><strong>𝒚</strong></em>̂, <em><strong>𝒚</strong></em>) =
        𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>) −
        <em><strong>𝒛</strong></em>̄<sup>𝜀</sup>(<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>̂).</p>
</blockquote>
<p>The regularization ensures uniqueness and Lipschitz property of 𝑧∗(<em><strong>𝒙</strong></em>,</p>
<p>𝑓<em><strong><sub>𝜽</sub></strong></em>) with respect to 𝑓<em><strong><sub>𝜽</sub></strong></em> and leads to
    finite-sample guarantees. To cir-</p>
<p>Evaluating this gradient can be done through Monte Carlo evaluations by sampling perturbations and solving the
    corresponding perturbed problems.</p>
<p><a href="#_bookmark87">Dalle et al.</a> (<a href="#_bookmark87">2022</a>) introduce a multiplicative perturbation
    with the advantage that it preserves the sign of
    𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) without adding any bias:</p>
<blockquote>
    <p><em><strong>𝒛</strong></em>̃<sup>𝜀</sup>(<em><strong>𝒙</strong></em>,
        𝑔<em><strong><sub>𝜽</sub></strong></em>, 𝛹 ) ∶= argmin 𝑐(<em><strong>𝒛</strong></em>,
        𝑔<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>) ⊙ exp(𝜀𝛹 − 𝜀<sup>2</sup>∕2)),</p>
    <p><em><strong>𝒛</strong></em>∈</p>
</blockquote>
<p>where ⊙ is the Hadamard dot-product and the exponential is taken elementwise. <a href="#_bookmark87">Dalle et al.</a>
    (<a href="#_bookmark87">2022</a>) and <a href="#_bookmark198">Sun et al.</a> (<a href="#_bookmark198">2023</a>) also
    show that there is a one-to-one equivalence between the perturbed optimizer approach and using a regularized
    randomized version of the CSO problem for combinatorial problems with linear objective functions. Finally, <a
        href="#_bookmark87">Dalle et al.</a> (<a href="#_bookmark87">2022</a>) show an intimate connection between the
    perturbed minimizer approach proposed by <a href="#_bookmark56">Berthet et al.</a> (<a href="#_bookmark56">2020</a>)
    and surrogate loss functions approaches such as <strong>SPO+</strong> by casting them as special cases of a more
    general surrogate loss formulation.</p>
<p><a href="#_bookmark135">Kong et al.</a> (<a href="#_bookmark135">2022</a>) and <a href="#_bookmark161">Mulamba et
        al.</a> (<a href="#_bookmark161">2021</a>) consider an ‘‘energy- based’’ perturbed optimizer defined by its
    density of the form:</p>
<blockquote>
    <p><em><strong>𝒛</strong></em>̃𝜀(<em><strong>𝒙</strong></em>, 𝑓 ) ∼ <span class="underline">
            exp(−ℎ(<em><strong>𝒛</strong></em>,
            𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>))∕𝜀)</span> , <span id="_bookmark39"
            class="anchor"></span>(19)</p>
</blockquote>
<p>cumvent the challenge associated with non-differentiability of 𝑧∗(<em><strong>𝒙</strong></em>,
    𝑓<em><strong><sub>𝜽</sub></strong></em>)</p>
<p>with respect to <em><strong>𝜽</strong></em>, they replace 𝑧∗(<em><strong>𝒙</strong></em>, 𝑓 ) with a smooth
    approxi<sup>𝜆</sup>mation</p>
<p>𝑧̃<sub>𝜆</sub>(<em><strong>𝒙</strong></em>, 𝑓<em><strong><sub>𝜽</sub></strong></em>) that is learned using a
    random data set (<em><strong>𝒑</strong></em><sub>𝑖</sub>, <em><strong>𝒛</strong></em><sub>𝑖</sub>) generated by
    sampling <em><strong>𝒑</strong></em><sub>𝑖</sub> from the probability simplex over the discrete support and then
    finding the optimal solution <em><strong>𝒛</strong></em><sub>𝑖</sub>. They show asymptotic optimality and
    consistency of their solutions when the hypothesis class is well-</p>
<p>specified. They compare their approach to other ILO pipelines and to the SLO approach that estimates the conditional
    distribution using cross-entropy.</p>
<blockquote>
    <p><a href="#_bookmark85">Cristian et al.</a> (<a href="#_bookmark85">2022</a>) introduce the
        <strong>ProjectNet</strong> model to solve</p>
</blockquote>
<p>uncertain constrained linear programs in an end-to-end framework by training an optimal policy network, which employs
    a differentiable approximation of the step of projection to feasibility.</p>
<p>Another approach, related to <a href="#_bookmark56">Berthet et al.</a> (<a href="#_bookmark56">2020</a>), that
    generalizes beyond LPs is given in <a href="#_bookmark193">Shah et al.</a> (<a href="#_bookmark193">2022</a>) that
    constructs locally optimized decision losses (<strong>LODL</strong>) with supervised learning to directly evaluate
    the performance of the predictors on the downstream op- timization task. To learn a convex <strong>LODL</strong> for
    each data point, this</p>
<p>approach first generates labels in the neighborhood of label <em><strong>𝒚</strong></em><sub>𝑖</sub> in the</p>
<p>training set, e.g., by adding Gaussian noise, and then chooses the</p>
<blockquote>
    <p><em><strong><sup>𝜽</sup></strong></em> ∫ exp(−ℎ(<em><strong>𝒛</strong></em>′,
        𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>))∕𝜀)𝑑<em><strong>𝒛</strong></em>′</p>
    <p>parameter that minimizes the MSE between <strong>LODL</strong> and the downstream</p>
</blockquote>
<p>with 𝜀 = 1, in the context of an imitation ILO problem. This general form of perturbed optimizer captures a varying
    amount of perturbation through 𝜀, with <em><strong>𝒛</strong></em>̃𝜀(<em><strong>𝒙</strong></em>,
    𝑓<em><strong><sub>𝜽</sub></strong></em>) converging in distribution to 𝑧∗(<em><strong>𝒙</strong></em>,
    𝑓<em><strong><sub>𝜽</sub></strong></em>) as 𝜀 goes to zero. They employ the negative log-likelihood to measure
    the di- vergence between <em><strong>𝒛</strong></em>̃𝜀(<em><strong>𝒙</strong></em>,
    𝑓<em><strong><sub>𝜽</sub></strong></em>) and the hindsight optimal solution 𝑧∗(<em><strong>𝒙</strong></em>,
    <em><strong>𝒚</strong></em>). Given the difficulties associated with calculating the partition function</p>
<p>in the denominator of (<a href="#_bookmark39">19</a>), <a href="#_bookmark161">Mulamba et al.</a> (<a
        href="#_bookmark161">2021</a>) devise a surrogate loss function based on noise-contrastive estimation, which
    replaces likelihood with relative likelihood when compared to a set of sampled suboptimal solutions. This scheme is
    shown to improve the perfor- mance over <strong>SPO+</strong> and <strong>DBB</strong> in terms of expected regret
    performance for linear combinatorial CSO.</p>
<p>Based on the noise contrastive estimation approach of <a href="#_bookmark161">Mulamba</a> <a href="#_bookmark161">et
        al.</a> (<a href="#_bookmark161">2021</a>), <a href="#_bookmark151">Mandi et al.</a> (<a
        href="#_bookmark151">2022</a>) note that ILO for combinatorial problems can be viewed as a learning-to-rank
    problem. They propose surrogate loss functions, with closed-form expressions for gradients, that are used to train
    to rank feasible points in terms of performance on the downstream optimization problem. Unlike (<a
        href="#_bookmark161">Mulamba et al.</a>, <a href="#_bookmark161">2021</a>), <a href="#_bookmark135">Kong et
        al.</a> (<a href="#_bookmark135">2022</a>) tackles the partition function challenge by employing a
    self-normalized importance sampler that provides a discrete approximation. To avoid overfitting, the authors also
    intro- duce a regularization that penalizes the KL divergence between the perturbed optimizer distribution and a
    subjective posterior distribution</p>
<p>over perturbed optimal hindsight actions P(<em><strong>𝒛</strong></em>̃𝜀(<em><strong>𝒙</strong></em>,
    <em><strong>𝒚</strong></em>)|<em><strong>𝒚</strong></em>):</p>
<p>task loss. The <strong>LODL</strong> is used in place of the task-specific surrogate optimization layers and
    outperforms SLO on three resource allocation problems (linear top-1 item selection problem, web advertising, and
    portfolio optimization). The numerical experiments indicate that hand- crafted surrogate functions only perform
    better for the web advertising problem.</p>
<ol start="6" type="1">
    <li>
        <blockquote>
            <p><em>Applications</em></p>
        </blockquote>
    </li>
</ol>
<p>In this subsection, we discuss the applications of the ILO framework to a wide range of real-world problems.</p>
<blockquote>
    <p><a href="#_bookmark206">Tian et al.</a> (<a href="#_bookmark206">2023</a>) and <a href="#_bookmark207">Tian et
            al.</a> (<a href="#_bookmark207">2023</a>) use <strong>SPOT</strong> and noise-</p>
</blockquote>
<p>contrastive estimation method (<a href="#_bookmark161">Mulamba et al.</a>, <a href="#_bookmark161">2021</a>),
    respectively, to solve the maritime transportation problem. A comprehensive tu- <a href="#_bookmark208">torial</a>
    on prescriptive analytics methods for logistics is given in <a href="#_bookmark208">Tian</a> <a
        href="#_bookmark78">et</a> <a href="#_bookmark208">al.</a> (<a href="#_bookmark208">2023</a>).
    <strong>SPO</strong> has been used in solving last-mile delivery (<a href="#_bookmark78">Chu</a> <a
        href="#_bookmark78">et al.</a>, <a href="#_bookmark78">2023</a>) and ship inspection problems (<a
        href="#_bookmark222">Yan et al.</a>, <a href="#_bookmark222">2021</a>, <a href="#_bookmark223">2020</a>, <a
        href="#_bookmark224">2023</a>). <a href="#_bookmark89">Demirović et al.</a> (<a href="#_bookmark89">2019</a>)
    and <a href="#_bookmark90">Demirović et al.</a> (<a href="#_bookmark90">2020</a>) mini- mize the same expected
    regret as <strong>SPO</strong> for specific applications re- lated to ranking optimization and dynamic programming
    problems, respectively.</p>
<p><a href="#_bookmark175">Perrault et al.</a> (<a href="#_bookmark175">2020</a>) solve a Stackelberg security game with
    the ILO framework by learning the attack probability distribution over a discrete set of targets to maximize a
    surrogate for the defender’s expected utility. They show that their model results in higher expected</p>
<blockquote>
    <p>𝐻<sub>Imitation</sub>(<em><strong>𝒛</strong></em>̃<sup>𝜀</sup>(⋅, 𝑓<em><strong><sub>𝜽</sub></strong></em>),
        P̂ ′ ) ∶= − EP̂ [log(P(<em><strong>𝒛</strong></em>̃<sup>𝜀</sup>(<em><strong>𝒙</strong></em>,
        𝑓<em><strong><sub>𝜽</sub></strong></em>) = 𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>,
        <em><strong>𝒚</strong></em>))|<em><strong>𝒙</strong></em>, <em><strong>𝒚</strong></em>)]</p>
    <p>utility for the defender on synthetic and human subjects data than the</p>
    <p>sequential models that learn the attack probability by minimizing the</p>
    <p>+ 𝜆EP̂ 𝑁 [KL(P(<em><strong>𝒛</strong></em>̃ (<em><strong>𝒙</strong></em>,
        <em><strong>𝒚</strong></em>)|<em><strong>𝒚</strong></em>)‖<em><strong>𝒛</strong></em>̃
        (<em><strong>𝒙</strong></em>, 𝑓<em><strong><sub>𝜽</sub></strong></em>)|<em><strong>𝒙</strong></em>,
        <em><strong>𝒚</strong></em>)].</p>
    <p>𝜀 𝜀</p>
    <p>cross entropy loss. <a href="#_bookmark216">Wang et al.</a> (<a href="#_bookmark216">2020</a>) replace the
        large-scale optimiza-</p>
</blockquote>
<p>The authors show that their model outperforms ILO trained using <strong>SQP</strong></p>
<p>and <strong>cvxpylayers</strong> in terms of computational time and gives lower task loss than sequential models
    trained using MLE and policy learning with neural networks.</p>
<blockquote>
    <p><em>5.5.2. Supervised learning</em></p>
</blockquote>
<p><a href="#_bookmark113">Grigas et al.</a> (<a href="#_bookmark113">2021</a>) solve a CSO problem with a convex and
    non- negative decision regularizer 𝛺(<em><strong>𝒛</strong></em>) assuming that the uncertain param- eter
    <em><strong>𝒚</strong></em> has discrete support. Their model, called <strong>ICEO</strong>-𝜆, is thus trained</p>
<p>by solving:</p>
<p>tion problem with a low dimensional surrogate by reparameterizing the</p>
<p>feasible space of decisions. They observe significant performance im-</p>
<p>provements for non-convex problems compared to the strongly convex case.</p>
<p><a href="#_bookmark199">Stratigakos et al.</a> (<a href="#_bookmark199">2022</a>) solve an integrated forecasting and
    opti- mization model for trading in renewable energy that trains an ensemble</p>
<p>of prescriptive trees by randomly splitting the feature space  based</p>
<p>on the task-specific cost function. <a href="#_bookmark189">Sang et al.</a> (<a href="#_bookmark189">2022</a>)
    introduce an ILO framework for electricity price prediction for energy storage system arbitrage. They present a
    hybrid loss function to measure prediction</p>
<p>and decision errors and a hybrid stochastic gradient descent learning</p>
<blockquote>
    <p>(I𝙲E−𝜆) min</p>
</blockquote>
<p><em><strong>𝜽</strong></em></p>
<blockquote>
    <p>𝐻(𝑧<sup>∗</sup>(⋅, 𝑓<em><strong><sub>𝜽</sub></strong></em>), P̂ 𝑁 ) + 𝜆EP̂</p>
</blockquote>
<p>[𝛺(𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>, 𝑓<em><strong><sub>𝜽</sub></strong></em>))] (20a)</p>
<blockquote>
    <p>method. <a href="#_bookmark190">Sang et al.</a> (<a href="#_bookmark190">2023</a>) solve a voltage regulation
        problem using a</p>
    <p>similar hybrid loss function, and backpropagation is done by implic-</p>
    <p>s.t. 𝑧<sup>∗</sup>(<em><strong>𝒙</strong></em>, 𝑓<em><strong><sub>𝜽</sub></strong></em>) = argmin
        𝑐(<em><strong>𝒛</strong></em>, 𝑓<em><strong><sub>𝜽</sub></strong></em>(<em><strong>𝒙</strong></em>)) +
        𝜆𝛺(<em><strong>𝒛</strong></em>), ∀<em><strong>𝒙</strong></em>.</p>
    <p>itly differentiating the optimality conditions of a second-order cone</p>
    <p>𝜆 <em><strong>𝒛</strong></em></p>
</blockquote>
<p>(20b)</p>
<blockquote>
    <p>program.</p>
</blockquote>
<p><a href="#_bookmark146">Liu et al.</a> (<a href="#_bookmark146">2023</a>) use a DNN to model the routing behavior of
    users in a transportation network and learn the parameters by minimizing the mismatch between the flow prescribed by
    the variational inequality and the observed flow. The backward pass is obtained by applying the IFT to the
    variational inequality. <a href="#_bookmark213">Wahdany et al.</a> (<a href="#_bookmark213">2023</a>) propose an
    integrated model for wind-power forecasting that learns the parameters of a neural network to optimize the energy
    system costs under the system constraints. <a href="#_bookmark211">Vohra et al.</a> (<a
        href="#_bookmark211">2023</a>) apply similar ideas to develop end-to- end renewable energy generation forecasts,
    using multiple contextual sources such as satellite images and meteorological time series.</p>
<p><a href="#_bookmark69">Butler and Kwon</a> (<a href="#_bookmark69">2023b</a>) solves the contextual mean–variance
    port- folio (MVP) optimization problem by learning the parameters of the linear prediction model using the ILO
    framework. The covariance ma- trix is estimated using the exponentially weighted moving average model. They provide
    analytical solutions to unconstrained and equality- constrained MVP optimization problems and show that they
    outperform SLO models based on ordinary least squares regression. These analytical solutions lead to lower variance
    when compared with the exact so- lutions of the corresponding inequality-constrained MVP optimization problem.</p>
<h2 id="active-research-directions">Active research directions</h2>
<p><span id="_bookmark40" class="anchor"></span>We now summarize active and future research directions for further work
    in contextual optimization.</p>
<p><em>Uncertainty in constraints.</em> Most studies on contextual optimization as- sume that there is no uncertainty in
    the constraints. If constraints are also uncertain, the SAA solutions that ignore the covariates informa- <a
        href="#_bookmark58">tion</a> might not be feasible (<a href="#_bookmark180">Rahimian &amp; Pagnoncelli</a>, <a
        href="#_bookmark180">2022</a>). <a href="#_bookmark58">Bertsimas</a> <a href="#_bookmark58">and Kallus</a> (<a
        href="#_bookmark58">2020</a>) have highlighted the challenges in using ERM in a constrained CSO problem. <a
        href="#_bookmark180">Rahimian and Pagnoncelli</a> (<a href="#_bookmark180">2022</a>) solve a conditional
    chance-constrained program that ensures with a high probability that the solution remains feasible under the
    conditional distribution given the realized covariates. Although they do not focus on contextual optimization,
    interesting links can be found with the literature on constraint learning (<a href="#_bookmark106">Fajemisin et
        al.</a>, <a href="#_bookmark106">2023</a>) and inverse optimization (<a href="#_bookmark71">Chan et al.</a>, <a
        href="#_bookmark71">2021</a>).</p>
<p><em>Risk aversion.</em> There has been a growing interest in studying contextual optimization in the risk-averse
    setting. Specifically, one can consider replacing the risk-neutral expectation from (<a href="#_bookmark7">1</a>)
    with a risk measure such as value-at-risk. By doing so, one would expect, with a high probability, that a
    decision-maker’s loss is lower than a particular threshold. One can easily represent such a risk measure using an
    uncertainty set which represents the set of all possible outcomes that may occur in the future. The resulting
    uncertainty set should be carefully chosen. It should capture the most relevant scenarios to balance the trade-off
    between avoiding risks and obtaining returns. The recently proposed Conditional Robust Optimization (CRO) paradigm,
    pioneered by <a href="#_bookmark76">Chenreddy et al.</a> (<a href="#_bookmark76">2022</a>) (see also <a
        href="#_bookmark171">Ohmori</a>, <a href="#_bookmark171">2021</a>; <a href="#_bookmark173">Patel et al.</a>, <a
        href="#_bookmark173">2023</a>; <a href="#_bookmark176">Peršak &amp; Anjos</a>, <a href="#_bookmark176">2023</a>;
    <a href="#_bookmark202">Sun et al.</a>, <a href="#_bookmark202">2024</a>), consists in learning a conditional set 
    (<em><strong>𝒙</strong></em>) to solve the following problem:</p>
<blockquote>
    <p>(CRO) min max 𝑐(<em><strong>𝒛</strong></em>, <em><strong>𝒚</strong></em>), (21)</p>
    <p><em><strong>𝒚 𝒙</strong></em></p>
</blockquote>
<p>where  (<em><strong>𝒙</strong></em>) is an uncertainty set designed to contain with high proba- bility the
    realization of <em><strong>𝒚</strong></em> conditionally on observing <em><strong>𝒙</strong></em>. Their approach
    solves the CRO problem sequentially where  (<em><strong>𝒙</strong></em>) is learned first and is subsequently used
    to solve the downstream RO problem. A challenging problem is to learn the uncertainty set to minimize the downstream
    cost function (see recent developments in <a href="#_bookmark77">Chenreddy and Delage</a> (<a
        href="#_bookmark77">2024</a>)).</p>
<p><em>Toolboxes and benchmarking.</em> Several toolboxes and packages have been proposed recently to train decision
    pipelines. <a href="#_bookmark42">Agrawal et al.</a> (<a href="#_bookmark42">2019</a>) provide the
    <strong>cvxpylayers</strong> library, which includes a subclass of convex optimization problems as differentiable
    layers in auto-</p>
<p>differentiation libraries in PyTorch, TensorFlow, and JAX. Other li- braries for differentiating non-linear
    optimization problems for end-to- <a href="#_bookmark65">end</a> learning include higher (<a
        href="#_bookmark112">Grefenstette et al.</a>, <a href="#_bookmark112">2019</a>), JAXopt (<a
        href="#_bookmark65">Blondel</a> <a href="#_bookmark65">et al.</a>, <a href="#_bookmark65">2022</a>), TorchOpt
    (<a href="#_bookmark181">Ren et al.</a>, <a href="#_bookmark181">2022</a>), and Theseus (<a
        href="#_bookmark177">Pineda et al.</a>, <a href="#_bookmark177">2022</a>). <a href="#_bookmark205">Tang and
        Khalil</a> (<a href="#_bookmark205">2022</a>) introduce an open-source software pack- age called PyEPO
    (Pytorch-based End-to-End Predict-then-Optimize) implemented in Python for ILO of problems that are linear in
    uncertain parameters. They implement various existing methods, such as <strong>SPO+</strong>, <strong>DBB</strong>,
    and <strong>PFYL</strong>. They also include new benchmarks and comprehensive <a href="#_bookmark87">experiments</a>
    highlighting the advantages of integrated learning. <a href="#_bookmark87">Dalle</a> <a href="#_bookmark87">et
        al.</a> (<a href="#_bookmark87">2022</a>) provide similar tools for combinatorial problems in Julia.</p>
<p>Comparisons of existing approaches in fixed simulation settings are scarce, especially with real-world data. <a
        href="#_bookmark70">Buttler et al.</a> (<a href="#_bookmark70">2022</a>) provide a meta-analysis of selected
    methods on an unconstrained newsvendor problem on four data sets from the retail and food sectors. They highlight
    that there is no single method that clearly outperforms all the others on the four data sets. <a
        href="#_bookmark153">Mandi et al.</a> (<a href="#_bookmark153">2023</a>) carried out a comprehensive
    benchmarking of ILO frameworks tailored for expected value-based models on seven distinct problems using public data
    sets.</p>
<p><em>Endogenous uncertainty.</em> While there has been some progress in study- <a href="#_bookmark52">ing</a> problems
    where the decision affects the uncertain parameters (<a href="#_bookmark52">Bas-</a> <a href="#_bookmark52">ciftci
        et al.</a>, <a href="#_bookmark52">2021</a>; <a href="#_bookmark145">Liu et al.</a>, <a
        href="#_bookmark145">2022</a>), the literature on decision-dependent <a href="#_bookmark59">uncertainty</a> with
    covariates is sparse (<a href="#_bookmark58">Bertsimas &amp; Kallus</a>, <a href="#_bookmark58">2020</a>; <a
        href="#_bookmark59">Bertsi-</a> <a href="#_bookmark59">mas &amp; Koduri</a>, <a href="#_bookmark59">2022</a>).
    An example could be a facility location problem where demand changes once a facility is located in a region or a
    price- <a href="#_bookmark147">setting</a> newsvendor problem whose demand depends on the price (<a
        href="#_bookmark147">Liu</a> <a href="#_bookmark147">&amp; Zhang</a>, <a href="#_bookmark147">2023</a>). In
    these problems, the causal relationship between de- mand and prices is unknown. These examples offer interesting
    parallels <a href="#_bookmark212">with</a> the research on heterogeneous treatment effects such as <a
        href="#_bookmark212">Wager</a> <a href="#_bookmark212">and Athey</a> (<a href="#_bookmark212">2018</a>), which
    introduce causal forests for estimating treat- ment effects and provide asymptotic consistency results. <a
        href="#_bookmark44">Alley et al.</a> (<a href="#_bookmark44">2023</a>) study a price-setting problem and provide
    a new loss function to isolate the causal effects of price on demand from the conditional effects due to other
    covariates.</p>
<p><em>Data privacy.</em> Another issue is that the data might come from multiple sources and contain sensitive private
    information, so it cannot be directly provided in its original form to the system operator. Differ- ential privacy
    techniques (see, e.g., <a href="#_bookmark43">Abadi et al.</a>, <a href="#_bookmark43">2016</a>) can be used to
    obfuscate data but may impact predictive and prescriptive perfor- mance. <a href="#_bookmark158">Mieth et al.</a>
    (<a href="#_bookmark158">2023</a>) determine the data quality after obfuscation in an optimal power flow problem
    with a Wasserstein ambiguity set and use a DRO model to determine the data value for decision-making.</p>
<p><em>Interpretability &amp; explainability.</em> Decision pipelines must be trusted to be implemented. This is evident
    from the European Union legislation ‘‘General Data Protection Regulation’’ that requires entities using au- tomated
    systems to provide ‘‘meaningful information about the logic involved’’ in making decisions, known popularly as the
    ‘‘right to ex- planation’’ (<a href="#_bookmark96">Doshi-Velez &amp; Kim</a>, <a href="#_bookmark96">2017</a>; <a
        href="#_bookmark129">Kaminski</a>, <a href="#_bookmark129">2019</a>). For instance, a citizen has the right to
    ask a bank for an explanation in the case of loan denial. While interpretability has received much attention in
    predictive ML applications (<a href="#_bookmark184">Rudin</a>, <a href="#_bookmark184">2019</a>), it remains largely
    unexplored in a contextual optimization, i.e., prescriptive context. Interpretability requires transparent decision
    pipelines that are intelligible to users, e.g., built over simple models such as decision trees or rule lists. In
    contrast, explainability may be achieved with an additional algorithm on top of a black box or complex model.
    Feature importance has been analyzed in a prescriptive context by <a href="#_bookmark192">Serrano et al.</a> (<a
        href="#_bookmark192">2022</a>). They introduce an integrated approach that solves a bilevel program with an
    integer master problem optimizing (cross-)validation accuracy. To achieve explainability, <a
        href="#_bookmark110">Forel et al.</a> (<a href="#_bookmark110">2023</a>) adapt the concept of coun- terfactual
    explanations to explain a given data-driven decision through differences of context that make this decision optimal,
    or better suited than a given expert decision. Having identified these differences, it becomes possible to correct
    or complete the contextual information,</p>
<p>if necessary, or otherwise to give explanative elements supporting different decisions. Another research direction
    could be to train tree- based models (such as optimal classification trees) to approximate the policy of a complex
    learning-and-optimization pipeline. This has interesting connections with model distillation, i.e., the idea in the
    ML community of approximating a large model by a smaller one, and the work of <a href="#_bookmark61">Bertsimas and
        Stellato</a> (<a href="#_bookmark61">2021</a>), which learns the mapping from the problem parameters to optimal
    decisions through interpretable models.</p>
<p><em>Fairness.</em> Applying decisions based on contextual information can raise fairness issues when the context is
    made of protected attributes. This has been studied especially in pricing problems, to ensure that different
    customers or groups of customers are proposed prices that do not differ significantly (<a href="#_bookmark82">Cohen
        et al.</a>, <a href="#_bookmark82">2022</a>, <a href="#_bookmark83">2021</a>).</p>
<p><em>Finite sample guarantees for ILO.</em> In <a href="#_bookmark113">Grigas et al.</a> (<a
        href="#_bookmark113">2021</a>), the authors derive finite-sample guarantees for ILO under the assumption of dis-
    crete support for the uncertain parameter. An open problem is to derive generalization bounds on the performance of
    ILO models for non-linear problems where the uncertain parameters have continuous support.</p>
<p><em>Correcting for in-sample bias of data-driven optimization.</em> When devising an optimal policy based on a finite
    number of samples, it is desired that low in-sample risk translates to low out-of-sample risk. However, decision
    rule optimization in (<a href="#_bookmark14">4</a>) or learning and optimization model in (<a
        href="#_bookmark12">5</a>) are known to produce optimistically biased estimates of the <a
        href="#_bookmark84">true</a> expected cost of the prescribed policy (<a href="#_bookmark51">Ban &amp; Rudin</a>,
    <a href="#_bookmark51">2019</a>; <a href="#_bookmark84">Costa</a> <a href="#_bookmark84">&amp; Iyengar</a>, <a
        href="#_bookmark84">2023</a>; <a href="#_bookmark115">Gupta et al.</a>, <a href="#_bookmark115">2022</a>). While
    one can replace this estimation with an unbiased one if data was reserved for this purpose, this is usually
    considered a wasteful use of data given that it could instead have been used to obtain a better-performing policy.
    Recent research has identified ways of circumventing this issue by estimating and correcting for the in-sample bias
    in contextual (<a href="#_bookmark115">Gupta et al.</a>, <a href="#_bookmark115">2022</a>) and non-contextual (<a
        href="#_bookmark116">Gupta &amp; Rusmevichientong</a>, <a href="#_bookmark116">2021</a>; <a
        href="#_bookmark125">Ito et al.</a>, <a href="#_bookmark125">2018</a>; <a href="#_bookmark126">Iyengar et
        al.</a>, <a href="#_bookmark126">2023</a>) stochastic optimization problems under the assumption that errors in
    the estimation of uncertain parameters are normally distributed. A promising future research direction could be to
    build a general framework to learn the in-sample policies that directly minimize the debiased objective functions.
    In this regard, one might find inspiration from the work of <a href="#_bookmark116">Gupta and Rusmevichientong</a>
    (<a href="#_bookmark116">2021</a>) addressing a similar issue in the non-contextual setting.</p>
<p><em>Costly label acquisition.</em> In many applications, it is costly to gather observations of uncertain vectors and
    covariate pairs. For instance, in personalized pricing, surveys can be sent to customers to obtain information on
    the sensitivity of purchasing an item with respect to its price. However, creating, sending, and collecting the
    surveys may have a cost. <a href="#_bookmark143">Liu et al.</a> (<a href="#_bookmark143">2023</a>) develop an active
    learning approach to obtain labels to solve the <strong>SPO</strong> problem, while the more general case of
    developing active learning methods for non-linear contextual opti- mization is an interesting future direction. <a
        href="#_bookmark63">Besbes et al.</a> (<a href="#_bookmark63">2023</a>) provide theoretical results on the
    trade-off between the quality and quantity of data in a newsvendor problem, thus guiding decision-makers on how to
    invest in data acquisition strategies.</p>
<p><em>Multi-agent decision-making.</em> A multi-agent perspective becomes neces- sary in transportation and operations
    management problems, where dif- ferent agents have access to different sources of information (i.e. covari- ates).
    In this regard, some recent work by <a href="#_bookmark120">Heaton et al.</a> (<a href="#_bookmark120">2022</a>)
    identifies the Nash equilibrium of contextual games using implicit differentiation of variational inequalities and
    jacobian-free backpropagation.</p>
<p><em>Multi-stage contextual optimization.</em> Most works on contextual optimiza- <a href="#_bookmark182">tion</a>
    focus on single and two-stage problems. <a href="#_bookmark50">Ban et al.</a> (<a href="#_bookmark50">2019</a>) and
    <a href="#_bookmark182">Rios</a> <a href="#_bookmark182">et al.</a> (<a href="#_bookmark182">2015</a>) use the
    residuals of the regression model to build multi- <a href="#_bookmark60">stage</a> scenario trees and solve
    multi-stage CSO problems. <a href="#_bookmark60">Bertsimas</a></p>
<p><a href="#_bookmark60">et al.</a> (<a href="#_bookmark60">2023</a>) generalize the weighted SAA model for multi-stage
    prob- lems. <a href="#_bookmark179">Qi et al.</a> (<a href="#_bookmark179">2023</a>) propose an end-to-end learning
    framework to solve a real-world multistage inventory replenishment problem. An interesting research direction is to
    challenge the assumption that the joint distribution of the covariate and uncertain parameters is sta- tionary. <a
        href="#_bookmark164">Neghab et al.</a> (<a href="#_bookmark164">2022</a>) study a newsvendor model with a hid-
    den Markov model underlying the distribution of the covariates and demand.</p>
<p>An active area of research is sequential decision-making with un- certainty. Inverse reinforcement learning (<a
        href="#_bookmark165">Ng et al.</a>, <a href="#_bookmark165">2000</a>) focuses on learning rewards that are
    consistent with observed trajectories. In the econometrics literature on dynamic discrete choice modeling the focus
    lies more broadly on estimating structural parameters of Markov decision processes (MDPs) (<a
        href="#_bookmark186">Rust</a>, <a href="#_bookmark186">1988</a>) including rewards, transition functions and
    discount factors. On both topics, estimates are typically obtained through MLE employing a soft version of the
    Bellman opera- tor (e.g., <a href="#_bookmark185">Rust</a>, <a href="#_bookmark185">1987</a>; <a
        href="#_bookmark232">Ziebart et al.</a>, <a href="#_bookmark232">2008</a>). In the context of model-based
    reinforcement learning, so-called decision awareness (i.e. explicitly training components of a reinforcement
    learning system to help the agent improve the total amount of collected reward, <a href="#_bookmark169">Nikishin et
        al.</a>, <a href="#_bookmark114">2022</a>) is receiving increasing attention (e.g., <a
        href="#_bookmark107">Farahmand</a>, <a href="#_bookmark107">2018</a>; <a href="#_bookmark114">Grimm</a> <a
        href="#_bookmark114">et al.</a>, <a href="#_bookmark114">2020</a>). For example, (<a
        href="#_bookmark168">Nikishin et al.</a>, <a href="#_bookmark168">2022</a>) introduce an approach that combines
    learning and planning to optimize expected returns for both tabular and non-tabular MDPs. Finally, an area that
    requires attention is the deployment of models for real-world applications by tackling computational hurdles
    associated with decision-aware learning in MDPs, such as large state–action pairs and high-dimensional policy spaces
    (<a href="#_bookmark215">Wang et al.</a>, <a href="#_bookmark215">2023</a>). An example is a service call scheduling
    problem that is formulated as a restless multi-armed bandit problem in <a href="#_bookmark156">Mate et al.</a> (<a
        href="#_bookmark156">2022</a>) to improve maternal and child health in a non-profit organization.</p>
<h2 id="conclusions">Conclusions</h2>
<p><span id="_bookmark41" class="anchor"></span>In this survey, we summarize advancements in contextual opti- mization
    methods developed to solve decision-making problems under uncertainty. A salient feature of the problems studied is
    that a covariate that is correlated with the uncertain parameter is revealed to the decision-maker before a decision
    is made. Therefore, historical data on both covariates and corresponding uncertain parameter values can be used to
    prescribe decisions based on the covariate information. We showed that contextual optimization literature can be
    categorized into three data-driven frameworks for learning policies, namely, de- cision rule optimization,
    sequential learning and optimization, and integrated learning and optimization. While decision rule optimization
    explicitly parameterizes the policy as a function of the covariates, learning and optimization require estimating
    the conditional distri- bution (or a sufficient statistic in the case of expected value-based models) of the
    uncertain parameter given the covariates in a sequential or integrated manner. By providing a parametric description
    of these three data-driven frameworks, we have introduced a uniform nota- tion and terminology for analyzing
    different methods. In particular, we have elaborately described integrated learning and optimization models using
    both their training pipeline and modeling choice, further emphasizing that these two aspects are intertwined.
    Furthermore, we have provided a list of active fields of research in CSO problems.</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Utsav Sadana was supported by the GERAD postdoctoral fellowship and FRQNT postdoctoral research scholarship [Grant
    301065]. Erick Delage was partially supported by the Canadian Natural Sciences and Engineering Research Council
    [Grant RGPIN-2022-05261] and by the Canada Research Chair program [950-23005]. Emma Frejinger was par- tially
    supported by the Canada Research Chair program [950-232244]. Finally, the authors also acknowledge the support of
    IVADO, SCALE-AI</p>
<p>through its AI Research Chair Program, and the Canada First Research Excellence Fund (Apogée/CFREF).</p>
<h2 id="references">References</h2>
<blockquote>
    <p><span id="_bookmark43" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb1">Abadi, M., Chu, A., Goodfellow, I., McMahan, H.
            B., Mironov, I., Talwar, K., &amp; Zhang, L.</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb1">(2016). Deep learning with differential privacy.
            In <em>Proceedings of the 2016 ACM</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb1"><em>SIGSAC conference on computer and
                communications security</em> (pp. 308–318). NY, USA:</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb1">Association for Computing Machinery.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb2">Agrawal,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb2">A., Amos, B., Barratt, S., Boyd, S., Diamond,
            S., &amp; Kolter, J. Z. (2019). Dif-</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb2">ferentiable convex optimization layers. In
            <em>Advances in Neural Information Processing</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb2"><em>Systems</em>: <em>vol. 32</em>, Curran
            Associates, Inc.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb3">Alley,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb3">M., Biggs, M., Hariss, R., Herrmann, C., Li, M.
            L., &amp; Perakis, G. (2023). Pricing</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb3">for
            heterogeneous products: Analytics for ticket reselling. <em>Manufacturing &amp; Service</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb3"><em>Operations Management</em>, <em>25</em>(2),
            409–426.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb4">Amos, B., &amp; Kolter, J. Z. (2017). OptNet:
            Differentiable optimization as a layer in neural</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb4">networks. <em>Vol. 70</em>, In <em>International
                Conference on Machine Learning</em> (pp. 136–145).</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb4">PMLR.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb5">Aronszajn,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb5">N. (1950). Theory of reproducing kernels.
            <em>Transactions of the American</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb5"><em>Mathematical Society</em>, <em>68</em>(3),
            337–404.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb6">Athey,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb6">S., Tibshirani, J., &amp; Wager, S. (2019).
            Generalized random forests. <em>The Annals</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb6"><em>of Statistics</em>, <em>47</em>(2),
            1148–1178.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb7">Backhoff,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb7">J., Beiglbock, M., Lin, Y., &amp; Zalashko, A.
            (2017). Causal transport in discrete</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb7">time
            and applications. <em>SIAM Journal on Optimization</em>, <em>27</em>(4), 2528–2562.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb8">Bai,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb8">S., Kolter, J. Z., &amp; Koltun, V. (2019). Deep
            equilibrium models. In <em>Advances in</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb8"><em>Neural Information Processing Systems</em>:
            <em>Vol. 32</em>, Curran Associates, Inc.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb9">Ban,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb9">G.-Y., Gallien, J., &amp; Mersereau, A. J.
            (2019). Dynamic procurement of new</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb9">products with covariate information: The
            residual tree method. <em>Manufacturing &amp;</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb9"><em>Service Operations Management</em>,
            <em>21</em>(4), 798–815.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb10">Ban,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb10">G.-Y., &amp; Rudin, C. (2019). The Big Data
            Newsvendor: Practical Insights from</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb10">Machine Learning. <em>Operations Research</em>,
            <em>67</em>(1), 90–108.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb11">Basciftci, B., Ahmed, S., &amp; Shen, S. (2021).
            Distributionally robust facility location prob-</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb11">lem under decision-dependent stochastic demand.
            <em>European Journal of Operational</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb11"><em>Research</em>, <em>292</em>(2),
            548–561.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb12">Bazier-Matte,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb12">T., &amp; Delage, E. (2020). Generalization
            bounds for regularized portfolio</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb12">selection with market side information.
            <em>INFOR: Information Systems and Operational</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb12"><em>Research</em>, <em>58</em>(2), 374–401.</a>
    </p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb13">Bengio, Y. (1997). Using a financial training
            criterion rather than a prediction criterion.</a></p>
    <p><span id="_bookmark55" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb13"><em>International Journal of Neural
                Systems</em>, <em>8</em>(4), 433–443.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb14">Bengio,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb14">Y., Lodi, A., &amp; Prouvost, A. (2021).
            Machine learning for combinatorial</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb14">optimization: A methodological tour d’horizon.
            <em>European Journal of Operational</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb14"><em>Research</em>, <em>290</em>(2),
            405–421.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb15">Berthet,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb15">Q., Blondel, M., Teboul, O., Cuturi, M., Vert,
            J.-P., &amp; Bach, F. (2020).</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb15">Learning
            with differentiable perturbed optimizers. In <em>Advances in Neural Information</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb15"><em>Processing Systems</em>: <em>Vol. 33</em>,
            (pp. 9508–9519). Curran Associates, Inc.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb16">Bertsimas,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb16">D., Dunn, J., &amp; Mundru, N. (2019). Optimal
            prescriptive trees. <em>INFORMS</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb16"><em>Journal on Optimization</em>,
            <em>1</em>(2), 164–183.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb17">Bertsimas,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb17">D., &amp; Kallus, N. (2020). From predictive to
            prescriptive analytics.</a></p>
    <p><span id="_bookmark59" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb17"><em>Management Science</em>, <em>66</em>(3),
            1025–1044.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb18">Bertsimas,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb18">D., &amp; Koduri, N. (2022). Data-driven
            optimization: A reproducing kernel</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb18">Hilbert space approach. <em>Operations
                Research</em>, <em>70</em>(1), 454–471.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb19">Bertsimas,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb19">D., McCord, C., &amp; Sturt, B. (2023). Dynamic
            optimization with side</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb19">information.
            <em>European Journal of Operational Research</em>, <em>304</em>(2), 634–651.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb20">Bertsimas, D., &amp; Stellato, B. (2021). The
            voice of optimization. <em>Machine Learning</em>, <em>110</em>(2),</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb20">249–277.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb21">Bertsimas,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb21">D., &amp; Van Parys, B. (2022). Bootstrap
            robust prescriptive analytics.</a></p>
    <p><span id="_bookmark63" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb21"><em>Mathematical Programming</em>,
            <em>195</em>(1–2), 39–78.</a></p>
    <p>Besbes, O., Ma, W., &amp; Mouchtaki, O. (2023). Quality vs. Quantity of data in contextual decision-making: Exact
        analysis under newsvendor loss. arXiv preprint <a href="http://arxiv.org/abs/2302.08424">arXiv:2302.</a> <a
            href="http://arxiv.org/abs/2302.08424">08424</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb23">Birge, J. R., &amp; Louveaux, F. (2011).
            <em>Introduction to stochastic programming</em>. New York,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb23">NY: Springer.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb24">Blondel, M., Berthet, Q., Cuturi, M., Frostig,
            R., Hoyer, S., Llinares-López, F., Pe-</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb24">dregosa, F., &amp; Vert, J.-P. (2022). Efficient
            and modular implicit differentiation. In</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb24"><em>Advances in Neural Information Processing
                Systems</em>: <em>Vol. 35</em>, (pp. 5230–5242). Curran</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb24">Associates, Inc.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb25">Blondel, M., Martins, A. F., &amp; Niculae, V.
            (2020). Learning with Fenchel-Young losses.</a></p>
    <p><span id="_bookmark67" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb25"><em>Journal of Machine Learning Research</em>,
            <em>21</em>(1), 1314–1382.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb26">Bolte, J., Le, T., Pauwels, E., &amp;
            Silveti-Falls, T. (2021). Nonsmooth implicit differen-</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb26">tiation for machine-learning and optimization.
            In <em>Advances in Neural Information</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb26"><em>Processing Systems</em>: <em>Vol. 34</em>,
            (pp. 13537–13549).</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb27">Butler, A., &amp; Kwon, R. H. (2023a). Efficient
            differentiable quadratic programming</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb27">layers: an ADMM approach. <em>Computational
                Optimization and Applications</em>, <em>84</em>(2),</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb27">449–476.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb28">Butler, A., &amp; Kwon, R. H. (2023b).
            Integrating prediction in mean-variance portfolio</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb28">optimization. <em>Quantitative Finance</em>,
            <em>23</em>(3), 429–452.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb29">Buttler, S., Philippi, A., Stein, N., &amp;
            Pibernik, R. (2022). A meta analysis of data-driven</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb29">newsvendor approaches. In <em>ICLR 2022
                workshop on setting up ML evaluation standards</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb29"><em>to accelerate progress</em>.</a></p>
    <p><span id="_bookmark71" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb30">Chan,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb30">T. C., Mahmood, R., &amp; Zhu, I. Y. (2021).
            Inverse optimization: Theory and</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb30">applications. <em>Operations Research</em>.</a>
    </p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb31">Chen,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb31">B., Donti, P. L., Baker, K., Kolter, J. Z.,
            &amp; Bergés, M. (2021). Enforcing policy</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb31">feasibility constraints through differentiable
            projection for energy optimization. In</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb31"><em>Proceedings of the Twelfth ACM
                International Conference on Future Energy Systems</em> (pp.</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb31">199–210). ACM.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb32">Chen,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb32">R., &amp; Paschalidis, I. C. (2018). A robust
            learning approach for regression models</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb32">based on distributionally robust optimization.
            <em>Journal of Machine Learning Research</em>,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb32"><em>19</em>(13), 1–48.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb33">Chen, R., &amp; Paschalidis, I. (2019). Selecting
            optimal decisions via distributionally robust</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb33">nearest-neighbor regression. In <em>Advances in
                Neural Information Processing Systems</em>:</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb33"><em>Vol. 32</em>, Curran Associates, Inc.</a>
    </p>
    <p>Chen, W., Tanneau, M., &amp; Van Hentenryck, P. (2023). End-to-end feasible optimization</p>
    <p><span id="_bookmark76" class="anchor"></span>proxies for large-scale economic dispatch. arXiv preprint <a
            href="http://arxiv.org/abs/2304.11726">arXiv:2304.11726</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb35">Chenreddy,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb35">A. R., Bandi, N., &amp; Delage, E. (2022).
            Data-driven conditional robust</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb35">optimization. In <em>Advances in Neural
                Information Processing Systems</em>: <em>Vol. 35</em>, (pp.</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb35">9525–9537). Curran Associates, Inc.</a></p>
    <p>Chenreddy, A., &amp; Delage, E. 2024. End-to-end Conditional Robust Optimization, arXiv</p>
    <p><span id="_bookmark78" class="anchor"></span>preprint <a
            href="http://arxiv.org/abs/2403.04670">arXiv:2403.04670</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb37">Chu,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb37">H., Zhang, W., Bai, P., &amp; Chen, Y. (2023).
            Data-driven optimization for last-mile</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb37">delivery. <em>Complex &amp; Intelligent
                Systems</em>, <em>9</em>(9), 2271–2284.</a></p>
    <p>Chung, T.-H., Rostami, V., Bastani, H., &amp; Bastani, O. (2022). Decision-aware learning</p>
    <p><span id="_bookmark80" class="anchor"></span>for optimizing health supply chains. arXiv preprint <a
            href="http://arxiv.org/abs/2211.08507">arXiv:2211.08507</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb39">Ciocan, D. F., &amp; Mišić, V. V. (2022).
            Interpretable optimal stopping. <em>Management Science</em>,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb39"><em>68</em>(3), 1616–1638.</a></p>
</blockquote>
<p><span id="_bookmark82" class="anchor"></span><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb40">Clarke,
        F. H. (1990). <em>Optimization and nonsmooth analysis</em>. SIAM.</a></p>
<blockquote>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb41">Cohen,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb41">M. C., Elmachtoub, A. N., &amp; Lei, X. (2022).
            Price discrimination with fairness</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb41">constraints. <em>Management Science</em>,
            <em>68</em>(12), 8536–8552.</a></p>
    <p>Cohen, M. C., Miao, S., &amp; Wang, Y. (2021). Dynamic pricing with fairness constraints.</p>
    <p><span id="_bookmark84" class="anchor"></span>Available at <a
            href="https://doi.org/10.2139/ssrn.3930622">https://doi.org/10.2139/ssrn.3930622</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb43">Costa,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb43">G., &amp; Iyengar, G. N. (2023).
            Distributionally robust end-to-end portfolio</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb43">construction. <em>Quantitative Finance</em>,
            <em>23</em>(10), 1465–1482.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb44">Cristian,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb44">R., Harsha, P., Perakis, G., Quanz, B. L.,
            &amp; Spantidakis, I. (2022). End-</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb44">to-end learning via constraint-enforcing
            approximators for linear programs with</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb44">applications to supply chains. In <em>AI for
                Decision Optimization Workshop of the AAAI</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb44"><em>Conference on Artificial
                Intelligence</em>.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb45">Cybenko,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb45">G. (1989). Approximation by superpositions of a
            sigmoidal function.</a></p>
</blockquote>
<p><span id="_bookmark87" class="anchor"></span><a
        href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb45"><em>Mathematics of Control, Signals, and
            Systems</em>, <em>2</em>(4), 303–314.</a></p>
<blockquote>
    <p>Dalle, G., Baty, L., Bouvier, L., &amp; Parmentier, A. (2022). Learning with combinatorial <span id="_bookmark88"
            class="anchor"></span>optimization layers: a probabilistic approach. arXiv preprint <a
            href="http://arxiv.org/abs/2207.13513">arXiv:2207.13513</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb47">Davis,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb47">D., &amp; Yin, W. (2017). A three-operator
            splitting scheme and its optimization</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb47">applications. <em>Set-Valued and Variational
                Analysis</em>, <em>25</em>(4), 829–858.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb48">Demirović,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb48">E., J. Stuckey, P., Bailey, J., Chan, J.,
            Leckie, C., Ramamohanarao, K.,</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb48">Guns, T., &amp; Kraus, S. (2019). Predict+
            optimise with ranking objectives: Ex-</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb48">haustively learning linear functions. In
            <em>International Joint Conferences on Artificial</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb48"><em>Intelligence</em> (pp. 1078–1085).</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb49">Demirović,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb49">E., Stuckey, P. J., Guns, T., Bailey, J.,
            Leckie, C., Ramamohanarao, K., &amp;</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb49">Chan, J. (2020). Dynamic programming for
            predict+optimise. <em>Vol. 34</em>, In <em>AAAI</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb49"><em>Conference on Artificial Intelligence</em>
            (0202), (pp. 1444–1451).</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb50">Deng,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb50">Y., &amp; Sen, S. (2022). Predictive stochastic
            programming. <em>Computational</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb50"><em>Management Science</em>, <em>19</em>(1),
            65–98.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb51">Domke,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb51">J. (2012). Generic methods for
            optimization-based modeling. In <em>International</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb51"><em>Conference on Artificial Intelligence and
                Statistics</em> (pp. 318–326). PMLR.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb52">Dontchev,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb52">A. L., Rockafellar, R. T., &amp; Rockafellar,
            R. T. (2009). <em>Vol. 616</em>, <em>Implicit</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb52"><em>functions and solution mappings: A view
                from variational analysis</em>. Springer.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb53">Donti,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb53">P., Amos, B., &amp; Kolter, J. Z. (2017).
            Task-based end-to-end model learning in</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb53">stochastic optimization. In <em>Advances in
                Neural Information Processing Systems</em>: <em>vol.</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb53"><em>30</em>, Curran Associates, Inc.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb54">Donti,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb54">P. L., Roderick, M., Fazlyab, M., &amp; Kolter,
            J. Z. (2021). Enforcing robust control</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb54">guarantees within neural network policies. In
            <em>International Conference on Learning</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb54"><em>Representations</em>.</a></p>
    <p>Doshi-Velez, F., &amp; Kim, B. (2017). Towards a rigorous science of interpretable machine</p>
    <p><span id="_bookmark97" class="anchor"></span>learning. arXiv preprint <a
            href="http://arxiv.org/abs/1702.08608">arXiv:1702.08608</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb56">Duvenaud,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb56">D., Kolter, J. Z., &amp; Johnson, M. (2020).
            Deep implicit layers tutorial -</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb56">neural
            ODEs, deep equilibrium models, and beyond. In <em>Neural Information Processing</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb56"><em>Systems Tutorial</em>.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb57">El</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb57">Balghiti, O., Elmachtoub, A. N., Grigas, P.,
            &amp; Tewari, A. (2019). Generalization</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb57">bounds in the predict-then-optimize framework.
            In <em>Advances in Neural Information</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb57"><em>Processing Systems</em>: <em>vol. 32</em>,
            Curran Associates, Inc.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb58">El</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb58">Balghiti, O., Elmachtoub, A. N., Grigas, P.,
            &amp; Tewari, A. (2022). Generalization</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb58">bounds in the predict-then-optimize framework.
            <em>Mathematics of Operations Research</em>,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb58"><em>48</em>(4), 1811–2382.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb59">Elmachtoub, A. N., &amp; Grigas, P. (2022). Smart
            ‘‘Predict, then Optimize’’. <em>Management</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb59"><em>Science</em>, <em>68</em>(1), 9–26.</a></p>
    <p>Elmachtoub, A. N., Lam, H., Zhang, H., &amp; Zhao, Y. (2023). Estimate-then-optimize ver-</p>
    <p>sus integrated-estimation-optimization: A stochastic dominance perspective. arXiv <span id="_bookmark102"
            class="anchor"></span>preprint <a href="http://arxiv.org/abs/2304.06833">arXiv:2304.06833</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb61">Elmachtoub, A. N., Liang, J. C. N., &amp;
            McNellis, R. (2020). Decision trees for decision-</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb61">making under the predict-then-optimize
            framework. In <em>International Conference on</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb61"><em>Machine Learning</em> (pp. 2858–2867).
            PMLR.</a></p>
    <p><span id="_bookmark103" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb62">Esteban-Pérez,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb62">A., &amp; Morales, J. M. (2022).
            Distributionally robust stochastic programs</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb62">with side information based on trimmings.
            <em>Mathematical Programming</em>, <em>195</em>(1),</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb62">1069–1105.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb63">Esteban-Pérez,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb63">A., &amp; Morales, J. M. (2023).
            Distributionally robust optimal power</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb63">flow with contextual information. <em>European
                Journal of Operational Research</em>, <em>306</em>(3),</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb63">1047–1058.</a></p>
</blockquote>
<p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb64">Estes, A. S., &amp; Richard, J.-P. P. (2023). Smart
        predict-then-optimize for two-stage linear</a> <a
        href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb64">programs with side information. <em>INFORMS Journal
            on Optimization</em>, <em>5</em>(3), 233–320.</a></p>
<blockquote>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb65">Fajemisin,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb65">A. O., Maragno, D., &amp; den Hertog, D.
            (2023). Optimization with constraint</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb65">learning: a framework and survey. <em>European
                Journal of Operational Research</em>.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb66">Farahmand,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb66">A.-M. (2018). Iterative value-aware model
            learning. In S. Bengio, H. Wal-</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb66">lach, H.
            Larochelle, K. Grauman, N. Cesa-Bianchi, &amp; R. Garnett (Eds.), <em>Advances in</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb66"><em>Neural Information Processing Systems</em>:
            <em>vol. 31</em>.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb67">Ferber,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb67">A., Wilder, B., Dilkina, B., &amp; Tambe, M.
            (2020). MIPaaL: Mixed integer</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb67">program as
            a layer. <em>Vol. 34</em>, In <em>AAAI Conference on Artificial Intelligence</em> (02), (pp.</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb67">1504–1511).</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb68">Ferreira,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb68">K. J., Lee, B. H., &amp; Simchi-Levi, D.
            (2016). Analytics for an online retailer:</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb68">Demand forecasting and price optimization.
            <em>Manufacturing &amp; Service Operations</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb68"><em>Management</em>, <em>18</em>(1), 69–88.</a>
    </p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb69">Forel,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb69">A., Parmentier, A., &amp; Vidal, T. (2023).
            Explainable data-driven optimization:</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb69">From context to decision and back again. In
            <em>International Conference on Machine</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb69"><em>Learning</em> (pp. 10170–10187). PMLR.</a>
    </p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb70">Fung,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb70">S. W., Heaton, H., Li, Q., Mckenzie, D., Osher,
            S., &amp; Yin, W. (2022). JFB:</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb70">Jacobian-free backpropagation for implicit
            networks. <em>Vol. 36</em>, In <em>AAAI Conference</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb70"><em>on Artificial Intelligence</em> (66), (pp.
            6648–6656).</a></p>
    <p>Grefenstette, E., Amos, B., Yarats, D., Htut, P. M., Molchanov, A., Meier, F., Kiela, D.,</p>
    <p>Cho, K., &amp; Chintala, S. (2019). Generalized inner loop meta-learning. arXiv preprint <a
            href="http://arxiv.org/abs/1910.01727">arXiv:1910.01727</a>.</p>
    <p>Grigas, P., Qi, M., &amp; Shen, M. (2021). Integrated conditional estimation-optimization. <span
            id="_bookmark114" class="anchor"></span>arXiv preprint <a
            href="http://arxiv.org/abs/2110.12351">arXiv:2110.12351</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb73">Grimm,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb73">C., Barreto, A., Singh, S., &amp; Silver, D.
            (2020). The value equivalence principle</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb73">for model-based reinforcement learning. In
            <em>Advances in Neural Information Processing</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb73"><em>Systems</em>: <em>vol. 33</em>.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb74">Gupta,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb74">V., Huang, M., &amp; Rusmevichientong, P.
            (2022). Debiasing in-sample policy</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb74">performance for small-data, large-scale
            optimization. <em>Operations Research</em>.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb75">Gupta,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb75">V., &amp; Rusmevichientong, P. (2021).
            Small-data, large-scale linear optimization</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb75">with uncertain objectives. <em>Management
                Science</em>, <em>67</em>(1), 220–241.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb76">Halkin,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb76">H. (1974). Implicit functions and optimization
            problems without continuous</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb76">differentiability of the data. <em>SIAM Journal
                on Control</em>, <em>12</em>(2), 229–236.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb77">Hannah,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb77">L., Powell, W., &amp; Blei, D. (2010).
            Nonparametric density estimation for</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb77">stochastic optimization with an observable
            state variable. In <em>Advances in Neural</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb77"><em>Information Processing Systems</em>:
            <em>vol. 23</em>, Curran Associates, Inc.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb78">Hastie,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb78">T., Tibshirani, R., &amp; Friedman, J. H.
            (2009). <em>Vol. 2</em>, <em>The elements of statistical</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb78"><em>learning: data mining, inference, and
                prediction</em>. Springer.</a></p>
    <p>Heaton, H., McKenzie, D., Li, Q., Fung, S. W., Osher, S., &amp; Yin, W. (2022). Learn to</p>
    <p><span id="_bookmark121" class="anchor"></span>predict equilibria via fixed point networks. <a
            href="http://arxiv.org/abs/2106.00906">arXiv:2106.00906</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb80">Ho-Nguyen,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb80">N., &amp; Kılınç-Karzan, F. (2022). Risk
            guarantees for end-to-end prediction</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb80">and
            optimization processes. <em>Management Science</em>, <em>68</em>(12), 8680–8698.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb81">Hofmann,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb81">T., Schölkopf, B., &amp; Smola, A. (2008).
            Kernel methods in machine learning.</a></p>
    <p><span id="_bookmark123" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb81"><em>The Annals of Statistics</em>,
            <em>36</em>(3), 1171–1220.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb82">Hu, Y., Kallus, N., &amp; Mao, X. (2022). Fast
            rates for contextual linear optimization.</a></p>
    <p><span id="_bookmark124" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb82"><em>Management Science</em>, <em>68</em>(6),
            4236–4245.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb83">Huber, J., Müller, S., Fleischmann, M., &amp;
            Stuckenschmidt, H. (2019). A data-driven</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb83">newsvendor problem: From data to decision.
            <em>European Journal of Operational</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb83"><em>Research</em>, <em>278</em>(3),
            904–915.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb84">Ito, S., Yabe, A., &amp; Fujimaki, R. (2018).
            Unbiased objective estimation in predictive</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb84">optimization. In <em>International Conference
                on Machine Learning</em> (pp. 2176–2185).</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb84">PMLR.</a></p>
</blockquote>
<p>Iyengar, G., Lam, H., &amp; Wang, T. (2023). Optimizer’s information criterion: Dissecting</p>
<blockquote>
    <p><span id="_bookmark127" class="anchor"></span>and correcting bias in data-driven optimization. arXiv preprint <a
            href="http://arxiv.org/abs/2306.10081">arXiv:2306.10081</a>. <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb86">Jeong, J., Jaggi, P., Butler, A., &amp; Sanner,
            S. (2022). An exact symbolic reduction of</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb86">linear smart predict+optimize to mixed integer
            linear programming. <em>Vol. 162</em>, In</a></p>
    <p><span id="_bookmark128" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb86"><em>International Conference on Machine
                Learning</em> (pp. 10053–10067). PMLR.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb87">Kallus, N., &amp; Mao, X. (2022). Stochastic
            optimization forests. <em>Management Science</em>, <em>69</em>(4),</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb87">1975–1994.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb88">Kaminski, M. E. (2019). The right to explanation,
            explained. <em>Berkeley Technology Law</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb88"><em>Journal</em>, <em>34</em>(1), 189–218.</a>
    </p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb89">Kannan, R., Bayraksan, G., &amp; Luedtke, J. R.
            (2020). Residuals-based distributionally</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb89">robust optimization with covariate information.
            <em>Mathematical Programming</em>.</a></p>
    <p>Kannan, R., Bayraksan, G., &amp; Luedtke, J. (2021). Heteroscedasticity-aware residuals-</p>
    <p><span id="_bookmark132" class="anchor"></span>based contextual stochastic optimization. arXiv preprint <a
            href="http://arxiv.org/abs/2101.03139">arXiv:2101.03139</a>.</p>
    <p>Kannan, R., Bayraksan, G., &amp; Luedtke, J. R. (2022). Data-driven sample average <span id="_bookmark133"
            class="anchor"></span>approximation with covariate information. arXiv preprint <a
            href="http://arxiv.org/abs/2207.13554">arXiv:2207.13554</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb92">Kantorovich, L. V., &amp; Rubinshtein, G. S.
            (1958). On a space of totally additive functions.</a></p>
    <p><span id="_bookmark134" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb92"><em>Vestnik Leningradskogo Universiteta</em>,
            <em>13</em>(7), 52–59.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb93">Keshavarz, P. (2022). <em>Interpretable
                contextual newsvendor models: A tree-based method to</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb93"><em>solving data-driven newsvendor
                problems</em> (pp. 1616–1638). University of Ottawa.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb94">Kong, L., Cui, J., Zhuang, Y., Feng, R., Prakash,
            B. A., &amp; Zhang, C. (2022). End-to-end</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb94">stochastic optimization with energy-based
            model. In <em>Advances in Neural Information</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb94"><em>Processing Systems</em>: <em>vol. 35</em>,
            (pp. 11341–11354). Curran Associates, Inc.</a></p>
    <p><span id="_bookmark137" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb95">Kotary, J., Dinh, M. H., &amp; Fioretto, F.
            (2023). Folded optimization for end-to-end</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb95">model-based learning. In <em>International
                Joint Conference on Artificial Intelligence</em>.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb96">Kotary, J., Fioretto, F., Van Hentenryck, P.,
            &amp; Wilder, B. (2021). End-to-end constrained</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb96">optimization learning: A survey. In
            <em>International Joint Conferences on Artificial</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb96"><em>Intelligence</em> (pp. 4475–4482).</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb97">Kullback, S., &amp; Leibler, R. A. (1951). On
            information and sufficiency. <em>The Annals of</em></a></p>
    <p><span id="_bookmark139" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb97"><em>Mathematical Statistics</em>,
            <em>22</em>(1), 79–86.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb98">Lassalle, R. (2018). Causal transport plans and
            their Monge–Kantorovich problems.</a></p>
    <p><span id="_bookmark140" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb98"><em>Stochastic Analysis and Applications</em>,
            <em>36</em>(3), 452–484.</a></p>
    <p>Lawless, C., &amp; Zhou, A. (2022). A note on task-aware loss via reweighing prediction <span id="_bookmark141"
            class="anchor"></span>loss by decision-regret. arXiv preprint <a
            href="http://arxiv.org/abs/2211.05116">arXiv:2211.05116</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb100">Lin, S., Chen, Y., Li, Y., &amp; Shen, Z.-J. M.
            (2022). Data-driven newsvendor problems</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb100">regularized by a profit risk constraint.
            <em>Production and Operations Management</em>, <em>31</em>(4),</a></p>
    <p><span id="_bookmark142" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb100">1630–1644.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb101">Liu, H., &amp; Grigas, P. (2021). Risk bounds
            and calibration for a smart predict-then-</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb101">optimize method. In <em>Advances in Neural
                Information Processing Systems</em>: <em>vol. 34</em>, (pp.</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb101">22083–22094). Curran Associates, Inc.</a></p>
    <p>Liu, M., Grigas, P., Liu, H., &amp; Shen, Z.-J. M. (2023). Active learning in the predict-</p>
    <p>then-optimize framework: A margin-based approach. arXiv preprint <a
            href="http://arxiv.org/abs/2305.06584">arXiv:2305.</a> <a href="http://arxiv.org/abs/2305.06584">06584</a>.
    </p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb103">Liu, S., He, L., &amp; Max Shen, Z.-J. (2021).
            On-time last-mile delivery: Order assignment</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb103">with travel-time predictors. <em>Management
                Science</em>, <em>67</em>(7), 4095–4119.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb104">Liu, J., Li, G., &amp; Sen, S. (2022). Coupled
            learning enabled stochastic programming with</a></p>
    <p><span id="_bookmark146" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb104">endogenous uncertainty. <em>Mathematics of
                Operations Research</em>, <em>47</em>(2), 1681–1705.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb105">Liu, Z., Yin, Y., Bai, F., &amp; Grimm, D. K.
            (2023). End-to-end learning of user</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb105">equilibrium with implicit neural networks.
            <em>Transportation Research Part C (Emerging</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb105"><em>Technologies)</em>, <em>150</em>, Article
            104085.</a></p>
    <p>Liu, W., &amp; Zhang, Z. (2023). Solving data-driven newsvendor pricing problems with</p>
    <p><span id="_bookmark148" class="anchor"></span>decision-dependent effect. arXiv preprint <a
            href="http://arxiv.org/abs/2304.13924">arXiv:2304.13924</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb107">Liyanage, L. H., &amp; Shanthikumar, J. G.
            (2005). A practical inventory control policy</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb107">using operational statistics. <em>Operations
                Research Letters</em>, <em>33</em>(4), 341–348.</a></p>
    <p>Loke, G. G., Tang, Q., &amp; Xiao, Y. (2022). Decision-driven regularization: A</p>
    <p>blended model for predict-then-optimize. Available at <a
            href="https://doi.org/10.2139/ssrn.3623006">https://doi.org/10.2139/ssrn.</a> <a
            href="https://doi.org/10.2139/ssrn.3623006">3623006</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb109">Lu, Z., Pu, H., Wang, F., Hu, Z., &amp; Wang, L.
            (2017). The expressive power of neural</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb109">networks: A view from the width. In
            <em>Advances in Neural Information Processing</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb109"><em>Systems</em>, Curran Associates, Inc.</a>
    </p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb110">Mandi, J., Bucarey, V., Tchomba, M. M. K., &amp;
            Guns, T. (2022). Decision-focused</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb110">learning: Through the lens of learning to
            rank. In <em>International Conference on</em></a></p>
    <p><span id="_bookmark152" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb110"><em>Machine Learning</em> (pp. 14935–14947).
            PMLR.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb111">Mandi, J., &amp; Guns, T. (2020). Interior point
            solving for LP-based predic-</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb111">tion+optimisation. In <em>Advances in Neural
                Information Processing Systems</em>: <em>vol. 33</em>,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb111">(pp. 7272–7282). Curran Associates, Inc.</a>
    </p>
    <p>Mandi, J., Kotary, J., Berden, S., Mulamba, M., Bucarey, V., Guns, T., &amp; Fioretto, F.</p>
    <p>(2023). Decision-focused learning: Foundations, state of the art, benchmark and <span id="_bookmark154"
            class="anchor"></span>future opportunities. arXiv preprint <a
            href="http://arxiv.org/abs/2307.13565">arXiv:2307.13565</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb113">Mandi, J., Stuckey, P. J., &amp; Guns, T.
            (2020). Smart predict-and-optimize for hard</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb113">combinatorial optimization problems. <em>Vol.
                34</em>, In <em>AAAI Conference on Artificial</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb113"><em>Intelligence</em> (02), (pp.
            1603–1610).</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb114">Martínez-de Albeniz, V., &amp; Belkaid, A.
            (2021). Here comes the sun: Fashion goods</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb114">retailing under weather fluctuations.
            <em>European Journal of Operational Research</em>,</a></p>
    <p><span id="_bookmark156" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb114"><em>294</em>(3), 820–830.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb115">Mate, A., Madaan, L., Taneja, A., Madhiwalla,
            N., Verma, S., Singh, G., Hegde, A.,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb115">Varakantham, P., &amp; Tambe, M. (2022). Field
            study in deploying restless multi-</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb115">armed bandits: Assisting non-profits in
            improving maternal and child health. <em>Vol.</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb115"><em>36</em>, In <em>AAAI Conference on
                Artificial Intelligence</em> (1111), (pp. 12017–12025).</a></p>
    <p>McKenzie, D., Fung, S. W., &amp; Heaton, H. (2023). Faster predict-and-optimize with</p>
    <p><span id="_bookmark158" class="anchor"></span>three-operator splitting. arXiv preprint <a
            href="http://arxiv.org/abs/2301.13395">arXiv:2301.13395</a>.</p>
    <p>Mieth, R., Morales, J. M., &amp; Poor, H. V. (2023). Data valuation from data-driven <span id="_bookmark159"
            class="anchor"></span>optimization. arXiv preprint <a
            href="http://arxiv.org/abs/2305.01775">arXiv:2305.01775</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb118">Mišić, V. V., &amp; Perakis, G. (2020). Data
            analytics in operations management: A review.</a></p>
    <p><span id="_bookmark160" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb118"><em>Manufacturing &amp; Service Operations
                Management</em>, <em>22</em>(1), 158–169.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb119">Monga, V., Li, Y., &amp; Eldar, Y. C. (2021).
            Algorithm unrolling: Interpretable, efficient</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb119">deep learning for signal and image processing.
            <em>IEEE Signal Processing Magazine</em>,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb119"><em>38</em>(2), 18–44.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb120">Mulamba, M., Mandi, J., Diligenti, M., Lombardi,
            M., Lopez, V. B., &amp; Guns, T. (2021).</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb120">Contrastive losses and solution caching for
            predict-and-optimize. In <em>International</em></a></p>
    <p><span id="_bookmark162" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb120"><em>Joint Conference on Artificial
                Intelligence</em> (pp. 2833–2840).</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb121">Muñoz, M. A., Pineda, S., &amp; Morales, J. M.
            (2022). A bilevel framework for</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb121">decision-making under uncertainty with
            contextual information. <em>Omega</em>, <em>108</em>, Article</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb121">102575.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb122">Nadaraya, E. (1964). On estimating regression.
            <em>Theory of Probability &amp; its Applications</em>,</a></p>
    <p><span id="_bookmark164" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb122"><em>9</em>(1), 141–142.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb123">Neghab, D. P., Khayyati, S., &amp; Karaesmen, F.
            (2022). An integrated data-driven method</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb123">using deep learning for a newsvendor problem
            with unobservable features. <em>European</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb123"><em>Journal of Operational Research</em>,
            <em>302</em>(2), 482–496.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb124">Ng, A. Y., &amp; Russell, S. (2000). Algorithms
            for inverse reinforcement learning. <em>Vol. 1</em>,</a></p>
    <p><span id="_bookmark166" class="anchor"></span><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb124">In
            <em>International Conference on Machine Learning</em> (p. 2).</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb125">Nguyen, V. A., Zhang, F., Blanchet, J., Delage,
            E., &amp; Ye, Y. (2020). Distributionally ro-</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb125">bust local non-parametric conditional
            estimation. In <em>Advances in Neural Information</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb125"><em>Processing Systems</em>: <em>vol. 33</em>,
            (pp. 15232–15242). Curran Associates, Inc.</a></p>
    <p><span id="_bookmark167" class="anchor"></span>Nguyen, V. A., Zhang, F., Blanchet, J., Delage, E., &amp; Ye, Y.
        (2021). Robustifying conditional portfolio decisions via optimal transport. arXiv preprint <a
            href="http://arxiv.org/abs/2103.16451">arXiv:2103.</a> <a href="http://arxiv.org/abs/2103.16451">16451</a>.
    </p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb127">Nikishin,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb127">E., Abachi, R., Agarwal, R., &amp; Bacon,
            P.-L. (2022). Control-oriented model-</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb127">based reinforcement learning with implicit
            differentiation. <em>Vol. 36</em>, In <em>AAAI</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb127"><em>Conference on Artificial Intelligence</em>
            (7), (pp. 7886–7894).</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb128">Nikishin,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb128">E., D’Oro, P., Precup, D., Barreto, A.,
            massoud Farahmand, A., Bacon, P.-L., &amp;</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb128">Hall, G. (2022). Decision awareness in
            reinforcement learning. In <em>Workshop abstract.</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb128"><em>International Conference on Machine
                Learning</em>.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb129">Notz,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb129">P. M., &amp; Pibernik, R. (2022). Prescriptive
            analytics for flexible capacity</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb129">management. <em>Management Science</em>,
            <em>68</em>(3), 1756–1775.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb130">Ohmori, S. (2021). A predictive prescription
            using minimum volume k-nearest neighbor</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb130">enclosing ellipsoid and robust optimization.
            <em>Mathematics</em>, <em>9</em>(2), 119.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb131">Oroojlooyjadid,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb131">A., Snyder, L. V., &amp; Takáč, M. (2020).
            Applying deep learning to the</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb131">newsvendor problem. <em>IISE
                Transactions</em>, <em>52</em>(4), 444–463.</a></p>
    <p>Patel, Y., Rayan, S., &amp; Tewari, A. (2023). Conformal contextual robust optimization.</p>
    <p><span id="_bookmark174" class="anchor"></span>arXiv preprint <a
            href="http://arxiv.org/abs/2310.10003">arXiv:2310.10003</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb133">Perakis,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb133">G., Sim, M., Tang, Q., &amp; Xiong, P. (2023).
            Robust pricing and production with</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb133">information partitioning and adaptation.
            <em>Management Science</em>, <em>69</em>(3), 1398–1419.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb134">Perrault,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb134">A., Wilder, B., Ewing, E., Mate, A., Dilkina,
            B., &amp; Tambe, M. (2020). End-</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb134">to-end
            game-focused learning of adversary behavior in security games. <em>Vol. 34</em>, In</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb134"><em>AAAI Conference on Artificial
                Intelligence</em> (02), (pp. 1378–1386).</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb135">Peršak, E., &amp; Anjos, M. F. (2023).
            Contextual robust optimisation with uncertainty quan-</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb135">tification. In <em>International Conference on
                the Integration of Constraint Programming,</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb135"><em>Artificial Intelligence, and Operations
                Research</em> (pp. 124–132). Springer.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb136">Pineda,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb136">L., Fan, T., Monge, M., Venkataraman, S.,
            Sodhi, P., Chen, R. T., Ortiz, J.,</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb136">DeTone, D., Wang, A., &amp; Anderson, S. (2022).
            Theseus: A library for differentiable</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb136">nonlinear optimization. In <em>Advances in
                Neural Information Processing Systems</em>: <em>vol.</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb136"><em>35</em>, (pp. 3801–3818). Curran
            Associates, Inc.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb137">Qi,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb137">M., &amp; Shen, Z.-J. (2022). Integrating
            prediction/estimation and optimization with</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb137">applications in operations management. In
            <em>Tutorials in operations research: emerging</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb137"><em>and impactful topics in operations</em>
            (pp. 36–58). INFORMS.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb138">Qi,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb138">M., Shi, Y., Qi, Y., Ma, C., Yuan, R., Wu, D.,
            &amp; Shen, Z.-J. M. (2023). A practical</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb138">end-to-end inventory management model with
            deep learning. <em>Management Science</em>,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb138"><em>69</em>(2), 759–773.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb139">Rahimian,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb139">H., &amp; Pagnoncelli, B. (2022). Data-driven
            approximation of contextual</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb139">chance-constrained stochastic programs.
            <em>SIAM Journal on Optimization</em>, <em>33</em>(3),</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb139">2248–2274.</a></p>
    <p>Ren, J., Feng, X., Liu, B., Pan, X., Fu, Y., Mai, L., &amp; Yang, Y. (2022). TorchOpt: An</p>
    <p><span id="_bookmark182" class="anchor"></span>efficient library for differentiable optimization. arXiv preprint
        <a href="http://arxiv.org/abs/2211.06934">arXiv:2211.06934</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb141">Rios,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb141">I., Wets, R. J., &amp; Woodruff, D. L. (2015).
            Multi-period forecasting and scenario</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb141">generation with limited data.
            <em>Computational Management Science</em>, <em>12</em>(2), 267–295.</a></p>
    <p><span id="_bookmark184" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb142">Rockafellar, R. T., &amp; Wets, R. J.-B.
            (2009). <em>Variational analysis</em>. Berlin: Springer.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb143">Rudin,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb143">C. (2019). Stop explaining black box machine
            learning models for high stakes</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb143">decisions and use interpretable models
            instead. <em>Nature Machine Intelligence</em>, <em>1</em>(5),</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb143">206–215.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb144">Rust,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb144">J. (1987). Optimal replacement of GMC bus
            engines: An empirical model of</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb144">Harold
            Zurcher. <em>Econometrica</em>, 999–1033.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb145">Rust,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb145">J. (1988). Maximum likelihood estimation of
            discrete control processes. <em>SIAM</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb145"><em>Journal on Control and Optimization</em>,
            <em>26</em>(5), 1006–1024.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb146">Rychener,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb146">Y., Kuhn, D., &amp; Sutter, T. (2023).
            End-to-end learning for stochastic</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb146">optimization: A Bayesian perspective. In
            <em>International Conference on Machine</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb146"><em>Learning</em>.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb147">Sahoo,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb147">S. S., Paulus, A., Vlastelica, M., Musil, V.,
            Kuleshov, V., &amp; Martius, G. (2023).</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb147">Backpropagation through combinatorial
            algorithms: Identity with projection works.</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb147">In <em>International Conference on Learning
                Representations</em>.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb148">Sang,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb148">L., Xu, Y., Long, H., Hu, Q., &amp; Sun, H.
            (2022). Electricity price prediction for</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb148">energy storage system arbitrage: A
            decision-focused approach. <em>IEEE Transactions on</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb148"><em>Smart Grid</em>, <em>13</em>(4),
            2822–2832.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb149">Sang,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb149">L., Xu, Y., Long, H., &amp; Wu, W. (2023).
            Safety-aware semi-end-to-end coordi-</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb149">nated decision model for voltage regulation in
            active distribution network. <em>IEEE</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb149"><em>Transactions on Smart Grid</em>,
            <em>14</em>(3), 1814–1826.</a></p>
    <p>Sen, S., &amp; Deng, Y. (2017). Learning enabled optimization: Towards a fusion of</p>
    <p>statistical learning and stochastic programming. Available at <a
            href="https://optimization-online.org/?p=14456">https://optimization-</a> <a
            href="https://optimization-online.org/?p=14456">online.org/?p=14456</a>.</p>
    <p>Serrano, B., Minner, S., Schiffer, M., &amp; Vidal, T. (2022). Bilevel optimization for feature <span
            id="_bookmark193" class="anchor"></span>selection in the data-driven newsvendor problem. arXiv preprint <a
            href="http://arxiv.org/abs/2209.05093">arXiv:2209.05093</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb152">Shah,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb152">S., Wang, K., Wilder, B., Perrault, A., &amp;
            Tambe, M. (2022). Decision-focused</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb152">learning without decision-making: Learning
            locally optimized decision losses. In</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb152"><em>Advances in Neural Information Processing
                Systems</em>: <em>vol. 35</em>, (pp. 1320–1332). Curran</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb152">Associates, Inc.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb153">Shapiro, A., Dentcheva, D., &amp; Ruszczyński,
            A. (2014). <em>Lectures on stochastic programming:</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb153"><em>modeling and theory</em> (second ed.).
            Philadelphia: SIAM.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb154">Shlezinger,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb154">N., Eldar, Y. C., &amp; Boyd, S. P. (2022).
            Model-based deep learning: On the</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb154">intersection of deep learning and
            optimization. <em>IEEE Access</em>, <em>10</em>, 115384–115398.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb155">Smith, J. E., &amp; Winkler, R. L. (2006). The
            optimizer’s curse: Skepticism and postdecision</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb155">surprise in decision analysis. <em>Management
                Science</em>, <em>52</em>(3), 311–322.</a></p>
    <p>Srivastava, P. R., Wang, Y., Hanasusanto, G. A., &amp; Ho, C. P. (2021). On data-</p>
    <p>driven prescriptive analytics with side information: A regularized Nadaraya-Watson approach. arXiv preprint <a
            href="http://arxiv.org/abs/2110.04855">arXiv:2110.04855</a>.</p>
    <p><span id="_bookmark199" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb157">Stratigakos,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb157">A., Camal, S., Michiorri, A., &amp;
            Kariniotakis, G. (2022). Prescriptive trees</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb157">for integrated forecasting and optimization
            applied in trading of renewable energy.</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb157"><em>IEEE Transactions on Power Systems</em>,
            <em>37</em>(6), 4696–4708.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb158">Sun,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb158">X., Leung, C. H., Li, Y., &amp; Wu, Q. (2023).
            A unified perspective on regularization</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb158">and perturbation in differentiable subset
            selection. In <em>International Conference on</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb158"><em>Artificial Intelligence and
                Statistics</em> (pp. 4629–4642). PMLR.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb159">Sun,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb159">J., Li, H., &amp; Xu, Z. (2016). Deep ADMM-Net
            for compressive sensing MRI. In</a></p>
    <p><span id="_bookmark201" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb159"><em>Advances in Neural Information Processing
                Systems</em>: <em>vol. 29</em>, Curran Associates, Inc.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb160">Sun,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb160">C., Liu, S., &amp; Li, X. (2023). Maximum
            optimality margin: A unified approach for</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb160">contextual linear programming and inverse
            linear programming. In <em>International</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb160"><em>Conference on Machine Learning</em> (pp.
            32886–32912). PMLR.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb161">Sun,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb161">C., Liu, L., &amp; Li, X. (2024).
            Predict-then-calibrate: A new perspective of robust</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb161">contextual LP. <em>36</em>, In <em>Advances in
                Neural Information Processing Systems</em>. Curran</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb161">Associates, Inc..</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb162">Sun,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb162">H., Shi, Y., Wang, J., Tuan, H. D., Poor, H.
            V., &amp; Tao, D. (2023). Alternating</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb162">differentiation for optimization layers. In
            <em>International Conference on Learning</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb162"><em>Representations</em>.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb163">Sutton,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb163">R. S., McAllester, D., Singh, S., &amp;
            Mansour, Y. (1999). Policy gradient methods</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb163">for reinforcement learning with function
            approximation. <em>vol. 12</em>, In <em>Advances in</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb163"><em>Neural Information Processing
                Systems</em>. MIT Press.</a></p>
    <p>Tang, B., &amp; Khalil, E. B. (2022). PyEPO: A PyTorch-based end-to-end predict-then-</p>
    <p>optimize library for linear and integer programming. arXiv preprint <a
            href="http://arxiv.org/abs/2206.14234">arXiv:2206.</a> <a href="http://arxiv.org/abs/2206.14234">14234</a>.
    </p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb165">Tian,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb165">X., Yan, R., Liu, Y., &amp; Wang, S. (2023). A
            smart predict-then-optimize method for</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb165">targeted and cost-effective maritime
            transportation. <em>Transportation Research, Part B</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb165"><em>(Methodological)</em>, <em>172</em>,
            32–52.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb166">Tian,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb166">X., Yan, R., Wang, S., &amp; Laporte, G.
            (2023). Prescriptive analytics for a maritime</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb166">routing problem. <em>Ocean &amp; Coastal
                Management</em>, <em>242</em>, Article 106695.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb167">Tian, X., Yan, R., Wang, S., Liu, Y., &amp;
            Zhen, L. (2023). Tutorial on prescriptive analytics</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb167">for logistics: What to predict and how to
            predict. <em>Electronic Research Archive</em>, <em>31</em>(4),</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb167">2265–2285.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb168">Van</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb168">Parys, B. P., Esfahani, P. M., &amp; Kuhn, D.
            (2021). From data to deci-</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb168">sions:
            Distributionally robust optimization is optimal. <em>Management Science</em>, <em>67</em>(6),</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb168">3387–3402.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb169">Vlastelica,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb169">M., Paulus, A., Musil, V., Martius, G., &amp;
            Rolinek, M. (2019). Differenti-</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb169">ation
            of blackbox combinatorial solvers. In <em>International Conference on Learning</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb169"><em>Representations</em>.</a></p>
    <p>Vohra, R., Rajaei, A., &amp; Cremer, J. L. (2023). End-to-end learning with multiple</p>
    <p>modalities for system-optimised renewables nowcasting. arXiv preprint <a
            href="http://arxiv.org/abs/2304.07151">arXiv:2304.</a> <a href="http://arxiv.org/abs/2304.07151">07151</a>.
    </p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb171">Wager,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb171">S., &amp; Athey, S. (2018). Estimation and
            inference of heterogeneous treatment</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb171">effects using random forests. <em>Journal of
                the American Statistical Association</em>, <em>113</em>(523),</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb171">1228–1242.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb172">Wahdany, D., Schmitt, C., &amp; Cremer, J. L.
            (2023). More than accuracy: end-to-end wind</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb172">power forecasting that optimises the energy
            system. <em>Electric Power Systems Research</em>,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb172"><em>221</em>, Article 109384.</a></p>
    <p>Wang, T., Chen, N., &amp; Wang, C. (2021). Distributionally robust prescriptive analytics</p>
    <p><span id="_bookmark215" class="anchor"></span>with Wasserstein distance. arXiv preprint <a
            href="http://arxiv.org/abs/2106.05724">arXiv:2106.05724</a>.</p>
    <p>Wang, K., Verma, S., Mate, A., Shah, S., Taneja, A., Madhiwalla, N., Hegde, A., &amp; Tambe, M. (2023). Scalable
        decision-focused learning in restless multi-armed bandits with application to maternal and child health. arXiv
        preprint <a href="http://arxiv.org/abs/2202.00916">arXiv:2202.</a> <a
            href="http://arxiv.org/abs/2202.00916">00916</a>.</p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb175">Wang, K., Wilder, B., Perrault, A., &amp; Tambe,
            M. (2020). Automatically learning compact</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb175">quality-aware surrogates for optimization
            problems. <em>vol. 33</em>, In <em>Advances in Neural</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb175"><em>Information Processing Systems</em> (pp.
            9586–9596). Curran Associates, Inc.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb176">Watson,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb176">G. (1964). Smooth regression analysis.
            <em>Sankhya¯ : The Indian Journal of Statistics,</em></a> <span id="_bookmark218" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb176"><em>Series A</em>, <em>26</em>(4),
            359–372.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb177">Wilder,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb177">B., Dilkina, B., &amp; Tambe, M. (2019).
            Melding the data-decisions pipeline:</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb177">Decision-focused learning for combinatorial
            optimization. <em>vol. 33</em>, In <em>AAAI</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb177"><em>Conference on Artificial Intelligence</em>
            (01), (pp. 1658–1665).</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb178">Wilder,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb178">B., Ewing, E., Dilkina, B., &amp; Tambe, M.
            (2019). End to end learning and</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb178">optimization on graphs. <em>vol. 32</em>, In
            <em>Advances in Neural Information Processing Systems</em>.</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb178">Curran Associates, Inc.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb179">Xie,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb179">X., Wu, J., Liu, G., Zhong, Z., &amp; Lin, Z.
            (2019). Differentiable linearized ADMM.</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb179">In <em>International Conference on Machine
                Learning</em> (pp. 6902–6911). PMLR.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb180">Xu,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb180">Y., &amp; Cohen, S. B. (2018). Stock movement
            prediction from tweets and historical</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb180">prices. In <em>Proceedings of the 56th Annual
                Meeting of the Association for Computational</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb180"><em>Linguistics</em> (pp. 1970–1979).
            Association for Computational Linguistics.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb181">Yan,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb181">R., Wang, S., Cao, J., &amp; Sun, D. (2021).
            Shipping domain knowledge informed</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb181">prediction and optimization in port state
            control. <em>Transportation Research, Part B</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb181"><em>(Methodological)</em>, <em>149</em>,
            52–78.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb182">Yan,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb182">R., Wang, S., &amp; Fagerholt, K. (2020). A
            semi-‘‘smart predict then optimize’’</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb182">(semi-SPO) method for efficient ship
            inspection. <em>Transportation Research, Part B</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb182"><em>(Methodological)</em>, <em>142</em>,
            100–125.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb183">Yan,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb183">R., Wang, S., &amp; Zhen, L. (2023). An
            extended smart ‘‘predict, and optimize’’</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb183">(SPO) framework based on similar sets for ship
            inspection planning. <em>Transportation</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb183"><em>Research Part E: Logistics and
                Transportation Review</em>, <em>173</em>, Article 103109.</a></p>
    <p>Yang, J., Zhang, L., Chen, N., Gao, R., &amp; Hu, M. (2023). Decision-making with side</p>
    <p>information: A causal transport robust approach. Available at <a
            href="https://optimization-online.org/?p=20639">https://optimization-</a> <a
            href="https://optimization-online.org/?p=20639">online.org/?p=20639</a>.</p>
    <p><span id="_bookmark227" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb185">Yanıkoğlu,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb185">I., Gorissen, B. L., &amp; den Hertog, D.
            (2019). A survey of adjustable robust</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb185">optimization. <em>European Journal of
                Operational Research</em>, <em>277</em>(3), 799–813.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb186">Zhang,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb186">Y., &amp; Gao, J. (2017). Assessing the
            performance of deep learning algorithms</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb186">for newsvendor problem. In D. Liu, S. Xie, Y.
            Li, D. Zhao, &amp; E.-S. M. El-Alfy</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb186">(Eds.), <em>Neural information processing</em>
            (pp. 912–921). Cham: Springer International</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb186">Publishing.</a></p>
    <p>Zhang, Y., Liu, J., &amp; Zhao, X. (2023). Data-driven piecewise affine decision rules for stochastic programming
        with covariate information. arXiv preprint <a href="http://arxiv.org/abs/2304.13646">arXiv:2304.</a> <a
            href="http://arxiv.org/abs/2304.13646">13646</a>.</p>
    <p><span id="_bookmark230" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb188">Zhang,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb188">L., Yang, J., &amp; Gao, R. (2023). Optimal
            robust policy for feature-based</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb188">newsvendor. <em>Management Science</em>.</a>
    </p>
    <p>Zhang, C., Zhang, Z., Cucuringu, M., &amp; Zohren, S. (2021). A universal end-to-end</p>
    <p>approach to portfolio optimization via deep learning. arXiv preprint <a
            href="http://arxiv.org/abs/2111.09170">arXiv:2111.</a> <a href="http://arxiv.org/abs/2111.09170">09170</a>.
    </p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb190">Zhu,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb190">T., Xie, J., &amp; Sim, M. (2022). Joint
            estimation and robustness optimization.</a></p>
    <p><span id="_bookmark232" class="anchor"></span><a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb190"><em>Management Science</em>, <em>68</em>(3),
            1659–1677.</a></p>
    <p><a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb191">Ziebart,</a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb191">B. D., Maas, A. L., Bagnell, J. A., &amp; Dey,
            A. K. (2008). Maximum entropy</a> <a href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb191">inverse
            reinforcement learning. <em>Vol. 8</em>, In <em>AAAI Conference on Artificial Intelligence</em></a> <a
            href="http://refhub.elsevier.com/S0377-2217(24)00220-0/sb191">(pp. 1433–1438). Chicago, IL, USA.</a></p>
</blockquote>